<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Zookeeper的安装]]></title>
    <url>%2F2018%2F10%2F19%2Fblog181019-3%2F</url>
    <content type="text"><![CDATA[概览1.Zookeeper简介2.Zookeeper的安装3.Zookeeper的配置4.启动集群5.数据同步测试 1.Zookeeper简介Zookeeper功能简介 ZooKeeper 是一个开源的分布式协调服务，由雅虎创建，是 Google Chubby 的开源实现分布式应用程序可以基于ZooKeeper 实现诸如数据发布/订阅、负载均衡、命名服务、分布式协调/通知、集群管理、Master选举、配置维护，名字服务、分布式同步、分布式锁和分布式队列等功能。 在Zookeeper中共有三个角色 1. Leader 2. Follower 3. Observe 一个 ZooKeeper 集群同一时刻只会有一个 Leader，其他都是 Follower 或 Observer。 2.Zookeeper的安装这次我们是在之前的Hadoop集群搭建的基础上安装的,所以没有的小伙伴最好先配置好Hadoop集群,不然还要安装jdk 在虚拟机的/usr下创建个zookeeper文件夹mkdir zookeeper然后将zookeeper的安装包(压缩包)上传到该文件夹下 解压 123456[root@master usr]# mkdir zookeeper[root@master usr]# lsbin etc games hadoop include java lib lib64 libexec local sbin share src tmp zookeeper[root@master usr]# cd zookeeper/#解压[root@master zookeeper]# tar -zxf zookeeper-3.4.12.tar.gz 3.Zookeeper的配置（先在一台节点上配置）添加一个zoo.cfg配置文件 1234#进入到Zookeeper的配置目录cd /usr/zookeeper/zookeeper-3.4.12/conf#copy出来一个配置文件cp zoo_sample.cfg zoo.cfg 这是zoo.cfg中各项的含义 zookeeper的默认配置文件为zookeeper/conf/zoo_sample.cfg，需要将其修改为zoo.cfg。其中各配置项的含义，解释如下： 1.tickTime：CS通信心跳时间 Zookeeper 服务器之间或客户端与服务器之间维持心跳的时间间隔，也就是每个 tickTime 时间就会发送一个心跳。tickTime以毫秒为单位。 tickTime=2000 2.initLimit：LF初始通信时限 集群中的follower服务器(F)与leader服务器(L)之间初始连接时能容忍的最多心跳数（tickTime的数量）。initLimit=5 3.syncLimit：LF同步通信时限 集群中的follower服务器与leader服务器之间请求和应答之间能容忍的最多心跳数（tickTime的数量）。 syncLimit=2 4.dataDir：数据文件目录 Zookeeper保存数据的目录，默认情况下，Zookeeper将写数据的日志文件也保存在这个目录里。 dataDir=/home/michael/opt/zookeeper/data 5.clientPort：客户端连接端口 客户端连接 Zookeeper 服务器的端口，Zookeeper 会监听这个端口，接受客户端的访问请求。 clientPort=2181 6.服务器名称与地址：集群信息（服务器编号，服务器地址，LF通信端口，选举端口） 这个配置项的书写格式比较特殊，规则如下： server.N=YYY:A:B 修改配置文件（zoo.cfg）将dataDir=/tmp/zookeeper 改为 dataDir=/usr/zookeeper/data然后在最后追加 123456#master,slave1,slave2这是我的三台虚拟机主机名#并且在/usr/hosts文件中做了ip映射#所以可以直接写主机名称,如果你没有做映射,将主机名替换为ip地址server.1=master:2888:3888server.2=slave1:2888:3888server.3=slave2:2888:3888 然后我们在/usr/zookeeper文件夹下创建一个data文件夹 1234[root@master conf]# cd /usr/zookeeper/[root@master zookeeper]# lszookeeper-3.4.12 zookeeper-3.4.12.tar.gz[root@master zookeeper]# mkdir data 然后在data下创建一个myid文件(ZooKeeper 配置很简单，每个节点的配置文件(zoo.cfg)都是一样的，只有 myid 文件不一样。myid 的值必须是 zoo.cfg中server.{数值} 的{数值}部分。)在（/usr/zookeeper/data 需要自己创建）创建一个myid文件，里面内容是server.N中的N（server.2里面内容为2） 1234567#建立文件[root@master data]# touch myid#传入值[root@master data]# echo 1 &gt; myid #查看[root@master data]# cat myid 1 然后我们把配置好的zookeeper传给另外两台主机slave1和slave2 12scp -r /usr/zookeeper root@slave1:/usrscp -r /usr/zookeeper root@slave2:/usr 注意：在其他节点上一定要修改myid的内容 在slave1应该讲myid的内容改为2（echo 2 &gt; myid） 在slave2应该讲myid的内容改为3 （echo 3 &gt; myid） 4.启动集群(注意关闭防火墙)在master , slave1 , slave2分别启动zookeeper 123456#进入zookeeper下的bin文件夹[root@master usr]# cd /usr/zookeeper/zookeeper-3.4.12/bin[root@master bin]# lsREADME.txt zkCleanup.sh zkCli.cmd zkCli.sh zkEnv.cmd zkEnv.sh zkServer.cmd zkServer.sh#启动服务[root@master bin]# ./zkServer.sh start 查看状态12345678[root@master bin]# ./zkServer.sh startZooKeeper JMX enabled by defaultUsing config: /usr/zookeeper/zookeeper-3.4.12/bin/../conf/zoo.cfgStarting zookeeper ... STARTED[root@master bin]# ./zkServer.sh statusZooKeeper JMX enabled by defaultUsing config: /usr/zookeeper/zookeeper-3.4.12/bin/../conf/zoo.cfgMode: follower #这里是follower便可以认为启动成功 5.数据同步测试进入到zkCilent 1./zkCli.sh 在主机master上创建一个文件,看看是否同步到其他机器上 1234567891011121314151617181920#查看[zk: localhost:2181(CONNECTED) 0] ls /[zookeeper]#创建文件 /路径 内容[zk: localhost:2181(CONNECTED) 1] create /test helloCreated /test#查看文件内容[zk: localhost:2181(CONNECTED) 2] get /testhellocZxid = 0x100000002ctime = Fri Oct 19 21:51:21 CST 2018mZxid = 0x100000002mtime = Fri Oct 19 21:51:21 CST 2018pZxid = 0x100000002cversion = 0dataVersion = 0aclVersion = 0ephemeralOwner = 0x0dataLength = 5numChildren = 0 在其他机器上也进行查看 123456789101112131415161718#进入到zkCli[root@slave1 bin]# ./zkCli.sh#查看[zk: localhost:2181(CONNECTED) 0] ls /[zookeeper, test][zk: localhost:2181(CONNECTED) 1] get /testhellocZxid = 0x100000002ctime = Fri Oct 19 21:51:21 CST 2018mZxid = 0x100000002mtime = Fri Oct 19 21:51:21 CST 2018pZxid = 0x100000002cversion = 0dataVersion = 0aclVersion = 0ephemeralOwner = 0x0dataLength = 5numChildren = 0 再到slave2上查看,数据是同步的 删除 123[zk: localhost:2181(CONNECTED) 3] delete /test[zk: localhost:2181(CONNECTED) 4] ls /[zookeeper] 再到其他机器进行查看,发现也删除了,表示你的zookeeper的集群安装已完成 接下来还会介绍hadoop+zookeeper3节点高可用集群搭建]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[将MapReduce分析手机上网记录的结果进行排序操作]]></title>
    <url>%2F2018%2F10%2F19%2Fblog181019-2%2F</url>
    <content type="text"><![CDATA[上次我们说过了MapReduce对手机上网记录的简单分析和Partitioner分区这次我们介绍一下如何将手机上网记录根据总流量的多少进行排序 1.编写Java代码,并将其打包成jar包在eclipse上创建个新的java项目,创建lib文件夹,将上次的jar同样导入进来 然后创建个TelBean类这里实现了WritableComparable接口,就是序列化的比较,详情查询api文档 public interface Comparator比较功能，对一些对象的集合施加了一个整体排序 。 可以将比较器传递给排序方法（如Collections.sort或Arrays.sort ），以便对排序顺序进行精确控制。 比较器还可以用来控制某些数据结构（如顺序sorted sets或sorted maps ），或对于不具有对象的集合提供的排序natural ordering 。通过比较c上的一组元素S的确定的顺序对被认为是与equals一致当且仅当c.compare(e1, e2)==0具有用于S每e1和e2相同布尔值e1.equals(e2)。 当使用能够强制排序不一致的比较器时，应注意使用排序集（或排序图）。 假设具有显式比较器c的排序集（或排序映射）与从集合S中绘制的元素（或键）一起使用 。 如果88446235254451上的c强制的排序与equals不一致，则排序集（或排序映射）将表现为“奇怪”。特别是排序集（或排序图）将违反用于设置（或映射）的一般合同，其按equals定义。 例如，假设一个将两个元件a和b ，使得(a.equals(b) &amp;&amp; c.compare(a, b) != 0)到空TreeSet与比较c。 因为a和b与树集的角度不相等，所以第二个add操作将返回true（并且树集的大小将增加），即使这与Set.add方法的规范相反。 注意：这通常是一个好主意比较，也能实现java.io.Serializable，因为它们可能被用来作为排序的序列化数据结构的方法（如TreeSet， TreeMap ）。 为了使数据结构成功序列化，比较器（如果提供）必须实现Serializable 。 对于数学上的倾斜，即限定了施加顺序 ，给定的比较器c上一组给定对象的S强加关系式为： {(x, y) such that c.compare(x, y) &lt;= 0}. 这个总订单的商是： {(x, y) suchthat c.compare(x, y) == 0}. 它从合同compare，该商数是S的等价关系紧随其后，而强加的排序是S， 总订单 。当我们说S上的c所规定的顺序与等于一致时，我们的意思是排序的商是由对象’ equals(Object)方法定义的等价关系： {(x,y) such that x.equals(y)}. 与Comparable不同，比较器可以可选地允许比较空参数，同时保持对等价关系的要求。 此接口是成员Java Collections Framework 。 从以下版本开始：1.2 另请参见： Comparable ， Serializable 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495package com.zy.hadoop.entity;import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;import org.apache.hadoop.io.Writable;import org.apache.hadoop.io.WritableComparable;public class TelBean implements WritableComparable&lt;TelBean&gt;&#123; private String tel; private long upPayLoad; private long downPayLoad; private long totalPayLoad; public String getTel() &#123; return tel; &#125; public void setTel(String tel) &#123; this.tel = tel; &#125; public long getUpPayLoad() &#123; return upPayLoad; &#125; public void setUpPayLoad(long upPayLoad) &#123; this.upPayLoad = upPayLoad; &#125; public long getDownPayLoad() &#123; return downPayLoad; &#125; public void setDownPayLoad(long downPayLoad) &#123; this.downPayLoad = downPayLoad; &#125; public long getTotalPayLoad() &#123; return totalPayLoad; &#125; public void setTotalPayLoad(long totalPayLoad) &#123; this.totalPayLoad = totalPayLoad; &#125; public TelBean(String tel, long upPayLoad, long downPayLoad, long totalPayLoad) &#123; super(); this.tel = tel; this.upPayLoad = upPayLoad; this.downPayLoad = downPayLoad; this.totalPayLoad = totalPayLoad; &#125; public TelBean() &#123; super(); // TODO Auto-generated constructor stub &#125; @Override public String toString() &#123; return tel + &quot;\t&quot; + upPayLoad + &quot;\t&quot; + downPayLoad + &quot;\t&quot; + totalPayLoad ; &#125; //反序列化的过程 @Override public void readFields(DataInput in) throws IOException &#123; this.tel = in.readUTF(); this.upPayLoad = in.readLong(); this.downPayLoad = in.readLong(); this.totalPayLoad = in.readLong(); &#125; //序列化的过程 @Override public void write(DataOutput out) throws IOException &#123; // TODO Auto-generated method stub out.writeUTF(this.tel); out.writeLong(this.upPayLoad); out.writeLong(this.downPayLoad); out.writeLong(this.totalPayLoad); &#125; //compare比较,详情查阅java的api文档 @Override public int compareTo(TelBean bean) &#123; // TODO Auto-generated method stub return (int)(this.totalPayLoad-bean.getTotalPayLoad()); &#125;&#125; 然后在mr包下依次建立SortMapper,SortReducer,SortCount 关于分析可以查看MapReduce对手机上网记录的简单分析和Partitioner分区SortMapper 12345678910111213141516171819202122232425262728293031323334353637package com.zy.hadoop.mr2;import java.io.IOException;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;import com.zy.hadoop.entity.TelBean;public class SortMapper extends Mapper&lt;LongWritable, Text, TelBean, NullWritable&gt;&#123; @Override protected void map(LongWritable key, Text value, Mapper&lt;LongWritable, Text, TelBean, NullWritable&gt;.Context context) throws IOException, InterruptedException &#123; //value ,第一mr出来的结果中的每一行 String line = value.toString(); //拆分字符串&quot;\t&quot; String[] strs = line.split(&quot;\t&quot;); //直接通过下标取值 //电话号码 String tel = strs[0]; //上行流量 long upPayLoad=Long.parseLong(strs[2]); //下行流量 long downPayLoad=Long.parseLong(strs[3]); //总流量 long totalPayLoad=Long.parseLong(strs[4]); //把去除的值封装到对象中 TelBean telBean = new TelBean(tel, upPayLoad, downPayLoad, totalPayLoad); //输出k2,v2 context.write(telBean, NullWritable.get()); &#125;&#125; SortReducer 1234567891011121314151617181920package com.zy.hadoop.mr2;import java.io.IOException;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.mapreduce.Reducer;import com.zy.hadoop.entity.TelBean;public class SortReducer extends Reducer&lt;TelBean, NullWritable, TelBean, NullWritable&gt;&#123; @Override protected void reduce(TelBean arg0, Iterable&lt;NullWritable&gt; arg1, Reducer&lt;TelBean, NullWritable, TelBean, NullWritable&gt;.Context arg2) throws IOException, InterruptedException &#123; arg2.write(arg0, NullWritable.get()); &#125; &#125; SortCount 123456789101112131415161718192021222324252627282930313233343536373839404142package com.zy.hadoop.mr2;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import com.zy.hadoop.entity.TelBean;public class SortCount &#123; public static void main(String[] args) throws Exception &#123; // 1.获取job Configuration conf = new Configuration(); Job job = Job.getInstance(conf); // 2.指定job使用的类 job.setJarByClass(SortCount.class); // 3.设置Mapper的属性 job.setMapperClass(SortMapper.class); job.setMapOutputKeyClass(TelBean.class); job.setMapOutputValueClass(NullWritable.class); // 4.设置输入文件 FileInputFormat.setInputPaths(job, new Path(args[0])); // 5.设置reducer的属性 job.setReducerClass(SortReducer.class); job.setOutputKeyClass(TelBean.class); job.setMapOutputValueClass(NullWritable.class); // 6.设置输出文件夹,查看结果保存到hdfs文件夹中的位置 FileOutputFormat.setOutputPath(job, new Path(args[1])); // 7.提交 true 提交的时候打印日志信息 job.waitForCompletion(true); &#125;&#125; 接下来将项目打包成jar包,上传到虚拟机/usr/tmp下 2.虚拟机上运行jar包,查看结果启动hadoop集群服务 1start-all.sh 查看是否成功 我们将之前处理过一次的文件/tel1/part-r-00000(/tel2下的进行过分区了,所以不进行处理)作为源文件进行分析排序 1hadoop jar tel_3.jar /tel/part-r-00000 /tel3 等待执行完毕查看结果 结果如下123456789101112131415161718192021222324252627282930[root@master tmp]# hadoop fs -ls /hadoopFound 5 items-rw-r--r-- 1 root supergroup 2315 2018-10-19 19:33 /tel.logdrwxr-xr-x - root supergroup 0 2018-10-19 19:59 /tel1drwxr-xr-x - root supergroup 0 2018-10-19 20:10 /tel2drwxr-xr-x - root supergroup 0 2018-10-19 20:47 /tel3drwx------ - root supergroup 0 2018-10-19 19:58 /tmp[root@master tmp]# hadoop fs -ls /tel3Found 2 items-rw-r--r-- 1 root supergroup 0 2018-10-19 20:47 /tel3/_SUCCESS-rw-r--r-- 1 root supergroup 477 2018-10-19 20:47 /tel3/part-r-00000[root@master tmp]# hadoop fs -cat /tel3/part-r-0000013926251106 240 0 24013826544101 264 0 26413480253104 180 180 36013926435656 132 1512 164415989002119 1938 180 211818211575961 1527 2106 363313560436666 2232 1908 414013602846565 1938 2910 484884138413 4116 1432 554815920133257 3156 2936 609213922314466 3008 3720 672815013685858 3659 3538 719713660577991 6960 690 765013560439658 2034 5892 792618320173382 9531 2412 1194313726238888 2481 24681 2716213925057413 11058 48243 5930113502468823 7335 110349 117684 这就是MapReduce进行简单的数据分析 不过hadoop集群的datanode节点如果过多会导致速度慢 , 接下来会介绍zookeeper的高效hadoop集群如何搭建]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Java</tag>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapReduce对手机上网记录的简单分析和Partitioner分区]]></title>
    <url>%2F2018%2F10%2F19%2Fblog181019-1%2F</url>
    <content type="text"><![CDATA[概览1.MapReduce处理手机上网记录2.Partitioner分区 上次说过了关于MapReduce的执行流程和原理,下面来说下分区和简单示例 1.MapReduce处理手机上网记录首先我们需要先模拟一个通话记录文件 在Windows的桌面建个tel.log的文件,里面模拟一些通话记录信息 12345678910111213141516171819202122231363157985066 13726230503 00-FD-07-A4-72-B8:CMCC 120.196.100.82 i02.c.aliimg.com 24 27 2481 24681 2001363157995052 13826544101 5C-0E-8B-C7-F1-E0:CMCC 120.197.40.4 4 0 264 0 2001363157991076 13926435656 20-10-7A-28-CC-0A:CMCC 120.196.100.99 2 4 132 1512 2001363154400022 13926251106 5C-0E-8B-8B-B1-50:CMCC 120.197.40.4 4 0 240 0 2001363157993044 18211575961 94-71-AC-CD-E6-18:CMCC-EASY 120.196.100.99 iface.qiyi.com 视频网站 15 12 1527 2106 2001363157995074 84138413 5C-0E-8B-8C-E8-20:7DaysInn 120.197.40.4 122.72.52.12 20 16 4116 1432 2001363157993055 13560439658 C4-17-FE-BA-DE-D9:CMCC 120.196.100.99 18 15 1116 954 2001363157995033 15920133257 5C-0E-8B-C7-BA-20:CMCC 120.197.40.4 sug.so.360.cn 信息安全 20 20 3156 2936 2001363157983019 13719199419 68-A1-B7-03-07-B1:CMCC-EASY 120.196.100.82 4 0 240 0 2001363157984041 13660577991 5C-0E-8B-92-5C-20:CMCC-EASY 120.197.40.4 s19.cnzz.com 站点统计 24 9 6960 690 2001363157973098 15013685858 5C-0E-8B-C7-F7-90:CMCC 120.197.40.4 rank.ie.sogou.com 搜索引擎 28 27 3659 3538 2001363157986029 15989002119 E8-99-C4-4E-93-E0:CMCC-EASY 120.196.100.99 www.umeng.com 站点统计 3 3 1938 180 2001363157992093 13560439658 C4-17-FE-BA-DE-D9:CMCC 120.196.100.99 15 9 918 4938 2001363157986041 13480253104 5C-0E-8B-C7-FC-80:CMCC-EASY 120.197.40.4 3 3 180 180 2001363157984040 13602846565 5C-0E-8B-8B-B6-00:CMCC 120.197.40.4 2052.flash2-http.qq.com 综合门户 15 12 1938 2910 2001363157995093 13922314466 00-FD-07-A2-EC-BA:CMCC 120.196.100.82 img.qfc.cn 12 12 3008 3720 2001363157982040 13502468823 5C-0A-5B-6A-0B-D4:CMCC-EASY 120.196.100.99 y0.ifengimg.com 综合门户 57 102 7335 110349 2001363157986072 18320173382 84-25-DB-4F-10-1A:CMCC-EASY 120.196.100.99 input.shouji.sogou.com 搜索引擎 21 18 9531 2412 2001363157990043 13925057413 00-1F-64-E1-E6-9A:CMCC 120.196.100.55 t3.baidu.com 搜索引擎 69 63 11058 48243 2001363157988072 13760778710 00-FD-07-A4-7B-08:CMCC 120.196.100.82 2 2 120 120 2001363157985066 13726238888 00-FD-07-A4-72-B8:CMCC 120.196.100.82 i02.c.aliimg.com 24 27 2481 24681 2001363157993055 13560436666 C4-17-FE-BA-DE-D9:CMCC 120.196.100.99 18 15 1116 954 2001363157993055 13560436666 C4-17-FE-BA-DE-D9:CMCC 120.196.100.99 18 15 1116 954 200 这些字段代表的是 首先我们需要将部分字段提取出来,以便之后进行分析 在主机master上启动hadoop集群,hadoop集群版的搭建可以参照简单的hadoop集群搭建 1start-all.sh 验证是否启动成功 然后将tel.log文件利用Xftp传输到虚拟机中的/usr/tmp下cd /usr/tmp/ 然后上传到hdfs 123456#上传[root@master tmp]# hadoop fs -put tel.log /#查看[root@master tmp]# hadoop fs -ls /Found 1 items-rw-r--r-- 1 root supergroup 2315 2018-10-19 19:33 /tel.log 然后在eclipse上新建java项目,并在项目下建个lib文件夹,然后将jar包放到lib中导入项目然后创建包,创建一个telBean实体类,这次我们分析的是手机号和其对应的上行流量,下行流量和总流量所以将其封装成实体类 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889package com.hd.entity;import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;import org.apache.hadoop.io.Writable;public class TelBean implements Writable&#123; private String tel; private long upPayLoad; private long downPayLoad; private long totalPayLoad; public String getTel() &#123; return tel; &#125; public void setTel(String tel) &#123; this.tel = tel; &#125; public long getUpPayLoad() &#123; return upPayLoad; &#125; public void setUpPayLoad(long upPayLoad) &#123; this.upPayLoad = upPayLoad; &#125; public long getDownPayLoad() &#123; return downPayLoad; &#125; public void setDownPayLoad(long downPayLoad) &#123; this.downPayLoad = downPayLoad; &#125; public long getTotalPayLoad() &#123; return totalPayLoad; &#125; public void setTotalPayLoad(long totalPayLoad) &#123; this.totalPayLoad = totalPayLoad; &#125; public TelBean(String tel, long upPayLoad, long downPayLoad, long totalPayLoad) &#123; super(); this.tel = tel; this.upPayLoad = upPayLoad; this.downPayLoad = downPayLoad; this.totalPayLoad = totalPayLoad; &#125; public TelBean() &#123; super(); // TODO Auto-generated constructor stub &#125; @Override public String toString() &#123; return tel + &quot;\t&quot; + upPayLoad + &quot;\t&quot; + downPayLoad + &quot;\t&quot; + totalPayLoad ; &#125; //反序列化的过程 @Override public void readFields(DataInput in) throws IOException &#123; this.tel = in.readUTF(); this.upPayLoad = in.readLong(); this.downPayLoad = in.readLong(); this.totalPayLoad = in.readLong(); &#125; //序列化的过程 @Override public void write(DataOutput out) throws IOException &#123; // TODO Auto-generated method stub out.writeUTF(this.tel); out.writeLong(this.upPayLoad); out.writeLong(this.downPayLoad); out.writeLong(this.totalPayLoad); &#125;&#125; 在mr包下创建个TelMapper类继承Mapper首先分析一下,我们要传入的第一个需要Map处理的&lt;k1,v1&gt;是long类型(电话号码)和String(Text)类型(与之对应的一行记录),而从Map处理过的&lt;k2,v2&gt;是String类型(电话号码)和TelBean对象(将我们需要的字段封装成对象) 1234567891011121314151617181920212223242526272829303132package com.hd.hadoop.mr;import java.io.IOException;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;import com.zy.hadoop.entity.TelBean;//k1,v1 long string k2,v2 string TelBeanpublic class TelMapper extends Mapper&lt;LongWritable, Text, Text, TelBean&gt; &#123; @Override protected void map(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, TelBean&gt;.Context context) throws IOException, InterruptedException &#123; //value 对应 tel.log中的每一行数据,行中的数据以\t隔开的 String line = value.toString(); //对正航读取的数据进行拆分 String[] res = line.split(&quot;\t&quot;);//0---res.length-1 //取数组中的电话号码 String tel = res[1]; //取上行流量 long upPayLoad = Long.parseLong(res[8]); //取下行流量 long downPayLoad = Long.parseLong(res[9]); //创建telBean对象 TelBean telBean = new TelBean(tel, upPayLoad, downPayLoad, 0); context.write(new Text(tel), telBean); &#125;&#125; 然后创建个TelReducer类继承Reducer分析一下,这里传入的&lt;k2,v2&gt;是String(Text)类型和TelBean类型,而我们处理过输出的&lt;k3,v3&gt;也是相同类型,这里要记得将TelBean的toString方法重写,不然输出的是对象地址 12345678910111213141516171819202122232425262728293031package com.hd.hadoop.mr;import java.io.IOException;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Reducer;import com.zy.hadoop.entity.TelBean;public class TelReducer extends Reducer&lt;Text, TelBean, Text, TelBean&gt; &#123; @Override protected void reduce(Text key, Iterable&lt;TelBean&gt; value, Reducer&lt;Text, TelBean, Text, TelBean&gt;.Context context) throws IOException, InterruptedException &#123; //声明一个上行流量的变量 long upPayLoad = 0; //声明一个下行流量 long downPayLoad = 0; for (TelBean telBean : value) &#123; //统计相同电话的上行流量的和,下行流量的和 upPayLoad += telBean.getUpPayLoad(); downPayLoad += telBean.getDownPayLoad(); &#125; //k3 v3 TelBean telBean= new TelBean(key.toString(), upPayLoad, downPayLoad, upPayLoad+downPayLoad); context.write(key, telBean); &#125;&#125; 最后我们创建个主方法TelCount类 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748package com.zy.hadoop.mr1;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import com.zy.hadoop.entity.TelBean;public class TelCount &#123; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; // 1.获取job Configuration conf = new Configuration(); Job job = Job.getInstance(conf); // 2.指定job使用的类 job.setJarByClass(TelCount.class); // 3.设置Mapper的属性 job.setMapperClass(TelMapper.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(TelBean.class); // 4.设置输入文件 args[0]手动输入输入文件的位置 FileInputFormat.setInputPaths(job, new Path(args[0])); // 5.设置reducer的属性 job.setReducerClass(TelReducer.class); job.setOutputKeyClass(Text.class); job.setMapOutputValueClass(TelBean.class); // 6.设置输出文件夹,查看结果保存到hdfs文件夹中的位置 //args[1]手动输入输出文件的位置 FileOutputFormat.setOutputPath(job, new Path(args[1])); // 7.提交 true 提交的时候打印日志信息 job.waitForCompletion(true); &#125;&#125; 最后将其打成Jar包,主方法选择TelCount,然后上传到虚拟机/usr/tmp下 然后执行jar包 1hadoop jar tel_1.jar /tel.log /tel1 等待执行成功后查看结果文件 1234567891011121314151617181920212223#结果如下[root@master tmp]# hadoop fs -cat /tel1/part-r-0000013480253104 13480253104 180 180 36013502468823 13502468823 7335 110349 11768413560436666 13560436666 2232 1908 414013560439658 13560439658 2034 5892 792613602846565 13602846565 1938 2910 484813660577991 13660577991 6960 690 765013719199419 13719199419 240 0 24013726230503 13726230503 2481 24681 2716213726238888 13726238888 2481 24681 2716213760778710 13760778710 120 120 24013826544101 13826544101 264 0 26413922314466 13922314466 3008 3720 672813925057413 13925057413 11058 48243 5930113926251106 13926251106 240 0 24013926435656 13926435656 132 1512 164415013685858 15013685858 3659 3538 719715920133257 15920133257 3156 2936 609215989002119 15989002119 1938 180 211818211575961 18211575961 1527 2106 363318320173382 18320173382 9531 2412 1194384138413 84138413 4116 1432 5548 这说明执行成功了 2.Partitioner分区什么是Partitioner? 在进行MapReduce计算时，有时候需要把最终的输出数据分到不同的文件中，比如按照省份划分的话，需要把同一省份的数据放到一个文件中；按照性别划分的话，需要把同一性别的数据放到一个文件中。我们知道最终的输出数据是来自于Reducer任务。那么，如果要得到多个文件，意味着有同样数量的Reducer任务在运行。Reducer任务的数据来自于Mapper任务，也就说Mapper任务要划分数据，对于不同的数据分配给不同的Reducer任务运行。Mapper任务划分数据的过程就称作Partition。负责实现划分数据的类称作Partitioner。 我们还是处理手机的上网记录在之前的mr包中见一个TCPartitioner类我们将135和136开头的号码视为移动用户,处理结果放到一起(part-r-00000),另外的号码处理结果放到一起(part-r-00001) 1234567891011121314151617181920212223242526package com.hd.hadoop.mr;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Partitioner;import com.zy.hadoop.entity.TelBean;public class TCPartitioner extends Partitioner&lt;Text, TelBean&gt; &#123; @Override public int getPartition(Text text, TelBean telBean, int arg2) &#123; // TODO Auto-generated method stub String tel = text.toString(); String sub_tel = tel.substring(0, 3);//取手机号前三位进行分区 //假设135 136的为移动的 放一个分区,其他的放一个分区 if(sub_tel.equals(&quot;135&quot;)||sub_tel.equals(&quot;136&quot;))&#123; //return的数对应着计算结果文件 part-r-00001 return 1; &#125; return 0; &#125;&#125; 然后在TelCount添加几行代码 再将项目打成jar包放入到虚拟机/usr/tmp下,然后执行 1hadoop jar tel_2.jar /tel.log /tel2 等待执行完毕后查看结果 12345678910111213#查看生成几个结果文件[root@master tmp]# hadoop fs -ls /tel2Found 3 items-rw-r--r-- 1 root supergroup 0 2018-10-19 20:10 /tel2/_SUCCESS-rw-r--r-- 1 root supergroup 603 2018-10-19 20:10 /tel2/part-r-00000-rw-r--r-- 1 root supergroup 198 2018-10-19 20:10 /tel2/part-r-00001#查看135和136开头的手机号的结果[root@master tmp]# hadoop fs -cat /tel2/part-r-0000113502468823 13502468823 7335 110349 11768413560436666 13560436666 2232 1908 414013560439658 13560439658 2034 5892 792613602846565 13602846565 1938 2910 484813660577991 13660577991 6960 690 7650 这就是简单的分区操作 接下来还有如何将分析的结果进行排序操作]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Java</tag>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapReduce的执行流程和原理]]></title>
    <url>%2F2018%2F10%2F18%2Fblog181018-2%2F</url>
    <content type="text"><![CDATA[概览1.MapReduce简介2.MapReduce的执行流程3.MapReduce的原理4.测试MapReduce5.Java代码实现 1.MapReduce简介MapReduce是一种分布式计算模型，是Google提出的，主要用于搜索领域，解决海量数据的计算问题。 MR有两个阶段组成：Map和Reduce，用户只需实现map()和reduce()两个函数，即可实现分布式计算。 MapReduce是一种并行可扩展计算模型，并且有较好的容错性，主要解决海量离线数据的批处理。实现下面目标★ 易于编程★ 良好的扩展性★ 高容错性 MapReduce有哪些角色？各自的作用是什么？MapReduce由JobTracker和TaskTracker组成。JobTracker负责资源管理和作业控制，TaskTracker负责任务的运行。 2.MapReduce的执行流程MapReduce程序执行流程程序执行流程图如下： (1) 开发人员编写好MapReduce program，将程序打包运行。(2) JobClient向JobTracker申请可用Job，JobTracker返回JobClient一个可用Job ID。(3) JobClient得到Job ID后，将运行Job所需要的资源拷贝到共享文件系统HDFS中。(4) 资源准备完备后，JobClient向JobTracker提交Job。(5) JobTracker收到提交的Job后，初始化Job。(6) 初始化完成后，JobTracker从HDFS中获取输入splits(作业可以该启动多少Mapper任务)。(7) 与此同时，TaskTracker不断地向JobTracker汇报心跳信息，并且返回要执行的任务。(8) TaskTracker得到JobTracker分配(尽量满足数据本地化)的任务后，向HDFS获取Job资源(若数据是本地的，不需拷贝数据)。(9) 获取资源后，TaskTracker会开启JVM子进程运行任务。注：(3)中资源具体指什么？主要包含： ● 程序jar包、作业配置文件xml ● 输入划分信息，决定作业该启动多少个map任务 ● 本地文件，包含依赖的第三方jar包(-libjars)、依赖的归档文件(-archives)和普通文件(-files)，如果已经上传，则不需上传 3.MapReduce原理 MapReduce的执行步骤： 1、Map任务处理1.1 读取HDFS中的文件。每一行解析成一个&lt;k,v&gt;。每一个键值对调用一次map函数。 &lt;0,hello you&gt; &lt;10,hello me&gt;1.2 覆盖map()，接收1.1产生的&lt;k,v&gt;，进行处理，转换为新的&lt;k,v&gt;输出。 &lt;hello,1&gt; &lt;you,1&gt; &lt;hello,1&gt; &lt;me,1&gt; 1.3 对1.2输出的&lt;k,v&gt;进行分区。默认分为一个区。详见《Partitioner》1.4 对不同分区中的数据进行排序（按照k）、分组。分组指的是相同key的value放到一个集合中。 排序后： &lt;hello,1&gt; &lt;hello,1&gt; &lt;me,1&gt; &lt;you,1&gt;分组后：&lt;hello,{1,1}&gt;&lt;me,{1}&gt;&lt;you,{1}&gt; 1.5 （可选）对分组后的数据进行归约。详见《Combiner》2、Reduce任务处理 2.1 多个map任务的输出，按照不同的分区，通过网络copy到不同的reduce节点上。（shuffle）详见《shuffle过程分析》2.2 对多个map的输出进行合并、排序。覆盖reduce函数，接收的是分组后的数据，实现自己的业务逻辑， &lt;hello,2&gt; &lt;me,1&gt; &lt;you,1&gt; 处理后，产生新的&lt;k,v&gt;输出。 2.3 对reduce输出的&lt;k,v&gt;写到HDFS中。 4.测试MapReduce启动虚拟机利用Xshell工具连接 启动hadoop 1start-all.sh 上传到hdfs上一个文件test1文件test1的内容如下 123456hello worldhello lileihello haimeimeihello hadoophello girlhello girl 进入/usr/hadoop/hadoop-2.7.3/share/hadoop/mapreduce下 1cd /usr/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/ 有个 hadoop-mapreduce-examples-2.7.3.jar的jar包为test1执行这个jar包 1hadoop jar hadoop-mapreduce-examples-2.7.3.jar wordcount /test1 /result 等待执行,然后查看result文件内容 12345678910111213#查看result文件夹[root@master mapreduce]# hadoop fs -ls /resultFound 2 items-rw-r--r-- 1 root supergroup 0 2018-10-18 21:40 /result/_SUCCESS-rw-r--r-- 1 root supergroup 52 2018-10-18 21:40 /result/part-r-00000#查看part-r-00000内容[root@master mapreduce]# hadoop fs -cat /result/part-r-00000girl 2hadoop 1haimeimei 1hello 6lilei 1world 1 这个就是按照你所执行的文件,一次读取一行的内容,然后每行用空格分隔如图 5.Java代码实现首先eclipse建个Java项目,然后项目下建个lib文件夹放置jar包将复制到lib下,然后add to Build Path建立个mr包,在其中建立WCMapper,WCReducer,WordCount WCMapper 12345678910111213141516171819202122232425262728package com.hd.hadoop.mr;import java.io.IOException;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;//Mapper&lt;k1,v1,k2,v2&gt;//&lt;k1 long,v1 String&gt; &lt;k2 String,v2 long&gt;public class WCMapper extends Mapper&lt;LongWritable, Text, Text, LongWritable&gt; &#123; //重新一个map方法 @Override protected void map(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, LongWritable&gt;.Context context) throws IOException, InterruptedException &#123; // value代表文件中的每一行的数据 String line = value.toString(); //根据空格拆分字符串 String[] results = line.split(&quot; &quot;); //遍历数组得到每一个结果 for (String str : results) &#123; context.write(new Text(str), new LongWritable(1)); &#125; &#125;&#125; WCReducer 123456789101112131415161718192021222324252627package com.hd.hadoop.mr;import java.io.IOException;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Reducer;//k2 ,v2 k3 , v3public class WCReducer extends Reducer&lt;Text, LongWritable, Text, LongWritable&gt; &#123; //重写一个reduce方法 @Override protected void reduce(Text key2, Iterable&lt;LongWritable&gt; v2, Reducer&lt;Text, LongWritable, Text, LongWritable&gt;.Context context) throws IOException, InterruptedException &#123; //写自己的逻辑,统计单词个数 //定义一个遍历存放累加数据 long count=0; for (LongWritable lw : v2) &#123; count += lw.get(); &#125; //输出k3,v3 --&gt; String,Long context.write(key2, new LongWritable(count)); &#125; &#125; WordCount 1234567891011121314151617181920212223242526272829303132333435363738394041package com.hd.hadoop.mr;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class WordCount &#123; public static void main(String[] args) throws Exception &#123; // 1.获取job Configuration conf = new Configuration(); Job job = Job.getInstance(conf); // 2.指定job使用的类 job.setJarByClass(WordCount.class); // 3.设置Mapper的属性 job.setMapperClass(WCMapper.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(LongWritable.class); // 4.设置输入文件 FileInputFormat.setInputPaths(job, new Path(&quot;/test1&quot;)); // 5.设置reducer的属性 job.setReducerClass(WCReducer.class); job.setOutputKeyClass(Text.class); job.setMapOutputValueClass(LongWritable.class); // 6.设置输出文件夹,查看结果保存到hdfs文件夹中的位置 FileOutputFormat.setOutputPath(job, new Path(&quot;/result1&quot;)); // 7.提交 true 提交的时候打印日志信息 job.waitForCompletion(true); &#125;&#125; 然后打成jar包项目右键export 然后将打好的jar包利用Xftp工具放入到虚拟机master的/usr/tmp文件夹下cd /usr/tmp 然后执行1hadoop jar test1.jar 等待执行完毕 查看 123456789101112[root@master tmp]# hadoop fs -ls /result1Found 2 items-rw-r--r-- 1 root supergroup 0 2018-10-18 22:01 /result1/_SUCCESS-rw-r--r-- 1 root supergroup 55 2018-10-18 22:01 /result1/part-r-00000[root@master tmp]# hadoop fs -cat /result1/part-r-00000#与上面的测试执行结果相同,成功girl 2hadoop 1haimeimei 1hello 6lilei 1world 1 这就是MapReduce的原理和执行流程不清楚的话可以去多查看一些资料 接下来还会写一些别的示例还有Partitioner分区的用法]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Java</tag>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop之简单的hdfs上文件的上传删除及查询]]></title>
    <url>%2F2018%2F10%2F18%2Fblog181018-1%2F</url>
    <content type="text"><![CDATA[概览1.Linux上进行上传下载查询操作2.Java代码实现上传下载查询操作 上次将Hadoop集群版搭建完成了,那么怎么上传下载文件呢? 1.Linux上进行上传下载查询操作首先将Hadoop服务启动 将master,slave1,slave2三台虚拟机启动 在master上启动Hadoop服务 1start-all.sh 然后jps查看命令是否启动成功,和Hadoop集群搭建中验证方法一致 确认启动成功后,使用Xshell分别连接虚拟机 这时候需要你先将需要操作的文件传输到虚拟机中或者在虚拟机中创建文件 使用Xftp将文件上传到虚拟机中或者自己创建文件,我们就将文件放置在/usr/tmp中 1cd /usr/tmp 然后我们创建一个文本文件 1234#touch 创建文件 mkdir 创建文件夹touch test#编辑文件vim test 可以随便输入一些内容 123456hello worldhello lileihello haimeimeihello hadoophello girlhello girl 保存退出然后将文件上传到hdfs根目录中 12345678#将文件上传到Hadoop根目录中hadoop fs -put test /#查看是否上传成功hadoop fs -ls /#查看文件内容,发现与之前的内容相同则上传成功#如果你在安装虚拟机的时候没有选择中文,而在文件中有中文内容,有可能造成乱码hadoop fs -cat /test#hdfs上的文件是不支持修改的 文件上传成功了,那么接下来试一试文件夹 123456#创建文件夹mkdir testdir#上传hadoop fs -put testdir /#查看hadoop fs -ls / 删除文件/文件夹 123456#删除文件 hadoop fs -rm -f /test#删除文件夹hadoop fs -rm -r /testdir#查看hadoop fs -ls / 将hdfs上的文件下载到本地 1234567891011121314151617#将文件上传上去[root@master tmp]# hadoop fs -put test /#将test改名为test1[root@master tmp]# hadoop fs -mv /test /test1#查看文件内容[root@master tmp]# hadoop fs -cat /test1hello worldhello lileihello haimeimeihello hadoophello girlhello girl#文件下载[root@master tmp]# hadoop fs -get /test1#查看tmp文件夹下的内容[root@master tmp]# lstest test1 testdir 这就是几种基本的Linux上进行上传下载查询操作 2.Java代码实现上传下载查询操作启动eclipse或者其他工具新建个Java项目,test包,TestHadoop类 然后在项目下创建个lib文件夹存放jar包 将hadoop解压文件夹中的jar复制到lib内如图,大概共有69个jar包 然后在eclipse中选择lib文件夹下所有jar包–&gt;右键Build Path–&gt;add to Build Path 因为是测试类,所以我们将JUnit4导入到Path中 在项目上右键–&gt;Build Path–&gt;Confirgure Build Path 然后,这是TestHadoop的内容 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111package com.hd.test;import java.io.FileNotFoundException;import java.io.IOException;import java.net.URI;import java.net.URISyntaxException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileStatus;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.junit.Test;public class TestHadoop &#123; /** * 上传文件 * @throws IOException * @throws InterruptedException * @throws URISyntaxException */ @Test public void upLoad() throws IOException, InterruptedException, URISyntaxException&#123; // 创建config对象 Configuration conf = new Configuration(); // 创建FileSystem对象 192.168.134.154是你的主机master的ip地址,root是你的用户名 FileSystem fs = FileSystem.get(new URI(&quot;hdfs://192.168.134.154:9000&quot;), conf, &quot;root&quot;); // 把本地磁盘上的文件(这个文件可以自己选择)上传到hdfs上面的根目录上,这里可以改名 fs.copyFromLocalFile(false, new Path(&quot;d:/TABS.DBF&quot;), new Path(&quot;/abc.a&quot;)); // 关闭 fs.close(); &#125; /** * 下载 * @throws IOException * @throws InterruptedException * @throws URISyntaxException */ @Test public void downLoad() throws IOException, InterruptedException, URISyntaxException&#123; // 创建config对象 Configuration conf = new Configuration(); // 创建FileSystem对象 FileSystem fs = FileSystem.get(new URI(&quot;hdfs://192.168.134.154:9000&quot;), conf, &quot;root&quot;); // 把hdfs上面的根目录上的文件下载到本地磁盘上 fs.copyToLocalFile(false, new Path(&quot;/abc.a&quot;), new Path(&quot;E:/&quot;), true); // 关闭 fs.close(); &#125; /** * 删除 * @throws IOException * @throws InterruptedException * @throws URISyntaxException */ @Test public void del() throws IOException, InterruptedException, URISyntaxException&#123; // 创建config对象 Configuration conf = new Configuration(); // 创建FileSystem对象 FileSystem fs = FileSystem.get(new URI(&quot;hdfs://192.168.134.154:9000&quot;), conf, &quot;root&quot;); // 删除,false只能删除空文件夹 fs.delete(new Path(&quot;/abc.a&quot;), true); // 关闭 fs.close(); &#125; /** * 创建文件夹 * @throws IOException * @throws InterruptedException * @throws URISyntaxException */ @Test public void mkdir() throws IOException, InterruptedException, URISyntaxException&#123; // 创建config对象 Configuration conf = new Configuration(); // 创建FileSystem对象 FileSystem fs = FileSystem.get(new URI(&quot;hdfs://192.168.134.154:9000&quot;), conf, &quot;root&quot;); // 创建目录 fs.mkdirs(new Path(&quot;/a/b&quot;)); // 关闭 fs.close(); &#125; /** * 遍历查询输出 * @throws FileNotFoundException * @throws IllegalArgumentException * @throws IOException * @throws InterruptedException * @throws URISyntaxException */ @Test public void out() throws FileNotFoundException, IllegalArgumentException, IOException, InterruptedException, URISyntaxException&#123; // 创建config对象 Configuration conf = new Configuration(); // 创建FileSystem对象 FileSystem fs = FileSystem.get(new URI(&quot;hdfs://192.168.134.154:9000&quot;), conf, &quot;root&quot;); // 遍历 FileStatus[] status = fs.listStatus(new Path(&quot;/&quot;)); for (int i = 0; i &lt; status.length; i++) &#123; if (status[i].isFile()) &#123; System.err.println(&quot;文件:&quot; + status[i].getPath().toString()); &#125; else if (status[i].isDirectory()) &#123; System.err.println(&quot;目录:&quot; + status[i].getPath().toString()); &#125; &#125; // 关闭 fs.close(); &#125; &#125; 每次运行只要双击方法名然后右键Run –&gt;JUnit Test就能测试运行 然后分别在hdfs上 , 本地E盘 和 eclipse控制台 查看是否运行成功 以后会接着介绍更多的关于hadoop的操作]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop集群版搭建]]></title>
    <url>%2F2018%2F10%2F17%2Fblog181017-1%2F</url>
    <content type="text"><![CDATA[概览1.规划2.克隆虚拟机3.在hosts文件修改ip映射4.修改hadoop配置文件5.更改slaves文件6.集群版ssh免密钥登录7.重新格式化namenode8.启动hadoop并验证是否成功9.若slave的datanode没有启动 上次说了Hadoop集群单机版的搭建,这次来依照单机版的基础搭建一个简单的集群版 1.规划这次搭建的是一个主机和两个从机,也就是只有两个node节点,也可以让主机上有node节点,之后会说|主机名|cluster规划 ||–|–|| master | namenode,secondarynamenode,ResourceManager || slave1 | Datanode, NodeManager || slave2 | Datanode, NodeManager | 首先克隆一台单机版虚拟机 2.克隆虚拟机虚拟机右键–&gt;管理–&gt;克隆,选择创建完整克隆,这里克隆几个要看你创建的集群规模,我便克隆三台,一台主机,两台从机. 然后启动虚拟机,改动静态ip 这个需要三台虚拟机都改动 还是到/etc/sysconfig/network-scripts文件夹下改动ifcfg-ens33文件 1vim /etc/sysconfig/network-scripts/ifcfg-ens33 前面配置单机版的时候,如果觉得vi的命令不好用,可以安装vim命令 yum -y install vim之后就可以使用vim命令了.比vi编辑命令更加清晰 改动如图:将改为这个虚拟机的ip只要是你没有使用的即可然后保存,重启服务 1systemctl restart network 接下来使用Xshell工具分别连接三台虚拟机(没有的去官网下载,有免费版本),Xshell的优点在于你可以随意的复制粘贴命令语句 3.在hosts文件修改ip映射找到到hosts文件进行编辑 1vim /etc/hosts 写入三台主机的ip地址和主机名 123192.168.134.154 master192.168.134.155 slave1192.168.134.156 slave2 这里改动过之后最好重启虚拟机reboot,让其生效,这样最后配置ssh免密钥登录时不会出现异常 4.修改hadoop配置文件如果你的主机名还是单机版的.可以不用更改 然后进入到 /usr/hadoop/hadoop-2.7.3/etc/hadoop/下修改core-site.xml和yarn-site.xml , 三个虚拟机都要更改将之前的主机名改为现在的主机 master如图改为 将yarn-site.xml内的改为 当然,如果你的主机名还是用的单机版的,那么上述两步可以不改 接下来更改slaves文件 5.更改slaves文件还是在该文件夹下更改slaves文件三个虚拟机都需要更改(其实这些都可以在克隆前更改,再克隆,不过都开始了就算了o(∩_∩)o)1vim slaves 改为这里需要说明一下,如果你要主机也当作一个节点的话 , 那么在里面也要添加主机名称,这个文件就是告诉hadoop该启动谁的datanode 之后最重要的是ssh免密登录的配置 6.集群版ssh免密钥登录三台虚拟机都需要操作 进入到~/.ssh #每台机器先使用ssh执行以下，以在主目录产生一个.ssh 文件夹 ssh master 创建 ,然后进入 cd ~/.ssh 12345678910111213141516171819202122232425#输入以下命令，一路回车，用以产生公钥和秘钥[root@master .ssh]# ssh-keygen -t rsa -P &apos;&apos;#出现以下信息说明生成成功Generating public/private rsa key pair.Enter file in which to save the key (/root/.ssh/id_rsa): Your identification has been saved in /root/.ssh/id_rsa.Your public key has been saved in /root/.ssh/id_rsa.pub.The key fingerprint is:SHA256:6YO1h1emM9gcWvv9OT6ftHxLnjP9u8p25x1o30oq3No root@masterThe key&apos;s randomart image is:+---[RSA 2048]----+| || || || . || S o o || + O * . || . B.X. o.+.|| +o=+=**%|| .oEo*^^|+----[SHA256]-----+#将每台机器上的id_rsa.pub公钥内容复制到authorized_keys文件中[root@master .ssh]# cp id_rsa.pub authorized_keys 如果more authorized_keys 查看如图@后是你的主机名则表示正常,否则重新进行上几步进行覆写 然后分别把从机slave1和slave2的authorized_keys文件进行合并（最简单的方法是将其余三台slave主机的文件内容追加到master主机上） 12345678910111213141516171819202122#将所有的authorized_keys文件进行合并（最简单的方法是将其余三台slave主机的文件内容追加到master主机上）[root@slave1 .ssh]# cat ~/.ssh/authorized_keys | ssh root@master &apos;cat &gt;&gt; ~/.ssh/authorized_keys&apos;[root@slave2 .ssh]# cat ~/.ssh/authorized_keys | ssh root@master &apos;cat &gt;&gt; ~/.ssh/authorized_keys&apos;#查看master上的authorized_keys文件内容，类似如下即可[root@master .ssh]# more authorized_keys ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC5iw8+LlLxo0d77uaTChOKKJqfMHzp2jgzqV2hFAneFXqqWmrZ4/FrMUPenmdss19bP4Up9G7PGbJu29yZDvkDwlmuqnVajYyDOsCl7PPXPWXMIlxMGUHgSXLnQQi6QnWp04vJKDs0EbiRTd0ZYCSQefzJcZ8jbQ7bLYt6jtil7FfUupTdHTeexKKd8Mq3K7YFZHumKvhzs6wWiM+n41jANS083ss3OYmAdO2cU0w1BhLVvJhdzd6fNG3RXVCXI2v0XxCUHiqI9Oewl2qPOfKzeyy09bJxo371Ezjmt8GMrkA/Ecepkvx12qwNzC9bSPLfbnPWVo2gIxe4mMaFqCFJ root@masterssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC3CkB3Jejavt+yFphsbciktWciJmbcUBOv3ZLPVLW18ZxXGZKvG50EPXo/4By7P6IRW0wCa5YuckQEW+q6kmSatxqJ8e/K9a1mAk13N4f7V7M71Nn8IkujlF3gHYjKrmnEWpGJCyURzywIQTRArlIac1xj2SeM6q+gTMV9WrAKJupIRHli+W0kHVaYHNdKl7KMUT4KVrSl+h4wFwAd7Tcyj7JIbUcCCL6o/v/LqGFwpcJfbfUsuKJJho+tImh41j7mSXR8kRbTSZkcq5KX+iANrANwOHZ58tV5KXmMQjuVq7aJ985C16hHssB6zq/zjAxpxAyQIeE8Incc8U8ix root@slave1ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC//uaMbzbkYqxdgZJSdq+gdQYldzMQ7D3SxsUaNO5oVnVOszwbNnmL8vp1EUUehabQHPCAvCmLKUPXzfcxlyJEF/pnY77u4ySwsRVEpHvsDZbrclgCOrS6hW00sSx303KHLOgX70LfrmnohfUhvTxajzLXT+C8f5ZfTZ8meKD73HKl16jRwZQ8YhW9GUyuCkgQTGtKtTKPsRUd9LpAc/7/u8xvvvNvTYPxgyTJcUMzGSOHh8J3upI54ykY0FgBkjs1fCUaDalxAgsHw9B1iyx706WbcT6ymiQVMKGnnnM6k2KPvUvfD0swVfUSG+4ZsYSRHRTgWuiBbHoIr7DVd root@slave2 然后分发主机上的密钥 authorized_keys 12345#将master上的authorized_keys文件分发到其他主机上[root@master .ssh]# scp ~/.ssh/authorized_keys root@slave1:~/.ssh/[root@master .ssh]# scp ~/.ssh/authorized_keys root@slave2:~/.ssh/ 7.重新格式化namenode三台虚拟机都需要1hadoop namenode -format 8.启动hadoop并验证是否成功在主机master上直接启动start-all.sh从机会跟着启动 然后分别在主机从机上查看jps应该与规划相同|主机名|cluster规划 ||–|–|| master | namenode,secondarynamenode,ResourceManager || slave1 | Datanode, NodeManager || slave2 | Datanode, NodeManager | 9.若slave的datanode没有启动如果发现从机的datanode没有启动首先在主机master停止 stop-all.sh 然后进入到从机的/usr/local/hadoop/tmp/dfs/data cd /usr/local/hadoop/tmp/dfs/data 也就是hdfs-site.xml文件中dfs.datanode.data.dir的路径,data的存放位置,将其中的current删除 rm -rf current/ 然后重新初始化namenode 再重新启动hadoop即可 hadoop namenode -format start-all.sh 然后在http://主机master的ip:50070和http://主机master的ip:8088分别查看 若是如此便启动成功了 至此,简单的hadoop集群版搭建完成了 接下来会进行java代码操作hadoop文件上传下载删除的操作,看之后的blog]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop集群单机版搭建]]></title>
    <url>%2F2018%2F10%2F16%2Fblog181016-1%2F</url>
    <content type="text"><![CDATA[概览1.CentOS的安装2.设置Linux静态ip3.JDK的安装4.修改主机名和ip映射5.安装hadoop并修改配置文件6.格式化namenode7.启动hadoop8.验证是否启动成功9.设置ssh免密登录 首先本文是基于CentOS 7 , jdk1.8.0_141 和Hadoop2.7.3环境搭建 1.CentOS的安装 首先准备好CentOS7 64位的镜像 然后在VMware上安装虚拟机 这里注意选择镜像自动检测CentOS 64位, 不然之后步骤比较麻烦其他步骤都与普通安装虚拟机一样,直接默认下一步,然后开启虚拟机 这里直接进行回车继续即可 语言选择可以选择简体中文这时选择安装位置,直接点击完成即可,这样才能继续下一步操作 然后开始安装,设置并且设置root密码然后重新开机登录root账号即可,成功登录则表示CentOS安装成功 2.设置Linux静态ip 开机完成后需要我们设置静态ip,这样之后开机都不需要dhclient动态分配ip地址 首先找到/etc/sysconfig/network-scripts/下的ifcfg-ens33配置文件(如果没有找到此文件,说明你没有选择安装CentOS64位系统,建议重新安装) 1vi /etc/sysconfig/network-scripts/ifcfg-ens33 2.首先把BOOTPROTO=”dhcp”改成BOOTPROTO=”static”表示静态获取,然后把UUID注释掉,把ONBOOT=no改为yes表示开机自动静态获取,然后在最后追加比如下面的配置： 1234IPADDR=192.168.134.151 #自己的ip地址NETMASK=255.255.255.0GATEWAY=192.168.134.2DNS1=8.8.8.8 IPADDR就是静态IP，NETMASK是子网掩码，GATEWAY就是网关或者路由地址；需要说明，原来还有个NETWORK配置的是局域网网络号，这个是ifcalc自动计算的，所以这里配置这些就足够了，最终配置如下图： 如果不知道自己的GATEWAY可以去虚拟机的编辑查看虚拟网络编辑器中的NAT设置 最后保存退出 重启服务 centos6的网卡重启方法：service network restart centos7的网卡重启方法：systemctl restart network 然后查看自己的ip地址是不是自己设置的 centos6的查看ip方法: ifconfig centos7的查看ip方法: ip addr 接下来可以用Xshell工具连接虚拟机了,这样比较好操作(如果没有此工具的需要去官网下载,Xshell和配套的文件传输工具Xftp,都有免费版本) 3.JDK的安装 首先在usr的目录下创建一个java文件夹用来存放jdk的安装包并作为安装路径 这时候新建文件传输,将jdk的压缩包放入到CentOS中 然后解压jdk 解压完成之后返回到根目录的etc文件夹下,改写profile配置文件 vi /etc/profile 在profile最后加上: 1234export JAVA_HOME=/usr/java/jdk1.8.0_141export JAVA_BIN=/usr/java/jdk1.8.0_141/binexport PATH=$PATH:$JAVA_HOME/binexport CLASSPATH=:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar 保存退出,source /etc/profile刷新配置文件用java -version看看jdk环境是否配置完毕 4.修改主机名和ip映射修改etc文件夹下的hosts文件vi /etc/hosts如图: 5.安装hadoop并修改配置文件 在usr文件夹下创建hadoop文件夹作为压缩包存放和解压路径,将hadoop的压缩包传输到此文件夹下 然后解压 tar -zxf hadoop-2.7.3.tar.gz 5.1配置proflie文件注意：hadoop2.x的配置文件$HADOOP_HOME/etc/hadoop在etc的profile最后添加,然后source刷新配置文件 12export HADOOP_HOME=/usr/hadoop/hadoop-2.7.3export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin 接下来需要改五个配置文件5.2 第一个：hadoop-env.sh进入到hadoop-2.7.3/etc/hadoop文件夹下,修改hadoop-env.sh vi hadoop-env.sh第25行将改为保存退出5.3 第二个 core-site.xml在configuration中加上 12345678910&lt;!-- 制定HDFS的老大（NameNode）的地址 --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://zhiyou:9000&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定hadoop运行时产生文件的存储目录[能自动生成目录] --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/zhiyou/hadoop/tmp&lt;/value&gt; &lt;/property&gt; 保存退出 5.4 第三个 hdfs-site.xml同上configuration中添加 12345678910111213&lt;!-- 指定HDFS副本的数量 --&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;/usr/local/hadoop/tmp/dfs/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;/usr/local/hadoop/tmp/dfs/data&lt;/value&gt; &lt;/property&gt; 5.5 第四个mapred-site.xml 这个是需要你复制一个模版文件出来的 cp mapred-site.xml.template mapred-site.xml然后 vi mapred-site.xml添加 12345&lt;!-- 指定mr运行在yarn上 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; 保存退出 5.6 第五个 yarn-site.xml 12345678910&lt;!-- 指定YARN的老大（ResourceManager）的地址 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;zhiyou&lt;/value&gt; &lt;/property&gt; &lt;!-- reducer获取数据的方式 --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; 6.格式化namenode是对namenode进行初始化 1hadoop namenode -format 如果没有报错说明配置文件成功,否则重新检查配置文件 7.启动hadoop 先启动HDFS 1start-dfs.sh 再启动YARN 1start-yarn.sh 这里需要yes三次并输入你的root密码三次 8.验证是否启动成功jps 3912 DataNode4378 Jps4331 NodeManager4093 SecondaryNameNode3822 NameNode4239 ResourceManager 关闭防火墙 #停止firewall systemctl stop firewalld systemctl disable firewalld.service #禁止firewall开机启动 浏览器查看 http://ip地址:50070 （HDFS管理界面） http://ip地址:8088 (yarn管理界面）如果页面正常则说明hadoop配置成功 有人觉得每次启动都需要输入密码很繁琐,那么就设置ssh免密登录 9.设置ssh免密登录生成ssh免登陆密钥 #进入到我的home目录 cd ~/.ssh ssh-keygen -t rsa （四个回车）执行完这个命令后，会生成两个文件id_rsa（私钥）、id_rsa.pub（公钥） 将公钥拷贝到要免登陆的机器上1ssh-copy-id 192.168.134.151 然后重新启动虚拟机reboot 进入到hadoop下的sbin文件夹 1cd /usr/hadoop/hadoop-2.7.3/sbin 有个start-all.sh和stop-all.sh,这是启动和停止所有服务,这样更加快捷 启动之后再次验证是否启动成功 至此,基本的搭建已经完成]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[海琴烟]]></title>
    <url>%2F2018%2F10%2F15%2Ftest%2F</url>
    <content type="text"><![CDATA[]]></content>
  </entry>
  <entry>
    <title><![CDATA[你好]]></title>
    <url>%2F2018%2F10%2F15%2Fhello%2F</url>
    <content type="text"><![CDATA[你好,欢迎来到我的blog.]]></content>
  </entry>
</search>
