<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Oracle根据字段排序问题]]></title>
    <url>%2F2019%2F05%2F24%2Fblog190524-1%2F</url>
    <content type="text"><![CDATA[Oracle根据字段排序,如果字段类型是varchar2字符串,会导致混乱解决方法:将varchar2转为数字(使用to_number()函数)或者将表的字段类型更改为number Oracle排序发生混乱即使是number类型数据排序也会发生混乱解决方法:加上一个唯一值的排序例如:select t.* from table1 t order by t.px(可重复),t.id(唯一约束)]]></content>
      <categories>
        <category>Oracle</category>
      </categories>
      <tags>
        <tag>Oracle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark的安装部署]]></title>
    <url>%2F2018%2F12%2F13%2Fblog181213-1%2F</url>
    <content type="text"><![CDATA[概览1.安装scala2.单机版的安装部署3.spark集群的安装部署4.基于Zookeeper的Spark高可用集群 准备CentOS7scala-2.10.5spark-2.1.1-bin-hadoop2.7hadoop高可用集群(或普通集群)工具:VMware12 Xshell5 Xftp5 1.安装scala因为spark需要scala的环境,所以需要先安装scala 启动虚拟机,利用Xshell连接,使操作更加方便,然后使用Xftp连接以便上传文件 首先在/usr路径下创建一个scala文件夹当作安装目录以及安装包的上传路径 12345678910[root@master ~]# cd /usr/[root@master usr]# lsbin flume hadoop hive java lib libexec sbin sqoop tmp zookeeper.outetc games hbase include kafka lib64 local share src zookeeper[root@master usr]# mkdir scala[root@master usr]# lsbin flume hadoop hive java lib libexec sbin share src zookeeperetc games hbase include kafka lib64 local scala sqoop tmp zookeeper.out[root@master usr]# cd scala/[root@master scala]# 利用Xftp上传scala的压缩包到此路径下解压 1[root@master scala]# tar -zxf scala-2.10.5.tgz 配置环境变量 1[root@master scala]# vim /etc/profile 在最后添加上两行 123#scala的安装目录export SCALA_HOME=/usr/scala/scala-2.10.5export PATH=$PATH:$SCALA_HOME/bin esc+:wq保存退出刷新环境变量 1[root@master scala]# source /etc/profile 验证是否安装成功 12[root@master scala]# scala -versionScala code runner version 2.10.5 -- Copyright 2002-2013, LAMP/EPFL 2.单机版的安装部署在/usr下创建一个spark文件夹当作安装目录和安装包上传路径 1234[root@master scala]# cd ..[root@master usr]# mkdir spark[root@master usr]# cd spark/[root@master spark]# 还是通过Xftp将spark安装包上传到此路径下 解压 1[root@master spark]# tar -zxf spark-2.1.1-bin-hadoop2.7.tgz 配置环境变量 1[root@master spark]# vim /etc/profile 在最后添加两行 12export SPARK_HOME=/usr/spark/spark-2.1.1-bin-hadoop2.7export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin 保存退出刷新环境变量 1[root@master spark]# source /etc/profile 测试运行一个简单的spark程序spark-shell 可以在/tmp文件夹下创建一个测试文件先复制个Xshell窗口 在新的窗口操作 12[root@master ~]# cd /tmp/[root@master tmp]# vim a.log 我们在a.log中创建一些假数据 1234hello worldhello sparkhello boyhello girl 然后在原来的窗口操作 123456scala&gt; sc.textFile(&quot;/tmp/a.log&quot;).flatMap(_.split(&quot; &quot;)).map((_, 1)).reduceByKey(_+_).collect.foreach(println)(spark,1)(girl,1)(hello,4)(boy,1)(world,1) 会出现如上的结果这说明我们的单机版spark安装成功了退出spark操作页面 1scala&gt; :q 接下来会在单机版的基础上安装spark集群 3.spark集群的安装部署首先我们需要在master主机上配置好单机版spark 集群的搭建需要我们预先搭建好hadoop高可用集群(或普通集群)才行 集群规划 首先我们在已经安装好单机spark的master上继续配置 修改spark-env .sh 记住先停止单机版的spark查看jps进程,如果还在运行就强制杀死进程 修改spark-env.sh文件 1234[root@master spark]# cd /usr/spark/spark-2.1.1-bin-hadoop2.7/conf/[root@master conf]# lsdocker.properties.template log4j.properties.template slaves.template spark-env.sh.templatefairscheduler.xml.template metrics.properties.template spark-defaults.conf.template 我们会发现没有这个文件,但是却有一个spark-env.sh.template文件,利用这个模版复制出来一个spark-env .sh 1[root@master conf]# cp spark-env.sh.template spark-env.sh 修改 1[root@master conf]# vim spark-env.sh 在最后添加这几行 123456789101112131415#Java的安装路径export JAVA_HOME=/usr/java/jdk1.8.0_141#scala安装路径export SCALA_HOME=/usr/scala/scala-2.10.5#master的IP地址或者配置过的主机映射名export SPARK_MASTER_IP=master#端口号export SPARK_MASTER_PORT=7077#核心线程export SPARK_WORKER_CORES=1export SPARK_WORKER_INSTANCES=1#设置处理内存export SPARK_WORKER_MEMORY=1g#Hadoop的路径export HADOOP_CONF_DIR=/usr/hadoop/hadoop-2.7.3/etc/hadoop 保存退出 配置slaves文件 依然拷贝出来一个slaves配置文件 1234[root@master conf]# lsdocker.properties.template log4j.properties.template slaves.template spark-env.shfairscheduler.xml.template metrics.properties.template spark-defaults.conf.template spark-env.sh.template[root@master conf]# cp slaves.template slaves 修改 1[root@master conf]# vim slaves 配置成如下 修改启动文件 为了避免和hadoop中的start/stop-all.sh脚本发生冲突，将spark/sbin/的start/stop-all.sh重命名 123[root@master conf]# cd /usr/spark/spark-2.1.1-bin-hadoop2.7/sbin/[root@master sbin]# mv start-all.sh start-spark-all.sh[root@master sbin]# mv stop-all.sh stop-spark-all.sh 发送将配置好的scala和spark发送给其他主机 123456789#发送scala[root@master usr]# scp -r /usr/scala root@slave1:/usr/[root@master usr]# scp -r /usr/scala root@slave2:/usr/#发送spark[root@master usr]# scp -r /usr/spark root@slave1:/usr/[root@master usr]# scp -r /usr/spark root@slave2:/usr/#发送环境变量配置[root@master usr]# scp -r /etc/profile root@slave1:/etc[root@master usr]# scp -r /etc/profile root@slave2:/etc 分别在slave1和slave2上刷新环境变量 12[root@slave1 ~]# source /etc/profile[root@slave2 ~]# source /etc/profile 再安装上面验证scala是否成功安装 启动 ①启动Zookeeper集群分别在三台主机上启动 1zkServer.sh start 启动之后查看状态 1zkServer.sh status 如果是leader或者follower便启动成功 ②启动hadoop集群在master主节点上启动hadoop集群 1start-all.sh 查看jpsmaster 12345678[root@master usr]# jps2936 DataNode3656 Jps2682 QuorumPeerMain3131 JournalNode3275 DFSZKFailoverController2829 NameNode3518 NodeManager slave112345678[root@slave1 ~]# jps2338 DataNode2626 NodeManager2515 DFSZKFailoverController2771 Jps2436 JournalNode2268 NameNode2207 QuorumPeerMain slave2 1234567[root@slave2 ~]# jps2752 Jps2641 ResourceManager2502 NodeManager2234 QuorumPeerMain2412 JournalNode2317 DataNode 如果少启动尝试各自启动其功能,例如ResourceManager没启动可以在slave2上使用start-yarn.sh启动,或者重新启动所有服务 ③启动spark集群在职责为master的主机上启动 start-spark-all.sh 如果启动出错请仔细查看异常,检查配置文件是否配置出错 测试 我们将之前的在tmp上创建的a.log上传到hdfs上 12345[root@master usr]# hadoop fs -put /tmp/a.log /[root@master usr]# hadoop fs -ls /Found 2 items-rw-r--r-- 3 root supergroup 45 2018-12-10 21:31 /a.logdrwx-wx-wx - root supergroup 0 2018-10-23 22:24 /tmp 然后在master上启动spark-shell 1[root@master usr]# spark-shell 如果启动出错请仔细查看异常,检查配置文件是否配置出错 然后输入scala命令 12345678#这里的master:9000是hadoop中active的namenode节点#如果有疑问可以去查看hadoop的高可用集群搭建scala&gt; sc.textFile(&quot;hdfs://master:9000/a.log&quot;).flatMap(_.split(&quot; &quot;)).map((_, 1)).reduceByKey(_+_).collect.foreach(println)(spark,1)(girl,1)(hello,4)(boy,1)(world,1) 如果得出以上结果便正面spark集群正常你可以访问master的IP地址:8080查看spark 4.基于Zookeeper的Spark高可用集群spark高可用是基于普通集群版的基础上搭建的 ①修改spark-env. sh 注释掉spark-env.sh中的两行 12[root@master spark]# cd /usr/spark/spark-2.1.1-bin-hadoop2.7/conf/[root@master spark]# vim spark-env.sh 将 12#export SPARK_MASTER_IP=master#export SPARK_MASTER_PORT=7077 注释掉在最后添加一行 1export SPARK_DAEMON_JAVA_OPTS=&quot;-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=master:2181,slave1:2181,slave2:2181 -Dspark.deploy.zookeeper.dir=/spark&quot; 解释 spark.deploy.recoveryMode设置成 ZOOKEEPER spark.deploy.zookeeper.url ZooKeeper URL spark.deploy.zookeeper.dir ZooKeeper 保存恢复状态的目录，缺省为 /spark ②重启集群首先确保Zookeeper和hadoop集群的启动,防火墙的关闭 在任何一台spark节点上启动start-spark-all. sh 手动在集群中其他从节点上再启动master进程：在slave1上启动sbin/start-master.sh ③测试通过浏览器方法 master:8080 /slave1:8080–&gt;Status: STANDBY Status: ALIVE验证HA高可用，只需要手动停掉master上spark进程Master，等一会slave1上的进程Master状态会从STANDBY变成ALIVE]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Zookeeper</tag>
        <tag>Spark</tag>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[简单说明什么是Kafka]]></title>
    <url>%2F2018%2F11%2F20%2Fblog181120-3%2F</url>
    <content type="text"><![CDATA[Kafka通俗理解Apache kafka是消息中间件的一种，我发现很多人不知道消息中间件是什么，在开始学习之前，我这边就先简单的解释一下什么是消息中间件，只是粗略的讲解，目前kafka已经可以做更多的事情。 举个例子，生产者消费者，生产者生产鸡蛋，消费者消费鸡蛋，生产者生产一个鸡蛋，消费者就消费一个鸡蛋，假设消费者消费鸡蛋的时候噎住了（系统宕机了），生产者还在生产鸡蛋，那新生产的鸡蛋就丢失了。再比如生产者很强劲（大交易量的情况），生产者1秒钟生产100个鸡蛋，消费者1秒钟只能吃50个鸡蛋，那要不了一会，消费者就吃不消了（消息堵塞，最终导致系统超时），消费者拒绝再吃了，”鸡蛋“又丢失了，这个时候我们放个篮子在它们中间，生产出来的鸡蛋都放到篮子里，消费者去篮子里拿鸡蛋，这样鸡蛋就不会丢失了，都在篮子里，而这个篮子就是”kafka“。鸡蛋其实就是“数据流”，系统之间的交互都是通过“数据流”来传输的（就是tcp、http什么的），也称为报文，也叫“消息”。消息队列满了，其实就是篮子满了，”鸡蛋“ 放不下了，那赶紧多放几个篮子，其实就是kafka的扩容。各位现在知道kafka是干什么的了吧，它就是那个”篮子”。 kafka名词解释后面大家会看到一些关于kafka的名词，比如topic、producer、consumer、broker，我这边来简单说明一下。 producer：生产者，就是它来生产“鸡蛋”的。 consumer：消费者，生出的“鸡蛋”它来消费。 topic：你把它理解为标签，生产者每生产出来一个鸡蛋就贴上一个标签（topic），消费者可不是谁生产的“鸡蛋”都吃的，这样不同的生产者生产出来的“鸡蛋”，消费者就可以选择性的“吃”了。 broker：就是篮子了。 大家一定要学会抽象的去思考，上面只是属于业务的角度，如果从技术角度，topic标签实际就是队列，生产者把所有“鸡蛋（消息）”都放到对应的队列里了，消费者到指定的队列里取。 —-w3c对kafka介绍https://www.w3cschool.cn/apache_kafka/apache_kafka_introduction.html]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka集群安装]]></title>
    <url>%2F2018%2F11%2F20%2Fblog181120-2%2F</url>
    <content type="text"><![CDATA[概览1.上传解压2.修改配置文件3.分发到其他节点下4.启动5.测试6.注意 准备安装好ZookeeperJDK版本:1.8.0_141Kafka版本:kafka_2.12-1.1.0工具:Xshell 5,Xftp 5 1.上传解压首先在master(随意一台)的主机上的/usr下创建kafka文件夹作为安装路径 1234[root@master ~]# cd /usr/[root@master usr]# lsbin etc flume games hadoop hbase hive include java lib lib64 libexec local sbin share sqoop src tmp zookeeper[root@master usr]# mkdir kafka 然后利用Xftp将压缩包上传到/usr/kafka下并解压 1[root@master usr]# cd kafka/ 123[root@master kafka]# lskafka_2.12-1.1.0.tgz[root@master kafka]# tar -zxf kafka_2.12-1.1.0.tgz 2.修改配置文件修改 /usr/kafka/kafka_2.12-1.1.0/config/server.properties 12[root@master kafka]# cd kafka_2.12-1.1.0/config/[root@master config]# vim server.properties 只修改broker.id和zookeeper就行将修改为broker.id每台主机上都不一样 将修改为保存退出 这是比较详细的配置(其实只要按照上面的更改即可) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162#broker的全局唯一编号，不能重复broker.id=01 #用来监听链接的端口，producer或consumer将在此端口建立连接port=9092 #处理网络请求的线程数量num.network.threads=3 #用来处理磁盘IO的线程数量num.io.threads=8 #发送套接字的缓冲区大小socket.send.buffer.bytes=102400 #接受套接字的缓冲区大小socket.receive.buffer.bytes=102400 #请求套接字的缓冲区大小socket.request.max.bytes=104857600 #kafka消息存放的路径log.dirs=/home/servers-kafka/logs/kafka #topic在当前broker上的分片个数num.partitions=2 #用来恢复和清理data下数据的线程数量num.recovery.threads.per.data.dir=1 #segment文件保留的最长时间，超时将被删除log.retention.hours=168 #滚动生成新的segment文件的最大时间log.roll.hours=168 #日志文件中每个segment的大小，默认为1Glog.segment.bytes=1073741824 #周期性检查文件大小的时间log.retention.check.interval.ms=300000 #日志清理是否打开log.cleaner.enable=true #broker需要使用zookeeper保存meta数据zookeeper.connect=master:2181,slave1:2181,slave2:2181 #zookeeper链接超时时间zookeeper.connection.timeout.ms=6000 #partion buffer中，消息的条数达到阈值，将触发flush到磁盘log.flush.interval.messages=10000 #消息buffer的时间，达到阈值，将触发flush到磁盘log.flush.interval.ms=3000 #删除topic需要server.properties中设置delete.topic.enable=true否则只是标记删除delete.topic.enable=true #此处的host.name为本机IP(重要),如果不改,则客户端会抛出:Producerconnection to localhost:9092 unsuccessful 错误!host.name=master 3.分发到其他节点下123[root@master config]# scp -r /usr/kafka/ root@slave1:/usr/[root@master config]# scp -r /usr/kafka/ root@slave2:/usr/ 然后分别在slave1和slave2上更改broker id 1vim /usr/kafka/kafka_2.12-1.1.0/config/server.properties 将broker id改为02和03 4.启动确保防火墙关闭 首先全部启动Zookeeper,参考Zookeeper安装中的启动 再全部启动kafka 1234# 由于没配置环境变量,所以需要进入安装目录下启动cd /usr/kafka/kafka_2.12-1.1.0# 后台启动,最后加&amp;./bin/kafka-server-start.sh -daemon config/server.properties &amp; 查看jps 5.测试在master上创建topic-test 12[root@master kafka_2.12-1.1.0]# ./bin/kafka-topics.sh --create --zookeeper master:2181,slave1:2181,slave2:2181 --replication-factor 3 --partitions 3 --topic testCreated topic &quot;test&quot;. 在master,slave1,2上查看已创建的topic列表 1./bin/kafka-topics.sh --list --zookeeper localhost:2181 在master上启动生产者 1[root@master kafka_2.12-1.1.0]# ./bin/kafka-console-producer.sh --broker-list master:9092,slave1:9092,slave2:9092 --topic test 随意输入一些内容 在其他节点上启动控制台消费者 1./bin/kafka-console-consumer.sh --bootstrap-server master:9092,slave1:9092,slave2:9092 --from-beginning --topic test 会出现 --from-beginning如果去掉不会出现消费者启动之前的消息 测试:在生产者输入内容,消费者查看消息 删除主题: 1./bin/kafka-topics.sh --delete --zookeeper localhost:2181 --topic test 6.注意 如果kafaka启动时加载的配置文件中server.properties没有配置delete.topic.enable=true，那么此时的删除并不是真正的删除，而是把topic标记为：marked for deletion 你可以通过命令：./bin/kafka-topics –zookeeper 【zookeeper server】 –list来查看所有topic 此时你若想真正删除它，可以如下操作： （1）登录zookeeper客户端：命令：./bin/zookeeper-client （2）找到topic所在的目录：ls /brokers/topics （3）找到要删除的topic，执行命令：rmr /brokers/topics/【topic name】即可，此时topic被彻底删除。 另外被标记为marked for deletion的topic你可以在zookeeper客户端中通过命令获得：ls /admin/delete_topics/【topic name】， 如果你删除了此处的topic，那么marked for deletion 标记消失 zookeeper 的config中也有有关topic的信息： ls /config/topics/【topic name】暂时不知道有什么用]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Zookeeper</tag>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flume多节点集群搭建]]></title>
    <url>%2F2018%2F11%2F20%2Fblog181120-1%2F</url>
    <content type="text"><![CDATA[概览1.Flume流程简介2.规划3.配置4.启动测试5.注意 准备操作系统:CentOS 7搭建好hadoop集群Flume版本:1.8.0jdk版本:1.8.0_141工具:Xshell 5,Xftp 5,VMware Workstation Pro 1.Flume流程简介Flume NG是一个分布式、可靠、可用的系统，它能够将不同数据源的海量日志数据进行高效收集、聚合，最后存储到一个中心化数据存储系统中,方便进行数据分析。事实上flume也可以收集其他信息，不仅限于日志。由原来的Flume OG到现在的Flume NG，进行了架构重构，并且现在NG版本完全不兼容原来的OG版本。相比较而言，flume NG更简单更易于管理操作。Flume OG:Flume original generation 即Flume 0.9.x版本Flume NG:Flume next generation 即Flume 1.x版本。Flume NG用户参考手册：http://flume.apache.org/FlumeUserGuide.html# 简单比较一下两者的区别：OG有三个组件agent、collector、master，agent主要负责收集各个日志服务器上的日志，将日志聚合到collector，可设置多个collector，master主要负责管理agent和collector，最后由collector把收集的日志写的HDFS中，当然也可以写到本地、给storm、给Hbase。 NG最大的改动就是不再有分工角色设置，所有的都是agent，可以彼此之间相连，多个agent连到一个agent，此agent也就相当于collector了，NG也支持负载均衡. 2.规划三台主机的主机名分别为master,slave1,slave2(防火墙已关闭) 由slave1和slave2收集日志信息,传给master,再由master上传到hdfs上 3.配置3.1上传解压在slave1上的usr文件夹下新建个flume文件夹,用作安装路径 1234[root@slave1 usr]# mkdir flume[root@slave1 usr]# lsbin etc flume games hadoop hbase include java lib lib64 libexec local sbin share sqoop src tmp zookeeper[root@slave1 usr]# cd flume/ 利用Xftp工具将flume压缩包上传到usr/flume文件夹下,解压 123[root@slave1 flume]# lsapache-flume-1.8.0-bin.tar.gz[root@slave1 flume]# tar -zxf apache-flume-1.8.0-bin.tar.gz 3.2 .配置flume-env.sh文件 12345678910# 进入到conf文件夹下[root@slave1 flume]# cd apache-flume-1.8.0-bin/conf/[root@slave1 conf]# lsflume-conf.properties.template flume-env.ps1.template flume-env.sh.template log4j.properties# 拷贝出来一个flume-env.sh文件[root@slave1 conf]# cp flume-env.sh.template flume-env.sh[root@slave1 conf]# lsflume-conf.properties.template flume-env.ps1.template flume-env.sh flume-env.sh.template log4j.properties# 修改flume-env.sh文件[root@slave1 conf]# vim flume-env.sh 将java的安装路径修改为自己的我的是/usr/java/jdk1.8.0_141 3.3 配置slave.conf文件在conf下创建一个新的slave.conf文件 1234#创建[root@slave1 conf]# touch slave.conf#修改[root@slave1 conf]# vim slave.conf 写入配置内容 123456789101112131415161718192021222324252627# 主要作用是监听目录中的新增数据，采集到数据之后，输出到avro （输出到agent）# 注意：Flume agent的运行，主要就是配置source channel sink# 下面的a1就是agent的代号，source叫r1 channel叫c1 sink叫k1a1.sources = r1a1.sinks = k1a1.channels = c1#具体定义source a1.sources.r1.type = spooldir#先创建此目录，保证里面空的 a1.sources.r1.spoolDir = /logs #对于sink的配置描述 使用avro日志做数据的消费a1.sinks.k1.type = avro# hostname是最终传给的主机名称或者ip地址a1.sinks.k1.hostname = mastera1.sinks.k1.port = 44444#对于channel的配置描述 使用文件做数据的临时缓存 这种的安全性要高a1.channels.c1.type = filea1.channels.c1.checkpointDir = /home/uplooking/data/flume/checkpointa1.channels.c1.dataDirs = /home/uplooking/data/flume/data#通过channel c1将source r1和sink k1关联起来a1.sources.r1.channels = c1a1.sinks.k1.channel = c1 保存退出3.4 将flume发送到其他主机 12[root@slave1 conf]# scp -r /usr/flume/ root@slave2:/usr/[root@slave1 conf]# scp -r /usr/flume/ root@master:/usr/ 3.5 修改master中flume的配置在master的flume的conf文件夹下创建一个master.conf文件 1[root@master conf]# vim master.conf 写入配置信息 12345678910111213141516171819202122232425262728293031323334353637383940414243# 获取slave1,2上的数据，聚合起来，传到hdfs上面# 注意：Flume agent的运行，主要就是配置source channel sink# 下面的a1就是agent的代号，source叫r1 channel叫c1 sink叫k1a1.sources = r1a1.sinks = k1a1.channels = c1#对于source的配置描述 监听avroa1.sources.r1.type = avro# hostname是最终传给的主机名称或者ip地址a1.sources.r1.bind = mastera1.sources.r1.port = 44444#定义拦截器，为消息添加时间戳 a1.sources.r1.interceptors = i1 a1.sources.r1.interceptors.i1.type = org.apache.flume.interceptor.TimestampInterceptor$Builder#对于sink的配置描述 传递到hdfs上面a1.sinks.k1.type = hdfs #集群的nameservers名字#单节点的直接写：hdfs://主机名(ip):9000/xxx#ns是hadoop集群名称a1.sinks.k1.hdfs.path = hdfs://ns/flume/%Y%m%d a1.sinks.k1.hdfs.filePrefix = events- a1.sinks.k1.hdfs.fileType = DataStream #不按照条数生成文件 a1.sinks.k1.hdfs.rollCount = 0 #HDFS上的文件达到128M时生成一个文件 a1.sinks.k1.hdfs.rollSize = 134217728 #HDFS上的文件达到60秒生成一个文件 a1.sinks.k1.hdfs.rollInterval = 60 #对于channel的配置描述 使用内存缓冲区域做数据的临时缓存a1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100#通过channel c1将source r1和sink k1关联起来a1.sources.r1.channels = c1a1.sinks.k1.channel = c1 保存退出 4.启动测试确认防火墙关闭首先启动Zookeeper和hadoop集群,参考hadoop集群搭建中的启动 然后先启动master上的flume(如果先启动slave上的会导致拒绝连接) 在apache-flume-1.8.0-bin目录下启动(因为没有配置环境变量) 1[root@master apache-flume-1.8.0-bin]# bin/flume-ng agent -n a1 -c conf -f conf/master.conf -Dflume.root.logger=INFO,console 如此便是启动成功如果想后台启动(这样可以不用另开窗口操作) 12# 命令后加&amp;[root@master apache-flume-1.8.0-bin]# bin/flume-ng agent -n a1 -c conf -f conf/master.conf -Dflume.root.logger=INFO,console &amp; 再启动slave1,2上的flume首先在slave1,2的根目录创建logs目录 12[root@slave1 apache-flume-1.8.0-bin]# cd /[root@slave1 /]# mkdir logs 不然会报错 12345678910111213141516[ERROR - org.apache.flume.lifecycle.LifecycleSupervisor$MonitorRunnable.run(LifecycleSupervisor.java:251)] Unable to start EventDrivenSourceRunner: &#123; source:Spool Directory source r1: &#123; spoolDir: /logs &#125; &#125; - Exception follows.java.lang.IllegalStateException: Directory does not exist: /logs at com.google.common.base.Preconditions.checkState(Preconditions.java:145) at org.apache.flume.client.avro.ReliableSpoolingFileEventReader.&lt;init&gt;(ReliableSpoolingFileEventReader.java:159) at org.apache.flume.client.avro.ReliableSpoolingFileEventReader.&lt;init&gt;(ReliableSpoolingFileEventReader.java:85) at org.apache.flume.client.avro.ReliableSpoolingFileEventReader$Builder.build(ReliableSpoolingFileEventReader.java:777) at org.apache.flume.source.SpoolDirectorySource.start(SpoolDirectorySource.java:107) at org.apache.flume.source.EventDrivenSourceRunner.start(EventDrivenSourceRunner.java:44) at org.apache.flume.lifecycle.LifecycleSupervisor$MonitorRunnable.run(LifecycleSupervisor.java:249) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308) at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180) at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) 1234567#slave1[root@slave1 /]# cd /usr/flume/apache-flume-1.8.0-bin[root@slave1 apache-flume-1.8.0-bin]# bin/flume-ng agent -n a1 -c conf -f conf/slave.conf -Dflume.root.logger=INFO,console#slave2[root@slave2 /]# cd /usr/flume/apache-flume-1.8.0-bin[root@slave2 apache-flume-1.8.0-bin]# bin/flume-ng agent -n a1 -c conf -f conf/slave.conf -Dflume.root.logger=INFO,console 测试 启动成功后(如果没有后台启动另开个窗口继续下面操作) 在slave1的usr/tmp文件夹下新建个test文件 1[root@slave1 tmp]# vim test 随便写入一些内容 12helloworldtest 保存退出将其复制到logs文件夹下 1[root@slave1 tmp]# cp test /logs/ 查看master登录http://(hadoop中active状态的namenode节点IP):50070/explorer.html# 如此便是flume多节点集群搭建完成 5.注意登录查看需要是active的节点地址,具体参考hadoop集群搭建中的测试 在启动slave上的flume前要先建立logs文件夹,也就是flume安装路径/conf下的slave.conf文件中的]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Flume</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flume采集日志到hadoop存储]]></title>
    <url>%2F2018%2F11%2F19%2Fblog181119-2%2F</url>
    <content type="text"><![CDATA[概览1.将hadoop的hdfs-site.xml和core-site.xml 放到flume/conf下2.将hadoop的jar包拷贝到flume的lib目录下3.配置flume2.conf4.启动flume(保证首先启动hdfs)5.测试6.注意 准备首先将flume配置完毕,参考flume的单机版配置及测试 hadoop集群搭建完毕,参考hadoop单机版搭建,hadoop集群搭建 工具:Xshell 5,Xftp 5 1.将hadoop的hdfs-site.xml和core-site.xml 放到flume/conf下12[root@localhost ~]# cp /usr/hadoop/hadoop-2.7.3/etc/hadoop/core-site.xml /usr/flume/apache-flume-1.8.0-bin/conf/[root@localhost ~]# cp /usr/hadoop/hadoop-2.7.3/etc/hadoop/hdfs-site.xml /usr/flume/apache-flume-1.8.0-bin/conf/ 2.将hadoop的jar包拷贝到flume的lib目录下123[root@localhost ~]# cp $HADOOP_HOME/share/hadoop/common/hadoop-common-2.7.3.jar /usr/flume/apache-flume-1.8.0-bin/lib/[root@localhost ~]# cp $HADOOP_HOME/share/hadoop/common/lib/hadoop-auth-2.7.3.jar /usr/flume/apache-flume-1.8.0-bin/lib/[root@localhost ~]# cp $HADOOP_HOME/share/hadoop/common/lib/commons-configuration-1.6.jar /usr/flume/apache-flume-1.8.0-bin/lib/ 3.配置flume2.conf在flume安装目录下的conf下创建一个flume2.conf文件,并写入配置 1234567891011121314151617181920212223242526272829303132333435363738#定义agent名， source、channel、sink的名称 a4.sources = r1 a4.channels = c1 a4.sinks = k1 #具体定义source a4.sources.r1.type = spooldir #先创建此目录，保证里面空的 a4.sources.r1.spoolDir = /logs #具体定义channel a4.channels.c1.type = memory a4.channels.c1.capacity = 10000 a4.channels.c1.transactionCapacity = 100 #定义拦截器，为消息添加时间戳 a4.sources.r1.interceptors = i1 a4.sources.r1.interceptors.i1.type = org.apache.flume.interceptor.TimestampInterceptor$Builder #具体定义sink a4.sinks.k1.type = hdfs #集群的nameservers名字 #单节点的直接写：hdfs://localhost:9000/flume/%Y%m%d #集群版的ns为集群名称a4.sinks.k1.hdfs.path = hdfs://ns/flume/%Y%m%d a4.sinks.k1.hdfs.filePrefix = events- a4.sinks.k1.hdfs.fileType = DataStream #不按照条数生成文件 a4.sinks.k1.hdfs.rollCount = 0 #HDFS上的文件达到128M时生成一个文件 a4.sinks.k1.hdfs.rollSize = 134217728 #HDFS上的文件达到60秒生成一个文件 a4.sinks.k1.hdfs.rollInterval = 60 #组装source、channel、sink a4.sources.r1.channels = c1 a4.sinks.k1.channel = c1 4.启动flume(保证首先启动hdfs)启动hdfs参考hadoop单机版搭建,hadoop集群搭建中的hadoop启动 1[root@localhost apache-flume-1.8.0-bin]# flume-ng agent -n a4 -c conf -f conf/flume2.conf -Dflume.root.logger=INFO,console 5.测试重新打开一个窗口 在/usr/tmp下创建一个a文件 1234[root@localhost ~]# cd /usr/tmp/[root@localhost tmp]# lshive_add.jar hive_name.jar hive_xx.jar student[root@localhost tmp]# vim a 随便输入一些内容 123456dadadadaaaaaaaaa 保存退出,将其copy到logs文件夹下 1[root@localhost tmp]# cp a /logs/ 查看原来的窗口 然后登录http://你的主机ip:50070查看 如此便是采集日志到hadoop成功了 6.注意注意在测试的时候不能同时开启两个监控目录的flume,不然会出异常]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Flume</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flume的单机版配置及测试]]></title>
    <url>%2F2018%2F11%2F19%2Fblog181119-1%2F</url>
    <content type="text"><![CDATA[概览1.上传解压2.修改配置文件(监控目录)3.启动及测试(监控目录)4.修改配置文件(监控文件)5.启动及测试(监控文件)6.修改配置文件(监控端口)7.启动及测试(监控端口)8.使flume后台执行,日志输入到指定文件 准备工具:Xshell 5,Xftp 5(官网有免费版本)flume版本:1.8.0jdk1.8.0_141 1.上传解压在flume官网上下载完成flume后,利用Xftp上传到虚拟机中,然后解压 1234567[root@localhost ~]# cd /usr/[root@localhost usr]# lsbin etc games hadoop hbase hive include java lib lib64 libexec local sbin share sqoop src tmp zookeeper#在usr下创建一个flume文件夹,用作安装路径[root@localhost usr]# mkdir flume[root@localhost usr]# lsbin etc flume games hadoop hbase hive include java lib lib64 libexec local sbin share sqoop src tmp zookeeper 利用Xftp将flume压缩包上传到usr/flume下 然后解压 1234[root@localhost usr]# cd flume/[root@localhost flume]# lsapache-flume-1.8.0-bin.tar.gz[root@localhost flume]# tar -zxf apache-flume-1.8.0-bin.tar.gz 2.修改配置文件(监控目录)2.1 .修改flume-env.sh 文件12345[root@localhost flume]# cd apache-flume-1.8.0-bin/conf/[root@localhost conf]# lsflume-conf.properties.template flume-env.ps1.template flume-env.sh.template log4j.properties# 提供了一个模版文件,我们copy出来一个flume-env.sh[root@localhost conf]# cp flume-env.sh.template flume-env.sh 修改 1vim flume-env.sh 将其中的java环境变量修改为自己的jdk安装目录,如果没有安装jdk请按照这篇文章中的参考安装 修改过后是2.2. 创建修改flume.conf文件在flume的安装路径下的conf文件夹下创建一个flume.conf文件,此配置文件是监控目录 123[root@localhost conf]# touch flume.conf[root@localhost conf]# lsflume.conf flume-conf.properties.template flume-env.ps1.template flume-env.sh flume-env.sh.template log4j.properties 修改其中的内容为(flume官网文档上也有此配置) 123456789101112131415161718192021# 指定Agent的组件名称a1.sources = r1a1.sinks = k1a1.channels = c1# 指定Flume source(要监听的路径)a1.sources.r1.type = spooldir#先创建这个目录a1.sources.r1.spoolDir = /logs # 指定Flume sinka1.sinks.k1.type = logger# 指定Flume channela1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100# 绑定source和sink到channel上a1.sources.r1.channels = c1a1.sinks.k1.channel = c1 保存退出2.3配置profile环境变量 1[root@localhost conf]# vim /etc/profile 在profile文件最后添加 12export FLUME_HOME=/usr/flume/apache-flume-1.8.0-binexport PATH=$PATH:$FLUME_HOME/bin 保存退出,刷新变量 1source /etc/profile 3.启动及测试(监控目录)在根目录创建logs日志文件夹 12[root@localhost conf]# cd /[root@localhost /]# mkdir logs 查看是否安装成功 1234567[root@localhost /]# cd /usr/flume/apache-flume-1.8.0-bin[root@localhost apache-flume-1.8.0-bin]# flume-ng versionFlume 1.8.0Source code repository: https://git-wip-us.apache.org/repos/asf/flume.gitRevision: 99f591994468633fc6f8701c5fc53e0214b6da4fCompiled by denes on Fri Sep 15 14:58:00 CEST 2017From source with checksum fbb44c8c8fb63a49be0a59e27316833d 启动flume 1[root@localhost apache-flume-1.8.0-bin]# flume-ng agent --conf conf --conf-file conf/flume.conf --name a1 -Dflume.root.logger=INFO,console 字段的意思为 查看是否启动成功没有报错如图所示便启动成功 测试 然后重新打开个窗口 在/usr/tmp下创建一个student文件,随便写入一些内容 1vim student 123451,lilei2,hanmeimei3,xiaohei4,haha5,lkkkk 然后将其copy到logs文件夹内 1[root@localhost tmp]# cp student /logs/ 返回刚开始的窗口,查看 4.修改配置文件(监控文件)其他修改和监控目录的一样 在flume安装目录下的conf文件夹下创建一个a.conf文件 12[root@localhost tmp]# cd /usr/flume/apache-flume-1.8.0-bin/conf/[root@localhost conf]# vim a.conf 写入配置内容为 1234567891011121314151617181920212223# 指定Agent的组件名称a1.sources = r1a1.sinks = k1a1.channels = c1# 指定Flume source(要监听的路径)#指定文件类型a1.sources.r1.type = exec#指定文件a1.sources.r1.command = tail -F /a.log # 指定Flume sinka1.sinks.k1.type = logger# 指定Flume channela1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100# 绑定source和sink到channel上a1.sources.r1.channels = c1a1.sinks.k1.channel = c1 5.启动及测试(监控文件)启动1[root@localhost tmp]# flume-ng agent --conf conf --conf-file conf/a.conf --name a1 -Dflume.root.logger=INFO,console 查看是否启动成功测试重新打开一个新的窗口在根目录下创建一个a.log日志文件 1234[root@localhost ~]# cd /[root@localhost /]# lsbin boot dev etc home lib lib64 logs media mnt opt proc root run sbin srv sys tmp usr var zhiyou[root@localhost /]# vim a.log 随便写入一些内容 1234ddawdadwddwwhelloworld 保存退出 在原先的窗口查看 6.修改配置文件(监控端口)其他修改和监控目录的一样 在flume安装目录下的conf文件夹下创建一个logger.conf文件 12[root@localhost /]# cd /usr/flume/apache-flume-1.8.0-bin/conf/[root@localhost conf]# vim logger.conf 写入配置内容 1234567891011121314151617181920212223#me the components on this agenta1.sources = r1a1.sinks = k1a1.channels = c1 # Describe/configure the sourcea1.sources.r1.type = netcata1.sources.r1.bind = localhosta1.sources.r1.port = 44444 # Describe the sinka1.sinks.k1.type = logger # Use a channel which buffers events in memorya1.channels.c1.type = memory#默认该通道中最大可以存储的event数量a1.channels.c1.capacity = 1000#每次最大可以从source中拿到或者送到sink中的event数量 a1.channels.c1.transactionCapacity = 100 # Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1 7.启动及测试(监控端口)启动1[root@localhost conf]# flume-ng agent --conf conf --conf-file logger.conf --name a1 -Dflume.root.logger=INFO,consoleI 查看是否启动成功 测试重新打开个窗口 1[root@localhost conf]# telnet localhost 44444 如果没有telnet命令,安装 123# 任意执行一个命令yum -y install telnetyum -y install nc 然后随便输入一些内容 回到原来的窗口查看 8.使flume后台执行,日志输入到指定文件有时觉得再开一个窗口较为麻烦,且需要存储日志信息,那么可以这样将日志输入到指定文件 先将监控端口的flume进程杀死,然后重新启动1[root@localhost conf]# flume-ng agent --conf conf --conf-file conf/logger.conf --name a1 -Dflume.root.logger=INFO,console &gt; x1nohup.out 2&gt;&amp;1 &amp; 程序会在后台执行,并将日志信息输入到安装目录下的x1nohup.out文件中 12# 查看日志文件[root@localhost apache-flume-1.8.0-bin]# cat x1nohup.out 类似于如图所示 简单的flume单机版的操作便完成了,接下来会介绍和hadoop单机版集群的搭配使用]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Flume</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringMVC学习(一)]]></title>
    <url>%2F2018%2F11%2F06%2Fblog181106-1%2F</url>
    <content type="text"><![CDATA[概览1.导入JAR包2.配置web.xml3.配置springmvc.xml4.创建controller层5.创建JSP测试页面6.测试7.注意 准备工具:eclipseJAR包版本:spring4.1.0和2.5.6JQuery版本:jquery-3.3.1.js资料参考:https://www.cnblogs.com/sunniest/p/4555801.html 1.导入JAR包创建个web项目在spring的官网上下载4.1.0和2.5.6版本的spring-framework然后分别在两个文件下找到以下的jar包,将其导入到项目中 2.配置web.xml在项目的WebContent/WEB-INF下生成web.xml文件在web.xml中配置springmvc 123456789101112131415161718192021222324252627282930&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;web-app xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns=&quot;http://xmlns.jcp.org/xml/ns/javaee&quot; xsi:schemaLocation=&quot;http://xmlns.jcp.org/xml/ns/javaee http://xmlns.jcp.org/xml/ns/javaee/web-app_3_1.xsd&quot; version=&quot;3.1&quot;&gt; &lt;display-name&gt;springmvc1&lt;/display-name&gt; &lt;welcome-file-list&gt; &lt;welcome-file&gt;index.html&lt;/welcome-file&gt; &lt;welcome-file&gt;index.htm&lt;/welcome-file&gt; &lt;welcome-file&gt;index.jsp&lt;/welcome-file&gt; &lt;welcome-file&gt;default.html&lt;/welcome-file&gt; &lt;welcome-file&gt;default.htm&lt;/welcome-file&gt; &lt;welcome-file&gt;default.jsp&lt;/welcome-file&gt; &lt;/welcome-file-list&gt; &lt;!-- 配置springmvc的核心过滤器 --&gt; &lt;servlet&gt; &lt;servlet-name&gt;springmvc&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;!-- 指定配置文件的路径 如果不写默认为WEB-INF下的&#123;servlet-name&#125;-servlet.xml --&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:springmvc.xml&lt;/param-value&gt; &lt;/init-param&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;springmvc&lt;/servlet-name&gt; &lt;!-- 类似于过滤任何的请求地址 --&gt; &lt;url-pattern&gt;/&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; &lt;/web-app&gt; 3.配置springmvc.xml然后在src下创建个springmvc(名字与web.xml中的servlet-name相同) 1234567891011121314151617181920212223242526272829303132333435363738394041424344&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot; xmlns:tx=&quot;http://www.springframework.org/schema/tx&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:mvc=&quot;http://www.springframework.org/schema/mvc&quot; xsi:schemaLocation=&quot; http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc.xsd&quot;&gt; &lt;!-- 扫描标注注解的对象 --&gt; &lt;context:component-scan base-package=&quot;com.zy.springmvc1.controller&quot;&gt;&lt;/context:component-scan&gt; &lt;!-- 使用注解配置 --&gt; &lt;mvc:annotation-driven&gt;&lt;/mvc:annotation-driven&gt; &lt;!-- 不过滤静态资源 --&gt; &lt;mvc:default-servlet-handler/&gt; &lt;!-- 配置springmvc的视图解析器 --&gt; &lt;bean class=&quot;org.springframework.web.servlet.view.InternalResourceViewResolver&quot;&gt; &lt;!-- 配置前缀 --&gt; &lt;property name=&quot;prefix&quot; value=&quot;/&quot;&gt;&lt;/property&gt; &lt;!-- 配置后缀 --&gt; &lt;property name=&quot;suffix&quot; value=&quot;.jsp&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!-- 文件上传的配置 --&gt; &lt;bean id=&quot;multipartResolver&quot; class=&quot;org.springframework.web.multipart.commons.CommonsMultipartResolver&quot;&gt; &lt;!-- 设置上传文件的最大值 --&gt; &lt;property name=&quot;maxUploadSize&quot; value=&quot;102400000&quot;&gt;&lt;/property&gt; &lt;/bean&gt;&lt;/beans&gt; 4.创建controller层创建一个controller包,在其中创建一个TestController的控制器(注意不是servlet了,而是普通的class)springmvc最为便捷的便是可以用注释替代大量的设置 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143package com.zy.springmvc1.controller;import java.io.IOException;import java.io.PrintWriter;import java.text.SimpleDateFormat;import java.util.Date;import java.util.Map;import javax.servlet.http.HttpServletRequest;import javax.servlet.http.HttpServletResponse;import org.springframework.beans.propertyeditors.CustomDateEditor;import org.springframework.stereotype.Controller;import org.springframework.ui.Model;import org.springframework.web.bind.ServletRequestDataBinder;import org.springframework.web.bind.annotation.InitBinder;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.servlet.ModelAndView;import com.zy.springmvc1.pojo.User;//把类作为springmvc的控制器@Controller //会自动创建控制器对象@RequestMapping(&quot;/mvc&quot;) //类似于路径或文件夹public class TestController &#123; //定义一个方法,返回到根目录下index.jsp页面 @RequestMapping(&quot;/toIndex&quot;) public String toIndex()&#123; //现在看返回的页面名称,实际上是一个路径 //由于配置过了前缀和后缀,所以其表示的还是index.jsp页面 return &quot;index&quot;; &#125; //访问web-inf/jsp下的文件 @RequestMapping(&quot;/toIndex2&quot;) public String toIndex2()&#123; //返回的是路径 return &quot;WEB-INF/jsp/index&quot;; &#125; //接受方法入参 @RequestMapping(&quot;revParm&quot;) public void revParm(String name,Integer age,Double weight)&#123; System.out.println(name+&quot; &quot;+age+&quot; &quot;+weight); &#125; //注入对象属性值 @RequestMapping(&quot;/getUser&quot;) public String toIndex3(User user)&#123; System.out.println(user); return &quot;index&quot;; &#125; //springmvc日期类型默认是以yyyy/MM/dd的格式,但咱们自己的习惯是yyyy-MM-dd的格式 //400 badrequest 参数格式有问题 @InitBinder public void initBinder(ServletRequestDataBinder binder) &#123; binder.registerCustomEditor(Date.class, new CustomDateEditor(new SimpleDateFormat(&quot;yyyy-MM-dd&quot;), true)); &#125; //向前台页面发送数据展示 @RequestMapping(&quot;/showMsg&quot;) public ModelAndView showMsg(Model model)&#123; User user = new User(10, &quot;lala&quot;, 22, null); //把对象放入到作用域,model类似于request作用域 model.addAttribute(&quot;u&quot;,user); return new ModelAndView(&quot;showMsg&quot;); &#125; //向前台发送数据展示 @RequestMapping(&quot;/showMsg2&quot;) public String showMsg2(HttpServletRequest request)&#123; User user = new User(10, &quot;lala&quot;, 22, null); request.setAttribute(&quot;u&quot;, user); return &quot;showMsg&quot;; &#125; //向前台发送数据展示map @RequestMapping(&quot;/showMsg3&quot;) public String showMsg3(Map&lt;String,Object&gt; map)&#123; //放入到作用域 User user = new User(10, &quot;lala&quot;, 22, null); map.put(&quot;u&quot;,user); return &quot;showMsg&quot;; &#125; //ajax的调用 @RequestMapping(&quot;/ajax&quot;) public void ajax(String name,PrintWriter out)&#123; System.out.println(&quot;name=&quot;+name); out.print(&quot;hello &quot;+name); &#125; //解决中文乱码问题 @RequestMapping(&quot;/ajax1&quot;) public void ajax1(String name,HttpServletResponse resp) throws IOException&#123; resp.setCharacterEncoding(&quot;utf-8&quot;); resp.getWriter().write(name+&quot;hello2&quot;); &#125; //跳转到ajax.jsp @RequestMapping(&quot;/toAjax&quot;) public String toAjax()&#123; return &quot;WEB-INF/jsp/ajax&quot;; &#125; //在同一个控制器下实现跳转操作,从一个方法跳转到另外一个方法 @RequestMapping(&quot;/goTo&quot;) public String goToIndex()&#123; return &quot;redirect:toIndex&quot;; &#125; //在同一个控制器下实现转发的操作 @RequestMapping(&quot;/goTo2&quot;) public String goToIndex1()&#123; //转发跳转,地址栏不变化 return &quot;forward:toIndex&quot;; &#125; //实现不同控制器之间的跳转 @RequestMapping(&quot;goTo3&quot;) public String goToController2()&#123; return &quot;redirect:/mvc2/test&quot;; &#125; @RequestMapping(&quot;goTo4&quot;) public String goToController22()&#123; return &quot;forward:/mvc2/test&quot;; &#125; &#125; 再创建一个TestController2为了实现不同控制器间的跳转 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152package com.zy.springmvc1.controller;import java.text.SimpleDateFormat;import java.util.Date;import org.springframework.beans.propertyeditors.CustomDateEditor;import org.springframework.stereotype.Controller;import org.springframework.web.bind.ServletRequestDataBinder;import org.springframework.web.bind.annotation.InitBinder;import org.springframework.web.bind.annotation.ModelAttribute;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RequestMethod;import org.springframework.web.bind.annotation.RequestParam;import com.zy.springmvc1.pojo.User;@Controller@RequestMapping(&quot;/mvc2&quot;)public class TestController2 &#123; @InitBinder public void initBinder(ServletRequestDataBinder binder) &#123; binder.registerCustomEditor(Date.class, new CustomDateEditor(new SimpleDateFormat(&quot;yyyy-MM-dd&quot;), true)); &#125; //修饰的方法 --- 在执行控制中的其他方法之前会执行此方法 @ModelAttribute public void test()&#123; System.out.println(&quot;000000000000&quot;); &#125; @RequestMapping(&quot;/te&quot;) public String test3(@ModelAttribute(&quot;u&quot;) User user)&#123; System.out.println(user); return &quot;showMsg&quot;; &#125; //定义一个方法 @RequestMapping(&quot;/test&quot;) public void test2()&#123; System.out.println(&quot;successful&quot;); &#125; //required默认为true 参数必须存在,false参数可以不传入 @RequestMapping(value=&quot;/reqParm&quot;,method=RequestMethod.GET) public void reqParm(@RequestParam(value=&quot;no&quot;,required=false) Integer no,@RequestParam(value=&quot;sex&quot;) String sex)&#123; //查看结果 System.out.println(&quot;no=&quot;+no +&quot;\t sex=&quot;+sex); &#125;&#125; 还要创建一个测试的实体类,随便创建一个User 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051package com.zy.springmvc1.pojo;import java.util.Date;public class User &#123; private Integer no; private String name; private Integer age; private Date birthDay; public Integer getNo() &#123; return no; &#125; public void setNo(Integer no) &#123; this.no = no; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public Integer getAge() &#123; return age; &#125; public void setAge(Integer age) &#123; this.age = age; &#125; public Date getBirthDay() &#123; return birthDay; &#125; public void setBirthDay(Date birthDay) &#123; this.birthDay = birthDay; &#125; public User() &#123; super(); // TODO Auto-generated constructor stub &#125; public User(Integer no, String name, Integer age, Date birthDay) &#123; super(); this.no = no; this.name = name; this.age = age; this.birthDay = birthDay; &#125; @Override public String toString() &#123; return &quot;User [no=&quot; + no + &quot;, name=&quot; + name + &quot;, age=&quot; + age + &quot;, birthDay=&quot; + birthDay + &quot;]&quot;; &#125; &#125; 然后创建一个Upload的控制器,控制文件的上传 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253package com.zy.springmvc1.controller;import java.io.FileOutputStream;import java.util.UUID;import javax.servlet.http.HttpServletRequest;import org.apache.commons.io.IOUtils;import org.springframework.stereotype.Controller;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.multipart.MultipartFile;import org.springframework.web.multipart.MultipartHttpServletRequest;@Controller@RequestMapping(&quot;/upload&quot;)public class UploadFile &#123; //跳转到upload.jsp页面 @RequestMapping(&quot;/toup&quot;) public String toUp()&#123; return &quot;/WEB-INF/jsp/upload&quot;; &#125; //文件上传操作的方法 @RequestMapping(&quot;/up&quot;) public String upload(HttpServletRequest req) throws Exception&#123; //强制类型转换 MultipartHttpServletRequest msr = (MultipartHttpServletRequest)req; //获得上传的文件 MultipartFile file = msr.getFile(&quot;img&quot;); //拿到文件的名称 String oriName = file.getOriginalFilename(); //文件名称需要处理 //System.out.println(oriName); //得到文件的后缀名称 String ext = oriName.substring(oriName.lastIndexOf(&quot;.&quot;)); //System.out.println(ext); //上传操作,知道上传的路径 String path = req.getServletContext().getRealPath(&quot;/upload&quot;); //System.out.println(&quot;服务器的上传地址&quot;+path); //通过输出流直接写入文件到服务器端的绝对路径下 FileOutputStream fos = new FileOutputStream(path+&quot;/&quot;+UUID.randomUUID().toString()+ext); //fos.write(file.getBytes()); IOUtils.copy(file.getInputStream(), fos); //关闭流 fos.close(); return &quot;success&quot;; &#125; &#125; 5.创建JSP测试页面在Webcontent下创建index.jsp,showMsg.jsp,success.jspindex.jsp 123456789101112&lt;%@ page language=&quot;java&quot; contentType=&quot;text/html; charset=UTF-8&quot; pageEncoding=&quot;UTF-8&quot;%&gt;&lt;!DOCTYPE html PUBLIC &quot;-//W3C//DTD HTML 4.01 Transitional//EN&quot; &quot;http://www.w3.org/TR/html4/loose.dtd&quot;&gt;&lt;html&gt;&lt;head&gt;&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=UTF-8&quot;&gt;&lt;title&gt;Insert title here&lt;/title&gt;&lt;/head&gt;&lt;body&gt; hello world!!!&lt;/body&gt;&lt;/html&gt; showMsg.jsp 123456789101112&lt;%@ page language=&quot;java&quot; contentType=&quot;text/html; charset=UTF-8&quot; pageEncoding=&quot;UTF-8&quot;%&gt;&lt;!DOCTYPE html PUBLIC &quot;-//W3C//DTD HTML 4.01 Transitional//EN&quot; &quot;http://www.w3.org/TR/html4/loose.dtd&quot;&gt;&lt;html&gt;&lt;head&gt;&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=UTF-8&quot;&gt;&lt;title&gt;Insert title here&lt;/title&gt;&lt;/head&gt;&lt;body&gt; $&#123;u.no &#125;--$&#123;u.name &#125;--$&#123;u.age &#125;&lt;/body&gt;&lt;/html&gt; success.jsp 123456789101112&lt;%@ page language=&quot;java&quot; contentType=&quot;text/html; charset=UTF-8&quot; pageEncoding=&quot;UTF-8&quot;%&gt;&lt;!DOCTYPE html PUBLIC &quot;-//W3C//DTD HTML 4.01 Transitional//EN&quot; &quot;http://www.w3.org/TR/html4/loose.dtd&quot;&gt;&lt;html&gt;&lt;head&gt;&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=UTF-8&quot;&gt;&lt;title&gt;Insert title here&lt;/title&gt;&lt;/head&gt;&lt;body&gt; success&lt;/body&gt;&lt;/html&gt; 再在WEB-INF下创建一个jsp的文件夹,在其中再创建ajax.jsp,index.jsp,upload.jsp ajax.jsp这里你需要导入JQuery的js在Webcontent下创建个js文件夹,将JQuery放入进去这里使用的是jquery-3.3.1.js12345678910111213141516171819202122232425262728293031323334&lt;%@ page language=&quot;java&quot; contentType=&quot;text/html; charset=UTF-8&quot; pageEncoding=&quot;UTF-8&quot;%&gt;&lt;!DOCTYPE html PUBLIC &quot;-//W3C//DTD HTML 4.01 Transitional//EN&quot; &quot;http://www.w3.org/TR/html4/loose.dtd&quot;&gt;&lt;html&gt;&lt;head&gt;&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=UTF-8&quot;&gt;&lt;title&gt;Insert title here&lt;/title&gt;&lt;!-- 引入JQuery --&gt;&lt;script type=&quot;text/javascript&quot; src=&quot;&lt;%=request.getContextPath() %&gt;/js/jquery-3.3.1.js&quot;&gt;&lt;/script&gt;&lt;script type=&quot;text/javascript&quot;&gt;/* onload函数 */ $(function()&#123; //获得文本框的对象 $(&quot;#name&quot;).blur(function()&#123; var value=$(this).val(); //alert(value); $.ajax(&#123; type : &quot;POST&quot;, url : &quot;ajax1&quot;, data : &quot;name=&quot;+value, success : function(msg) &#123; alert(&quot;Data Saved: &quot; + msg); &#125; &#125;); &#125;) &#125;) &lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;input name=&quot;name&quot; id=&quot;name&quot;&gt;&lt;/body&gt;&lt;/html&gt; WEB-INF/jsp/index.jsp 123456789101112&lt;%@ page language=&quot;java&quot; contentType=&quot;text/html; charset=UTF-8&quot; pageEncoding=&quot;UTF-8&quot;%&gt;&lt;!DOCTYPE html PUBLIC &quot;-//W3C//DTD HTML 4.01 Transitional//EN&quot; &quot;http://www.w3.org/TR/html4/loose.dtd&quot;&gt;&lt;html&gt;&lt;head&gt;&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=UTF-8&quot;&gt;&lt;title&gt;Insert title here&lt;/title&gt;&lt;/head&gt;&lt;body&gt; hello world!!!222&lt;/body&gt;&lt;/html&gt; upload.jsp 123456789101112131415161718&lt;%@ page language=&quot;java&quot; contentType=&quot;text/html; charset=UTF-8&quot; pageEncoding=&quot;UTF-8&quot;%&gt;&lt;!DOCTYPE html PUBLIC &quot;-//W3C//DTD HTML 4.01 Transitional//EN&quot; &quot;http://www.w3.org/TR/html4/loose.dtd&quot;&gt;&lt;html&gt;&lt;head&gt;&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=UTF-8&quot;&gt;&lt;title&gt;Insert title here&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;!-- post提交 enctype:multipart/form-data --&gt; &lt;form action=&quot;up&quot; method=&quot;post&quot; enctype=&quot;multipart/form-data&quot;&gt; 名称:&lt;input name=&quot;name&quot; /&gt;&lt;br/&gt; 图片:&lt;input name=&quot;img&quot; type=&quot;file&quot; /&gt;&lt;br/&gt; &lt;input type=&quot;submit&quot; value=&quot;上传&quot;/&gt; &lt;/form&gt;&lt;/body&gt;&lt;/html&gt; 6.测试将项目布置到tomcat中,启动tomcat打开浏览器在地址栏分别输入你要测试的方法的RequestMapping注释然后查看jsp页面和控制台输出(有些是没有页面的所以会404,这是正常的,查看控制台有没有输出) 7.注意1.springmvc的默认路径不是src下[classpath],默认路径在web-inf下2.springmvc默认名称有命名规则:web.xml中的servlet的[name-servlet.xml]3.springmvc是面向方法编程的,定义方法实现跳转4.springmvc的文件上传必须用post提交,还要有enctype属性 enctype:multipart/form-data 具体还是参考:https://www.cnblogs.com/sunniest/p/4555801.html这里对springmvc的讲解十分详细,所以本文就拿来自己实验测试用下]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>SpringMVC</tag>
        <tag>Ajax</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java代码实现对Hive的基本操作]]></title>
    <url>%2F2018%2F10%2F26%2Fblog181026-3%2F</url>
    <content type="text"><![CDATA[概览1.导入jar包2.测试 1.导入jar包确保你的Zookeeper,Hadoop集群和hive启动着 在eclipse上新建java项目,并在项目下建个lib文件夹,然后将jar包放到lib中导入项目 hive的lib下的将其全部导入到项目中 2.测试在你要测试的hive的主机的/usr/tmp建个student文件,里面放入一些数据数据列间使用一个逗号(,)隔开12341,lilei2,hanmeimei3,xiaoming4,haha 建个包,建个HiveJDBC测试类 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495package com.zy.hivejdbc;import java.sql.Connection;import java.sql.DriverManager;import java.sql.ResultSet;import java.sql.Statement;import org.junit.After;import org.junit.Before;import org.junit.Test;public class HiveJDBC &#123; private static String driverName=&quot;org.apache.hive.jdbc.HiveDriver&quot;; private static String url = &quot;jdbc:hive2://192.168.134.153:10000/mydb&quot;; private static String user = &quot;root&quot;; private static String password=&quot;root&quot;; private static Connection conn = null; private static Statement stmt = null; private static ResultSet rs = null; @Before public void init() throws Exception&#123; Class.forName(driverName); conn = DriverManager.getConnection(url, user, password); stmt = conn.createStatement(); &#125; @Test public void createDatabase() throws Exception&#123; String sql = &quot;create database hive_jdbc_test&quot;; System.out.println(&quot;Running: &quot; + sql); stmt.executeQuery(sql); &#125; @Test public void dropDatabase() throws Exception &#123; String sql = &quot;drop database if exists hive_jdbc_test&quot;; System.out.println(&quot;Running: &quot; + sql); stmt.execute(sql); &#125; @Test public void showDatabases() throws Exception &#123; String sql = &quot;show databases&quot;; System.out.println(&quot;Running: &quot; + sql + &quot;\n&quot;); rs = stmt.executeQuery(sql); while (rs.next()) &#123; System.out.println(rs.getString(1) ); &#125; &#125; @Test public void createTable() throws Exception &#123; String sql = &quot;create table t2(id int ,name String) row format delimited fields terminated by &apos;,&apos;;&quot;; System.out.println(&quot;Running: &quot; + sql); stmt.execute(sql); &#125; @Test public void loadData() throws Exception &#123; String filePath = &quot;/usr/tmp/student&quot;; String sql = &quot;load data local inpath &apos;&quot; + filePath + &quot;&apos; overwrite into table t2&quot;; System.out.println(&quot;Running: &quot; + sql); stmt.execute(sql); &#125; @Test public void selectData() throws Exception &#123; String sql = &quot;select * from t2&quot;; System.out.println(&quot;Running: &quot; + sql); rs = stmt.executeQuery(sql); System.out.println(&quot;编号&quot; + &quot;\t&quot; + &quot;姓名&quot; ); while (rs.next()) &#123; System.out.println(rs.getInt(1) + &quot;\t&quot; + rs.getString(2)); &#125; &#125; @Test public static void drop(Statement stmt) throws Exception &#123; String dropSQL = &quot;drop table t2&quot;; boolean bool = stmt.execute(dropSQL); System.out.println(&quot;删除表是否成功:&quot; + bool); &#125; @After public void destory() throws Exception &#123; if (rs != null) &#123; rs.close(); &#125; if (stmt != null) &#123; stmt.close(); &#125; if (conn != null) &#123; conn.close(); &#125; &#125;&#125; 分别双击方法名右键run Junit test 测试,并在hive和eclipse控制台查看结果]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Java</tag>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HBase中什么是Region，什么是RegionServer]]></title>
    <url>%2F2018%2F10%2F26%2Fblog181026-2%2F</url>
    <content type="text"><![CDATA[regionServer 其实是hbase的服务，部署在一台物理服务器上，region有一点像关系型数据的分区，数据存放在region中，当然region下面还有很多结构，确切来说数据存放在memstore和hfile中。我们访问hbase的时候，先去hbase 系统表查找定位这条记录属于哪个region，然后定位到这个region属于哪个服务器，然后就到哪个服务器里面查找对应region中的数据 Region是HBase数据存储和管理的基本单位。 一个表中可以包含一个或多个Region。 每个Region只能被一个RS（RegionServer）提供服务，RS可以同时服务多个Region，来自不同RS上的Region组合成表格的整体逻辑视图。]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>HBase</tag>
        <tag>Region</tag>
        <tag>RegionServer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java代码实现对HBase的基本操作]]></title>
    <url>%2F2018%2F10%2F26%2Fblog181026-1%2F</url>
    <content type="text"><![CDATA[概览1.导入jar包2.测试3.异常处理 首先将HBase搭建完成,然后启动Zookeeper,Hadoop,HBase集群 1.导入jar包准备:1.CentOS72.Zookeeper集群3.Hadoop2.7.3集群4.hbase2.0.0集群5.eclipse 在eclipse中建个java项目,项目中新建个lib文件夹用来存放jar包将hbase目录下的lib下的所有jar包导入到项目中 2.测试然后建个包,建个TestHbase类 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125package com.hd.hbase;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.hbase.HBaseConfiguration;import org.apache.hadoop.hbase.HColumnDescriptor;import org.apache.hadoop.hbase.HTableDescriptor;import org.apache.hadoop.hbase.TableName;import org.apache.hadoop.hbase.client.Admin;import org.apache.hadoop.hbase.client.Connection;import org.apache.hadoop.hbase.client.ConnectionFactory;import org.apache.hadoop.hbase.client.Delete;import org.apache.hadoop.hbase.client.Get;import org.apache.hadoop.hbase.client.Put;import org.apache.hadoop.hbase.client.Result;import org.apache.hadoop.hbase.client.ResultScanner;import org.apache.hadoop.hbase.client.Scan;import org.apache.hadoop.hbase.client.Table;import org.apache.hadoop.hbase.util.Bytes;import org.junit.After;import org.junit.Before;import org.junit.Test;public class TestHbase &#123; private Configuration conf = null; private Connection conn = null; private Admin admin = null; private Table table = null; @Before public void createConf() throws IOException&#123; //解决异常,可以不用 System.setProperty(&quot;hadoop.home.dir&quot;,&quot;D:\\software\\01-软件资料\\hadoop-common-2.2.0-bin-master&quot;); conf = HBaseConfiguration.create(); //设置zk集群地址,这里需要修改windows下的hosts文件 conf.set(&quot;hbase.zookeeper.quorum&quot;,&quot;master:2181,slave1:2181,slave2:2181&quot;); //建立连接 conn = ConnectionFactory.createConnection(conf); &#125; @Test public void createTable() throws IOException&#123; //获取表管理类 admin = conn.getAdmin(); //定义表 HTableDescriptor hTableDescriptor = new HTableDescriptor(TableName.valueOf(&quot;person&quot;)); //定义列族 HColumnDescriptor hColumnDescriptor = new HColumnDescriptor(&quot;info&quot;); //将列族添加到表中 hTableDescriptor.addFamily(hColumnDescriptor); //执行建表操作 admin.createTable(hTableDescriptor); &#125; @Test public void put() throws IOException&#123; //获取表对象 table = conn.getTable(TableName.valueOf(&quot;person&quot;)); //创建put对象 Put put = new Put(&quot;p1&quot;.getBytes()); //添加列 put.addColumn(&quot;info&quot;.getBytes(), &quot;name&quot;.getBytes(), &quot;haha&quot;.getBytes()); //向表格中添加put对象 table.put(put); &#125; @Test public void get() throws IOException&#123; //获取表对象 table = conn.getTable(TableName.valueOf(&quot;person&quot;)); //用行键实例化get Get get = new Get(&quot;p1&quot;.getBytes()); //增加列族名和列名条件 get.addColumn(&quot;info&quot;.getBytes(), &quot;name&quot;.getBytes()); //执行,返回结果 Result result = table.get(get); //取出结果 String valStr = Bytes.toString(result.getValue(&quot;info&quot;.getBytes(), &quot;name&quot;.getBytes())); System.out.println(valStr); &#125; @Test public void scan() throws IOException&#123; //获取表对象 table = conn.getTable(TableName.valueOf(&quot;person&quot;)); //初始化scan示例 Scan scan = new Scan(); //增加过滤条件 scan.addColumn(&quot;info&quot;.getBytes(), &quot;name&quot;.getBytes()); //返回结果 ResultScanner rss = table.getScanner(scan); //迭代取出结果 for (Result result : rss) &#123; String valStr = Bytes.toString(result.getValue(&quot;info&quot;.getBytes(), &quot;name&quot;.getBytes())); System.out.println(valStr); &#125; &#125; @Test public void del() throws IOException&#123; //获取表对象 table = conn.getTable(TableName.valueOf(&quot;person&quot;)); //用行键实例化Delete实例 Delete del = new Delete(&quot;p1&quot;.getBytes()); //执行删除 table.delete(del); &#125; @After public void close() throws IOException&#123; //关闭连接 if(admin!=null)&#123; admin.close(); &#125; if(table!=null)&#123; table.close(); &#125; if(conn!=null)&#123; conn.close(); &#125; &#125; &#125; 修改Windows下的hosts文件找到这个路径C:\Windows\System32\drivers\etc在hosts文件最后添加你的hbase集群的主机ip和主机名 然后分别双击方法名,右键run Junit测试每个方法 3.异常处理其中 1System.setProperty(&quot;hadoop.home.dir&quot;,&quot;D:\\software\\01-软件资料\\hadoop-common-2.2.0-bin-master&quot;); 是为了解决找不到hadoop的bin目录下winutils.exe文件的异常 1234562018-10-26 19:19:10,309 ERROR [main] util.Shell (Shell.java:getWinUtilsPath(400)) - Failed to locate the winutils binary in the hadoop binary pathjava.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries. at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:382) at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:397) at org.apache.hadoop.util.Shell.&lt;clinit&gt;(Shell.java:390) at org.apache.hadoop.util.StringUtils.&lt;clinit&gt;(StringUtils.java:80) 其实这个异常就算不处理也是可以正常连接hbase操作的,但是最好还是要解决掉,因为我采用的hadoop2.7.3下的bin目录是没有这个文件的,所以我上网上找了一份hadoop2.2.0版本的,链接：https://pan.baidu.com/s/1koyq-8D5Z7u88DBtHGNe7g提取码：4ut9 这个解压后只有bin目录将其路径填入System.setProperty()中即可.]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>HBase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HBase集群搭建]]></title>
    <url>%2F2018%2F10%2F25%2Fblog181025-3%2F</url>
    <content type="text"><![CDATA[概览1.HBase简介及规划2.上传解压3.配置hbase集群4.拷贝hbase到其他节点5.同步时间6.启动所有hbase7.查看测试 首先HBase是基于Hadoop 3节点高可用集群和Zookeeper集群的,所以先将这两个搭建完毕 1.HBase简介及规划简介 HBase – Hadoop Database，是一个高可靠性、高性能、面向列、可伸缩的分布式存储系统，利用HBase技术可在廉价PC Server上搭建起大规模结构化存储集群。 HBase是一个分布式的、面向列的开源数据库，该技术来源于 Fay Chang 所撰写的Google论文“Bigtable：一个结构化数据的分布式存储系统”。就像Bigtable利用了Google文件系统（File System）所提供的分布式数据存储一样，HBase在Hadoop之上提供了类似于Bigtable的能力。HBase是Apache的Hadoop项目的子项目。HBase不同于一般的关系数据库，它是一个适合于非结构化数据存储的数据库。另一个不同的是HBase基于列的而不是基于行的模式。 与FUJITSU Cliq等商用大数据产品不同，HBase是Google Bigtable的开源实现，类似Google Bigtable利用GFS作为其文件存储系统，HBase利用Hadoop HDFS作为其文件存储系统；Google运行MapReduce来处理Bigtable中的海量数据，HBase同样利用Hadoop MapReduce来处理HBase中的海量数据；Google Bigtable利用Chubby作为协同服务，HBase利用Zookeeper作为对应。 规划 2.上传解压准备:1.CentOS7.02.hbase-2.0.03.XShell 54.Xftp 5 使用Xshell工具(官网下载免费版本即可)连接虚拟机 在主节点的虚拟机的usr下创建一个hbase文件夹作为压缩包存放路径和安装路径 123456[root@master ~]# cd /usr/[root@master usr]# lsbin etc games hadoop hive include java lib lib64 libexec local sbin share sqoop src tmp zookeeper[root@master usr]# mkdir hbase[root@master usr]# lsbin etc games hadoop hbase hive include java lib lib64 libexec local sbin share sqoop src tmp zookeeper 利用Xftp将HBase压缩包上传到hbase文件夹下 1[root@master usr]# cd hbase/ 然后解压 1[root@master hbase]# tar -zxf hbase-2.0.0-bin.tar.gz 3.配置hbase集群配置hbase集群，要修改4个文件 , copy两个文件以及新建一个文件（首先zk集群已经安装好了） 3.1 .把hadoop的hdfs-site.xml和core-site.xml 拷贝到hbase安装目录的conf下 123456[root@master hbase]# cd hbase-2.0.0/conf/[root@master conf]# cp /usr/hadoop/hadoop-2.7.3/etc/hadoop/hdfs-site.xml /usr/hbase/hbase-2.0.0/conf/[root@master conf]# cp /usr/hadoop/hadoop-2.7.3/etc/hadoop/core-site.xml /usr/hbase/hbase-2.0.0/conf/[root@master conf]# lscore-site.xml hbase-env.cmd hbase-policy.xml hdfs-site.xml regionservershadoop-metrics2-hbase.properties hbase-env.sh hbase-site.xml log4j.properties 3.2 .修改hbase-env.sh 1[root@master conf]# vim hbase-env.sh 将28行的改为自己的jdk路径 1export JAVA_HOME=/usr/java/jdk1.8.0_141 再将125行的改为 12#告诉hbase使用外部的zk export HBASE_MANAGES_ZK=false 保存退出 3.3 .修改hbase-site.xml 1[root@master conf]# vim hbase-site.xml 在configuration中添加 12345678910111213141516171819202122232425262728&lt;!-- 指定hbase在HDFS上存储的路径 --&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://ns/hbase&lt;/value&gt; &lt;/property&gt;&lt;!-- 指定hbase是分布式的 --&gt; &lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt;&lt;!-- 指定zk的地址，多个用“,”分割 --&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;master:2181,slave1:2181,slave2:2181&lt;/value&gt; &lt;/property&gt;&lt;!-- 时间服务器不同步导致集群服务无法启动 目前regionserver节点的时间和master的时间差距大于30000ms，就是30秒时无法启动 服务 --&gt;&lt;property&gt; &lt;name&gt;hbase.master.maxclockskew&lt;/name&gt; &lt;value&gt;150000&lt;/value&gt;&lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.master.info.port&lt;/name&gt; &lt;value&gt;60010&lt;/value&gt;&lt;/property&gt; esc+:wq保存退出 3.4 .修改regionservers 1[root@master conf]# vim regionservers 将其改为你的集群的ip映射名 12345#localhostmasterslave1slave2 3.5 .新建backup-masters(高可用) 12[root@master conf]# touch backup-masters[root@master conf]# vim backup-masters 在其中写入你规划处于backup状态的hbase,因为你启动的hbase节点如果只有一台,而当其宕机的时候整个hbase集群都会变得不可用,所以我们设置一个或多个备用节点 1slave1 3.6 .配置profile环境变量1[root@master conf]# vim /etc/profile 在最后添加 12export HBASE_HOME=/usr/hbase/hbase-2.0.0export PATH=$PATH:$HBASE_HOME/bin 保存退出,刷新 1[root@master conf]# source /etc/profile 4.拷贝hbase到其他节点12[root@master conf]# scp -r /usr/hbase/ root@slave1:/usr/[root@master conf]# scp -r /usr/hbase/ root@slave2:/usr/ 5.同步时间如果你集群中所有节点机器的时间相同可以跳过此步骤 在所有节点机器上查看时间 123456[root@master conf]# date2018年 10月 24日 星期三 01:27:46 CST[root@slave1 conf]# date2018年 10月 24日 星期三 01:27:46 CST[root@slave2 conf]# date2018年 10月 24日 星期三 01:27:46 CST 如果不同就执行此操作 12345678910111213141516centos 安装 ntpdate 并同步时间在命令行中做如下操作，来安装ntpdateyum install -y ntp继续在命令行中操作，进行同步时间ntpdate 210.72.145.44ntp常用服务器： 中国国家授时中心：210.72.145.44 NTP服务器(上海) ：ntp.api.bz美国：time.nist.gov 复旦：ntp.fudan.edu.cn 微软公司授时主机(美国) ：time.windows.com 台警大授时中心(台湾)：asia.pool.ntp.org 6.启动所有hbase启动Zookeeper集群启动Hadoop集群 启动hbase,在主节点上运行: start-hbase.sh 1[root@master conf]# start-hbase.sh jps查看其中你的主节点和备用节点上会多出两个进程 HMaster 和HRegionServer 7.查看测试通过浏览器访问hbase管理页面 ip(主节点和备用节点):60010主节点是Master备用的是backup状态 进入shell页面 1[root@master bin]# hbase shell list查看表123456hbase(main):001:0&gt; listTABLE 0 row(s)Took 1.1337 seconds =&gt; []hbase(main):002:0&gt; 其他shell命令参考hbase的基本shell操作 HBase集群搭建完成,接下来会介绍下Java代码实现对HBase的基础操作]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>HBase</tag>
        <tag>shell</tag>
        <tag>HA高可用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HBase之简单的shell操作]]></title>
    <url>%2F2018%2F10%2F25%2Fblog181025-2%2F</url>
    <content type="text"><![CDATA[这些只是简单的shell操作,更多的shell操作还是去网上查看 首先启动相应的服务 进入客户端1[root@localhost conf]# hbase shell 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117显示hbase中的表list创建user表，包含info、data两个列族create &apos;user&apos;, &apos;info1&apos;, &apos;data1&apos;create &apos;user&apos;, &#123;NAME =&gt; &apos;info&apos;, VERSIONS =&gt; 3&#125;向user表中插入信息，row key为rk0001，列族info中添加name列标示符，值为zhangsanput &apos;user&apos;, &apos;rk0001&apos;, &apos;info:name&apos;, &apos;zhangsan&apos;向user表中插入信息，row key为rk0001，列族info中添加gender列标示符，值为femaleput &apos;user&apos;, &apos;rk0001&apos;, &apos;info:gender&apos;, &apos;female&apos;向user表中插入信息，row key为rk0001，列族info中添加age列标示符，值为20put &apos;user&apos;, &apos;rk0001&apos;, &apos;info:age&apos;, 20向user表中插入信息，row key为rk0001，列族data中添加pic列标示符，值为pictureput &apos;user&apos;, &apos;rk0001&apos;, &apos;data:pic&apos;, &apos;picture&apos;获取user表中row key为rk0001的所有信息get &apos;user&apos;, &apos;rk0001&apos;获取user表中row key为rk0001，info列族的所有信息get &apos;user&apos;, &apos;rk0001&apos;, &apos;info&apos;获取user表中row key为rk0001，info列族的name、age列标示符的信息get &apos;user&apos;, &apos;rk0001&apos;, &apos;info:name&apos;, &apos;info:age&apos;获取user表中row key为rk0001，info、data列族的信息get &apos;user&apos;, &apos;rk0001&apos;, &apos;info&apos;, &apos;data&apos;get &apos;user&apos;, &apos;rk0001&apos;, &#123;COLUMN =&gt; [&apos;info&apos;, &apos;data&apos;]&#125;get &apos;user&apos;, &apos;rk0001&apos;, &#123;COLUMN =&gt; [&apos;info:name&apos;, &apos;data:pic&apos;]&#125;获取user表中row key为rk0001，列族为info，版本号最新5个的信息get &apos;people&apos;, &apos;rk0002&apos;, &#123;COLUMN =&gt; &apos;info&apos;, VERSIONS =&gt; 2&#125;get &apos;user&apos;, &apos;rk0001&apos;, &#123;COLUMN =&gt; &apos;info:name&apos;, VERSIONS =&gt; 5&#125;get &apos;user&apos;, &apos;rk0001&apos;, &#123;COLUMN =&gt; &apos;info:name&apos;, VERSIONS =&gt; 5, TIMERANGE =&gt; [1392368783980, 1392380169184]&#125;获取user表中row key为rk0001，列标示符中含有a的信息get &apos;people&apos;, &apos;rk0001&apos;, &#123;FILTER =&gt; &quot;(QualifierFilter(=,&apos;substring:a&apos;))&quot;&#125;put &apos;user&apos;, &apos;rk0002&apos;, &apos;info:name&apos;, &apos;fanbingbing&apos;put &apos;user&apos;, &apos;rk0002&apos;, &apos;info:gender&apos;, &apos;female&apos;put &apos;user&apos;, &apos;rk0002&apos;, &apos;info:nationality&apos;, &apos;中国&apos;get &apos;user&apos;, &apos;rk0002&apos;, &#123;FILTER =&gt; &quot;ValueFilter(=, &apos;binary:中国&apos;)&quot;&#125;查询user表中的所有信息scan &apos;user&apos;查询user表中列族为info的信息scan &apos;people&apos;, &#123;COLUMNS =&gt; &apos;info&apos;&#125;scan &apos;user&apos;, &#123;COLUMNS =&gt; &apos;info&apos;, RAW =&gt; true, VERSIONS =&gt; 5&#125;scan &apos;persion&apos;, &#123;COLUMNS =&gt; &apos;info&apos;, RAW =&gt; true, VERSIONS =&gt; 3&#125;查询user表中列族为info和data的信息scan &apos;user&apos;, &#123;COLUMNS =&gt; [&apos;info&apos;, &apos;data&apos;]&#125;scan &apos;user&apos;, &#123;COLUMNS =&gt; [&apos;info:name&apos;, &apos;data:pic&apos;]&#125;查询user表中列族为info、列标示符为name的信息scan &apos;user&apos;, &#123;COLUMNS =&gt; &apos;info:name&apos;&#125;查询user表中列族为info、列标示符为name的信息,并且版本最新的5个scan &apos;user&apos;, &#123;COLUMNS =&gt; &apos;info:name&apos;, VERSIONS =&gt; 5&#125;查询user表中列族为info和data且列标示符中含有a字符的信息scan &apos;people&apos;, &#123;COLUMNS =&gt; [&apos;info&apos;, &apos;data&apos;], FILTER =&gt; &quot;(QualifierFilter(=,&apos;substring:a&apos;))&quot;&#125;查询user表中列族为info，rk范围是[rk0001, rk0003)的数据scan &apos;people&apos;, &#123;COLUMNS =&gt; &apos;info&apos;, STARTROW =&gt; &apos;rk0001&apos;, ENDROW =&gt; &apos;rk0003&apos;&#125; 查询user表中row key以rk字符开头的scan &apos;user&apos;,&#123;FILTER=&gt;&quot;PrefixFilter(&apos;rk&apos;)&quot;&#125;查询user表中指定范围的数据scan &apos;user&apos;, &#123;TIMERANGE =&gt; [1392368783980, 1392380169184]&#125;删除数据删除user表row key为rk0001，列标示符为info:name的数据delete &apos;people&apos;, &apos;rk0001&apos;, &apos;info:name&apos;删除user表row key为rk0001，列标示符为info:name，timestamp为1392383705316的数据delete &apos;user&apos;, &apos;rk0001&apos;, &apos;info:name&apos;, 1392383705316清空user表中的数据truncate &apos;people&apos;修改表结构首先停用user表（新版本不用）disable &apos;user&apos;添加两个列族f1和f2alter &apos;people&apos;, NAME =&gt; &apos;f1&apos;alter &apos;user&apos;, NAME =&gt; &apos;f2&apos;启用表enable &apos;user&apos;###disable &apos;user&apos;(新版本不用)删除一个列族：alter &apos;user&apos;, NAME =&gt; &apos;f1&apos;, METHOD =&gt; &apos;delete&apos; 或 alter &apos;user&apos;, &apos;delete&apos; =&gt; &apos;f1&apos;添加列族f1同时删除列族f2alter &apos;user&apos;, &#123;NAME =&gt; &apos;f1&apos;&#125;, &#123;NAME =&gt; &apos;f2&apos;, METHOD =&gt; &apos;delete&apos;&#125;将user表的f1列族版本号改为5alter &apos;people&apos;, NAME =&gt; &apos;info&apos;, VERSIONS =&gt; 5启用表enable &apos;user&apos;删除表disable &apos;user&apos;drop &apos;user&apos;]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>HBase</tag>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HBase单节点的安装以及shell操作]]></title>
    <url>%2F2018%2F10%2F25%2Fblog181025-1%2F</url>
    <content type="text"><![CDATA[概览1.HBase简介2.上传解压3.修改配置文件4.启动5.进入客户端进行shell操作 1.HBase简介 HBase – Hadoop Database，是一个高可靠性、高性能、面向列、可伸缩的分布式存储系统，利用HBase技术可在廉价PC Server上搭建起大规模结构化存储集群。 HBase是一个分布式的、面向列的开源数据库，该技术来源于 Fay Chang 所撰写的Google论文“Bigtable：一个结构化数据的分布式存储系统”。就像Bigtable利用了Google文件系统（File System）所提供的分布式数据存储一样，HBase在Hadoop之上提供了类似于Bigtable的能力。HBase是Apache的Hadoop项目的子项目。HBase不同于一般的关系数据库，它是一个适合于非结构化数据存储的数据库。另一个不同的是HBase基于列的而不是基于行的模式。 与FUJITSU Cliq等商用大数据产品不同，HBase是Google Bigtable的开源实现，类似Google Bigtable利用GFS作为其文件存储系统，HBase利用Hadoop HDFS作为其文件存储系统；Google运行MapReduce来处理Bigtable中的海量数据，HBase同样利用Hadoop MapReduce来处理HBase中的海量数据；Google Bigtable利用Chubby作为协同服务，HBase利用Zookeeper作为对应。 2.上传解压准备:1.hbase-2.0.02.XShell 53.Xftp 5 使用Xshell工具(官网下载免费版本即可)连接虚拟机 在usr下创建一个hbase文件夹作为压缩包存放路径和安装路径 123456[root@localhost ~]# cd /usr/[root@localhost usr]# lsbin etc games hadoop hive include java lib lib64 libexec local sbin share sqoop src tmp zookeeper[root@localhost usr]# mkdir hbase[root@localhost usr]# lsbin etc games hadoop hbase hive include java lib lib64 libexec local sbin share sqoop src tmp zookeeper 利用Xftp将HBase的压缩包上传到hbase文件夹下 1[root@localhost usr]# cd hbase 然后解压 1[root@localhost hbase]# tar -zxf hbase-2.0.0-bin.tar.gz 3.修改配置文件进入到hbase安装目录下的conf文件夹下修改配置文件 3.1 .hbase-env.sh 123[root@localhost conf]# lshadoop-metrics2-hbase.properties hbase-env.cmd hbase-env.sh hbase-policy.xml hbase-site.xml log4j.properties regionservers[root@localhost conf]# vim hbase-env.sh 在第28行左右找到这个将其修改为你的jdk安装路径,jdk的安装参考Hadoop集群单机版搭建中的jdk安装 123# The java implementation to use. Java 1.8+ required.#这是我的jdk安装路径 export JAVA_HOME=/usr/java/jdk1.8.0_141 3.2 .hbase-site.xml 1[root@localhost conf]# vim hbase-site.xml 在configuration中添加 1234&lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;file:///usr/hbase/data&lt;/value&gt;&lt;/property&gt; 3.3 .profile环境变量 1[root@localhost conf]# vim /etc/profile 在最后添加 12export HBASE_HOME=/usr/hbase/hbase-2.0.0export PATH=$PATH:$HBASE_HOME/bin 保存退出,刷新 1[root@localhost conf]# source /etc/profile 4.启动首先确保你的hadoop启动start-all.sh如果没有hadoop参考Hadoop单机版搭建1234567[root@localhost conf]# start-hbase.sh SLF4J: Class path contains multiple SLF4J bindings.SLF4J: Found binding in [jar:file:/usr/hbase/hbase-2.0.0/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: Found binding in [jar:file:/usr/hadoop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]running master, logging to /usr/hbase/hbase-2.0.0/logs/hbase-root-master-zhiyou.out 在浏览器查看ip:16010(注意关闭防火墙systemctl stop firewalld(CentOS7.0)) 5.进入客户端进行shell操作1[root@localhost conf]# hbase shell 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117显示hbase中的表list创建user表，包含info、data两个列族create &apos;user&apos;, &apos;info1&apos;, &apos;data1&apos;create &apos;user&apos;, &#123;NAME =&gt; &apos;info&apos;, VERSIONS =&gt; 3&#125;向user表中插入信息，row key为rk0001，列族info中添加name列标示符，值为zhangsanput &apos;user&apos;, &apos;rk0001&apos;, &apos;info:name&apos;, &apos;zhangsan&apos;向user表中插入信息，row key为rk0001，列族info中添加gender列标示符，值为femaleput &apos;user&apos;, &apos;rk0001&apos;, &apos;info:gender&apos;, &apos;female&apos;向user表中插入信息，row key为rk0001，列族info中添加age列标示符，值为20put &apos;user&apos;, &apos;rk0001&apos;, &apos;info:age&apos;, 20向user表中插入信息，row key为rk0001，列族data中添加pic列标示符，值为pictureput &apos;user&apos;, &apos;rk0001&apos;, &apos;data:pic&apos;, &apos;picture&apos;获取user表中row key为rk0001的所有信息get &apos;user&apos;, &apos;rk0001&apos;获取user表中row key为rk0001，info列族的所有信息get &apos;user&apos;, &apos;rk0001&apos;, &apos;info&apos;获取user表中row key为rk0001，info列族的name、age列标示符的信息get &apos;user&apos;, &apos;rk0001&apos;, &apos;info:name&apos;, &apos;info:age&apos;获取user表中row key为rk0001，info、data列族的信息get &apos;user&apos;, &apos;rk0001&apos;, &apos;info&apos;, &apos;data&apos;get &apos;user&apos;, &apos;rk0001&apos;, &#123;COLUMN =&gt; [&apos;info&apos;, &apos;data&apos;]&#125;get &apos;user&apos;, &apos;rk0001&apos;, &#123;COLUMN =&gt; [&apos;info:name&apos;, &apos;data:pic&apos;]&#125;获取user表中row key为rk0001，列族为info，版本号最新5个的信息get &apos;people&apos;, &apos;rk0002&apos;, &#123;COLUMN =&gt; &apos;info&apos;, VERSIONS =&gt; 2&#125;get &apos;user&apos;, &apos;rk0001&apos;, &#123;COLUMN =&gt; &apos;info:name&apos;, VERSIONS =&gt; 5&#125;get &apos;user&apos;, &apos;rk0001&apos;, &#123;COLUMN =&gt; &apos;info:name&apos;, VERSIONS =&gt; 5, TIMERANGE =&gt; [1392368783980, 1392380169184]&#125;获取user表中row key为rk0001，列标示符中含有a的信息get &apos;people&apos;, &apos;rk0001&apos;, &#123;FILTER =&gt; &quot;(QualifierFilter(=,&apos;substring:a&apos;))&quot;&#125;put &apos;user&apos;, &apos;rk0002&apos;, &apos;info:name&apos;, &apos;fanbingbing&apos;put &apos;user&apos;, &apos;rk0002&apos;, &apos;info:gender&apos;, &apos;female&apos;put &apos;user&apos;, &apos;rk0002&apos;, &apos;info:nationality&apos;, &apos;中国&apos;get &apos;user&apos;, &apos;rk0002&apos;, &#123;FILTER =&gt; &quot;ValueFilter(=, &apos;binary:中国&apos;)&quot;&#125;查询user表中的所有信息scan &apos;user&apos;查询user表中列族为info的信息scan &apos;people&apos;, &#123;COLUMNS =&gt; &apos;info&apos;&#125;scan &apos;user&apos;, &#123;COLUMNS =&gt; &apos;info&apos;, RAW =&gt; true, VERSIONS =&gt; 5&#125;scan &apos;persion&apos;, &#123;COLUMNS =&gt; &apos;info&apos;, RAW =&gt; true, VERSIONS =&gt; 3&#125;查询user表中列族为info和data的信息scan &apos;user&apos;, &#123;COLUMNS =&gt; [&apos;info&apos;, &apos;data&apos;]&#125;scan &apos;user&apos;, &#123;COLUMNS =&gt; [&apos;info:name&apos;, &apos;data:pic&apos;]&#125;查询user表中列族为info、列标示符为name的信息scan &apos;user&apos;, &#123;COLUMNS =&gt; &apos;info:name&apos;&#125;查询user表中列族为info、列标示符为name的信息,并且版本最新的5个scan &apos;user&apos;, &#123;COLUMNS =&gt; &apos;info:name&apos;, VERSIONS =&gt; 5&#125;查询user表中列族为info和data且列标示符中含有a字符的信息scan &apos;people&apos;, &#123;COLUMNS =&gt; [&apos;info&apos;, &apos;data&apos;], FILTER =&gt; &quot;(QualifierFilter(=,&apos;substring:a&apos;))&quot;&#125;查询user表中列族为info，rk范围是[rk0001, rk0003)的数据scan &apos;people&apos;, &#123;COLUMNS =&gt; &apos;info&apos;, STARTROW =&gt; &apos;rk0001&apos;, ENDROW =&gt; &apos;rk0003&apos;&#125; 查询user表中row key以rk字符开头的scan &apos;user&apos;,&#123;FILTER=&gt;&quot;PrefixFilter(&apos;rk&apos;)&quot;&#125;查询user表中指定范围的数据scan &apos;user&apos;, &#123;TIMERANGE =&gt; [1392368783980, 1392380169184]&#125;删除数据删除user表row key为rk0001，列标示符为info:name的数据delete &apos;people&apos;, &apos;rk0001&apos;, &apos;info:name&apos;删除user表row key为rk0001，列标示符为info:name，timestamp为1392383705316的数据delete &apos;user&apos;, &apos;rk0001&apos;, &apos;info:name&apos;, 1392383705316清空user表中的数据truncate &apos;people&apos;修改表结构首先停用user表（新版本不用）disable &apos;user&apos;添加两个列族f1和f2alter &apos;people&apos;, NAME =&gt; &apos;f1&apos;alter &apos;user&apos;, NAME =&gt; &apos;f2&apos;启用表enable &apos;user&apos;###disable &apos;user&apos;(新版本不用)删除一个列族：alter &apos;user&apos;, NAME =&gt; &apos;f1&apos;, METHOD =&gt; &apos;delete&apos; 或 alter &apos;user&apos;, &apos;delete&apos; =&gt; &apos;f1&apos;添加列族f1同时删除列族f2alter &apos;user&apos;, &#123;NAME =&gt; &apos;f1&apos;&#125;, &#123;NAME =&gt; &apos;f2&apos;, METHOD =&gt; &apos;delete&apos;&#125;将user表的f1列族版本号改为5alter &apos;people&apos;, NAME =&gt; &apos;info&apos;, VERSIONS =&gt; 5启用表enable &apos;user&apos;删除表disable &apos;user&apos;drop &apos;user&apos; 接下来会介绍HBase集群搭建]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>HBase</tag>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hiveserver2的配置和启动]]></title>
    <url>%2F2018%2F10%2F24%2Fblog181024-1%2F</url>
    <content type="text"><![CDATA[概览1.配置hive-site.xml2.配置hadoop中的core-site.xml文件3.测试 首先将hive安装 然后更改部分配置文件 1.配置hive-site.xml利用Xshell工具连接虚拟机 进入hive安装目录下的conf文件夹,更改hive-site.xml配置文件其中配置参考hive安装中的配置hive-site.xml 你会发现/usr/hive/apache-hive-2.3.3-bin/conf/并没有hive-site.xml文件直接新建一个就行,如果之前安装hive的时候加上了hiveserver2的配置,可以跳过这步 12345[root@master hive]# cd apache-hive-2.3.3-bin/conf/#创建文件[root@master conf]# touch hive-site.xml#编辑[root@master conf]# vim hive-site.xml 在hive-site.xml中写入 ip改为自己的12345678910111213141516171819202122232425262728293031323334353637383940414243444546&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;no&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://192.168.134.154:3306/hive&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.metastore.schema.verification&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.cli.print.current.db&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.cli.print.header&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 这是hiveserver2 --&gt; &lt;property&gt; &lt;name&gt;hive.server2.thrift.port&lt;/name&gt; &lt;value&gt;10000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.server2.thrift.bind.host&lt;/name&gt; &lt;value&gt;192.168.134.154&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 2.配置hadoop中的core-site.xml文件进入到hadoop的安装目录下的etc/hadoop文件夹,配置其中的core-site.xml文件 注意:如果是hadoop集群而不是单机版,需要更改所有虚拟机上的core-site.xml文件 在最后追加 123456789101112131415161718&lt;!-- 如果连接不上10000 --&gt;&lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.proxyuser.zhaoshb.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.zhaoshb.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; 3.测试首先启动你集群中的Zookeeper和Hadoop 然后在安装hive的主机上启动hiveserver2 因为配置好了hive的环境变量.所以我们可以直接在任何文件夹下运行不然的话需要进到hive安装目录的bin下 123[root@master ~]# hiveserver2#或者[root@master ~]# hive --service hiveserver2 这个时候你的页面会一直卡在这里,这是正常的,因为你是启动了一个服务 你只需要再开一个Xshell窗口连接即可 在新开的窗口操作 1234567#进入beeline[root@master ~]# beeline#连接10000端口beeline&gt; !connect jdbc:hive2://192.168.134.154:10000Connecting to jdbc:hive2://192.168.134.154:10000Enter username for jdbc:hive2://192.168.134.154:10000: root #用户名rootEnter password for jdbc:hive2://192.168.134.154:10000: **** #密码root 然后使用浏览器在192.168.134.154(安装hive的主机ip地址):10002查看 接下来可以在hiveserver2中操作,基本操作和hive中一样 12345670: jdbc:hive2://192.168.134.154:10000&gt; show databases;+----------------+| database_name |+----------------+| default |+----------------+1 row selected (2.51 seconds) 接下来会说hbase的安装]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hive</tag>
        <tag>Hiveserver2</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive的安装]]></title>
    <url>%2F2018%2F10%2F23%2Fblog181023-3%2F</url>
    <content type="text"><![CDATA[概览1.上传解压2.配置环境变量3.配置hive-site.xml4.将mysql的连接驱动放到hive/lib下5.初始化操作6.执行hive命令并测试7.异常处理 如果是集群版,Hive安装在一台虚拟机上就行 首先确保hdfs和mysql是正确启动的 1.上传解压软件:hive-2.3.3 在/usr在创建hive文件夹,用来存放压缩包和作为安装路径 1234[root@master ~]# cd /usr[root@master usr]# mkdir hive[root@master usr]# lsbin etc games hadoop hive include java lib lib64 libexec local sbin share sqoop src tmp zookeeper 利用Xshell连接虚拟机,并使用Xftp将hive压缩包上传到hive文件夹下解压 1[root@master hive]# tar -zxf apache-hive-2.3.3-bin.tar.gz 2.配置环境变量1[root@master hive]# vim /etc/profile 在最后添加12export HIVE_HOME=/usr/hive/apache-hive-2.3.3-binexport PATH=$PATH:$HIVE_HOME/bin 刷新环境变量并测试 123456[root@master hive]# source /etc/profile[root@master hive]# hive --versionHive 2.3.3Git git://daijymacpro-2.local/Users/daijy/commit/hive -r 8a511e3f79b43d4be41cd231cf5c99e43b248383Compiled by daijy on Wed Mar 28 16:58:33 PDT 2018From source with checksum 8873bba6c55a058614e74c0e628ab022 3.配置hive-site.xml你会发现/usr/hive/apache-hive-2.3.3-bin/conf/并没有hive-site.xml文件直接新建一个就行 12345[root@master hive]# cd apache-hive-2.3.3-bin/conf/#创建文件[root@master conf]# touch hive-site.xml#编辑[root@master conf]# vim hive-site.xml 在hive-site.xml中写入 ip改为自己的12345678910111213141516171819202122232425262728293031323334353637383940414243444546&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;no&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://192.168.134.154:3306/hive&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.metastore.schema.verification&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.cli.print.current.db&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.cli.print.header&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 这是hiveserver2 --&gt; &lt;property&gt; &lt;name&gt;hive.server2.thrift.port&lt;/name&gt; &lt;value&gt;10000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.server2.thrift.bind.host&lt;/name&gt; &lt;value&gt;192.168.134.154&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 4.将mysql的连接驱动放到hive/lib下进入到你的hive安装目录的lib下,利用Xftp将驱动传输进去我的mysql数据库连接驱动的百度云链接：https://pan.baidu.com/s/1wCBuZQaCP_nKT5t504SeoA提取码：fakd 5.初始化操作利用数据库工具SQLyog在mysql创建个新的数据库hive 或者使用命令创建 123#之后输入密码,我的mysql密码是root(因为之前配置sqoop的时候设置了一次user表)[root@master ~]# mysql -u root -p mysql&gt;create database hive; 此时保证hadoop启动着初始化操作如下 1[root@master conf]# schematool -initSchema -dbType mysql 6.执行hive命令并测试12345[root@master conf]# hivehive&gt; show databases;OKdefaultTime taken: 7.897 seconds, Fetched: 1 row(s) 7.异常处理如果出错了12hive&gt; show databases;FAILED: SemanticException org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient HIVE_HOME/lib 下的derby-10.11.1.1.jar问题，把derby-10.10.2.0.jar 换成derby-10.10.1.1.jar,把derby-10.10.2.0.jar删除,问题成功解决 derby-10.10.1.1.jar的百度云链接：https://pan.baidu.com/s/1v6mIGRjf8mlRQicRoau7iw提取码：5ga1 其中hive-site.xml中的部分hiveserver2的配置是用于hiveserver2的,现在可以不配置,之后会介绍如何配置hiveserver2]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Hive</tag>
        <tag>Hiveserver2</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sqoop的安装]]></title>
    <url>%2F2018%2F10%2F23%2Fblog181023-2%2F</url>
    <content type="text"><![CDATA[概览1.sqoop简介2.sqoop的安装和配置3.测试4.异常处理 1.sqoop简介Sqoop(发音：skup)是一款开源的工具，主要用于在Hadoop(Hive)与传统的数据库(mysql、postgresql…)间进行数据的传递，可以将一个关系型数据库（例如 ： MySQL ,Oracle ,Postgres等）中的数据导进到Hadoop的HDFS中，也可以将HDFS的数据导进到关系型数据库中。 sqoop 是 apache 旗下一款“Hadoop 和关系数据库服务器之间传送数据”的工具。 核心的功能有两个：导入和导出! 导入数据：MySQL，Oracle 导入数据到 Hadoop 的 HDFS、HIVE、HBASE 等数据存储系统 导出数据：从 Hadoop 的文件系统中导出数据到关系数据库 mysql 等 Sqoop 的本质还是一个命令行工具，和 HDFS，Hive 相比，并没有什么高深的理论。 sqoop： 工具：本质就是迁移数据， 迁移的方式：就是把sqoop的迁移命令转换成MR程序 hive 工具，本质就是执行计算，依赖于HDFS存储数据，把SQL转换成MR程序 工作原理是将导入或导出命令翻译成 MapReduce 程序来实现 在翻译出的 MapReduce 中主要是对 InputFormat 和 OutputFormat 进行定制 2.sqoop的安装和配置软件:sqoop-1.4.7.bin__hadoop-2.6.0.tar2.1.安装在/usr下创建个sqoop文件夹,作为压缩包的存放路径和解压路径 123456789#进入/usr下[root@master ~]# cd /usr/#创建sqoop文件夹[root@master usr]# mkdir sqoop[root@master usr]# lsbin etc games hadoop include java lib lib64 libexec local sbin share sqoop src tmp zookeeper#进入sqoop下[root@master usr]# cd sqoop[root@master sqoop]# 利用Xshell连接虚拟机,并利用Xftp将sqoop压缩包上传到sqoop文件夹下解压sqoop 1[root@master sqoop]# tar -zxf sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz 2.2.配置profile环境变量进入到根目录的etc文件夹下,更改profile文件 1[root@master sqoop]# vim /etc/profile 在profile最后添加 12export SQOOP_HOME=/usr/sqoop/sqoop-1.4.7.bin__hadoop-2.6.0export PATH=$PATH:$SQOOP_HOME/bin 保存退出,刷新profile 1[root@master sqoop]# source /etc/profile 2.3拷贝驱动 将数据库连接驱动拷贝到$SQOOP_HOME(sqoop安装目录)/lib里进入到你的sqoop安装目录的lib下,利用Xftp将驱动传输进去我的mysql数据库连接驱动的百度云链接：https://pan.baidu.com/s/1wCBuZQaCP_nKT5t504SeoA提取码：fakd 2.4.使用前准备,mysql允许远程连接如果你在安装mysql的时候已经允许过了,就可以跳过这个步骤 123456#进入mysql[root@master sqoop]# mysql#允许远程连接mysql&gt;GRANT ALL PRIVILEGES ON *.* TO &apos;root&apos;@&apos;%&apos; IDENTIFIED BY &apos;root&apos; WITH GRANT OPTION;#刷新权限mysql&gt;FLUSH PRIVILEGES; 2.5.集群配置如果是集群可以将sqoop发送到其他主机上,伪集群版可以跳过发送sqoop123[root@master sqoop]# scp -r /usr/sqoop root@slave1:/usr/[root@master sqoop]# scp -r /usr/sqoop root@slave2:/usr/ 发送配置好的profile文件 123[root@master sqoop]# scp -r /etc/profile root@slave1:/etc/[root@master sqoop]# scp -r /etc/profile root@slave2:/etc/ 分别刷新profile 123[root@slave1 ~]# source /etc/profile[root@slave2 ~]# source /etc/profile 3.测试关闭防火墙 1systemctl stop firewalld 分别在集群的Zookeeper安装目录下的bin下启动Zookeeper 123456[root@master sqoop]# cd /usr/zookeeper/zookeeper-3.4.12/bin/#启动Zookeeper[root@master bin]# ./zkServer.sh startZooKeeper JMX enabled by defaultUsing config: /usr/zookeeper/zookeeper-3.4.12/bin/../conf/zoo.cfgStarting zookeeper ... STARTED 确保的你的集群正常启动,请参考Hadoop HA高可用集群搭建 1[root@master bin]# start-all.sh 然后随便在哪个目录下(已经设置完成环境变量)输入 12#mysql后是我安装mysql主机的ip地址[root@master bin]# sqoop list-databases --connect jdbc:mysql://192.168.134.154:3306/ --username root --password root 在分别在别的虚拟机使用sqoop看结果是否相同如上图则说明sqoop安装成功了 4.异常处理如果你出了类似的异常 12345618/10/23 21:07:17 ERROR manager.CatalogQueryManager: Failed to list databasesjava.sql.SQLException: Access denied for user &apos;root&apos;@&apos;master&apos; (using password: YES) at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1094) at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4208) at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4140) at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:925) 利用SQLyog或者Navicat连接虚拟机数据库 将%这行的password复制给上面所以没有password的保存然后重启数据库服务 1[root@master bin]# systemctl restart mysqld 再次尝试,应该就能成功了 1[root@master bin]# sqoop list-databases --connect jdbc:mysql://192.168.134.154:3306/ --username root --password root 分别在别的虚拟机上查看是否成功 123[root@slave1 sbin]# sqoop list-databases --connect jdbc:mysql://192.168.134.154:3306/ --username root --password root[root@slave2 sbin]# sqoop list-databases --connect jdbc:mysql://192.168.134.154:3306/ --username root --password root]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Sqoop</tag>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql的安装(Linux)]]></title>
    <url>%2F2018%2F10%2F23%2Fblog181023-1%2F</url>
    <content type="text"><![CDATA[概览1.安装mysql客户端2.数据库字符集设置3.启动mysql服务4.测试mysql命令5.设置root密码6.设置mysql运行远程访问7.设置开机自启8.测试连接如果是集群版的安装在你规划的需要安装mysql的虚拟机(服务器)上安装 1.安装mysql客户端1.1 安装wget命令 1yum -y install wget 1.2 下载mysql的repo源 1wget http://repo.mysql.com/mysql-community-release-el7-5.noarch.rpm 1.3 安装 安装1.3.mysql-community-release-el7-5.noarch.rpm包 1rpm -ivh mysql-community-release-el7-5.noarch.rpm 1.4 安装mysql客户端 123yum install mysql-serveryun install mysql-devel 等待安装完毕 2.数据库字符集设置配置mysql文件: 1vim /etc/my.cnf 在最后添加配置参数 1character-set-server=utf8 3.启动mysql服务123service mysqld start或者systemctl start mysqld 4.测试mysql命令输入mysql命令 12345678910111213[root@master ~]# systemctl start mysqld[root@master ~]# mysqlWelcome to the MySQL monitor. Commands end with ; or \g.Your MySQL connection id is 2Server version: 5.6.42 MySQL Community Server (GPL)Copyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type &apos;help;&apos; or &apos;\h&apos; for help. Type &apos;\c&apos; to clear the current input statement. 5.设置root密码12345mysql&gt; use mysqlReading table information for completion of table and column namesYou can turn off this feature to get a quicker startup with -ADatabase changed 设置密码 123mysql&gt; update user set authentication_string = password(&apos;root&apos;), password_expired = &apos;N&apos; where user = &apos;root&apos;;Query OK, 4 rows affected (0.00 sec)Rows matched: 4 Changed: 4 Warnings: 0 6.设置mysql运行远程访问12345mysql&gt; GRANT ALL PRIVILEGES ON *.* TO &apos;root&apos;@&apos;%&apos; IDENTIFIED BY &apos;root&apos; WITH GRANT OPTION;Query OK, 0 rows affected (0.00 sec)mysql&gt; flush privileges; #刷新权限Query OK, 0 rows affected (0.00 sec) 7.设置开机自启123456789101112131415161718192021mysql&gt; exit;Bye[root@master ~]# vim /etc/rc.local #!/bin/bash# THIS FILE IS ADDED FOR COMPATIBILITY PURPOSES## It is highly advisable to create own systemd services or udev rules# to run scripts during boot instead of using this file.## In contrast to previous versions due to parallel execution during boot# this script will NOT be run after all other services.## Please note that you must run &apos;chmod +x /etc/rc.d/rc.local&apos; to ensure# that this script will be executed during boot.touch /var/lock/subsys/local#追加内容service mysqld start~ 8.测试连接使用SQLyog或者Navicat工具测试连接,注意将防火墙关闭systemctl stop firewalld 测试连接成功则代表安装成功]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>mysql</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop中JournalNode的作用]]></title>
    <url>%2F2018%2F10%2F22%2Fblog181022-2%2F</url>
    <content type="text"><![CDATA[NameNode之间共享数据（NFS 、Quorum Journal Node（用得多）） 两个NameNode为了数据同步，会通过一组称作JournalNodes的独立进程进行相互通信。当active状态的NameNode的命名空间有任何修改时，会告知大部分的JournalNodes进程。standby状态的NameNode有能力读取JNs中的变更信息，并且一直监控edit log的变化，把变化应用于自己的命名空间。standby可以确保在集群出错时，命名空间状态已经完全同步了。 Hadoop中的NameNode好比是人的心脏，非常重要，绝对不可以停止工作。在hadoop1时代，只有一个NameNode。如果该NameNode数据丢失或者不能工作，那么整个集群就不能恢复了。这是hadoop1中的单点问题，也是hadoop1不可靠的表现，如图1所示。hadoop2就解决了这个问题。图1 hadoop2.2.0（HA）中HDFS的高可靠指的是可以同时启动2个NameNode。其中一个处于工作状态，另一个处于随时待命状态。这样，当一个NameNode所在的服务器宕机时，可以在数据不丢失的情况下，手工或者自动切换到另一个NameNode提供服务。 这些NameNode之间通过共享数据，保证数据的状态一致。多个NameNode之间共享数据，可以通过Nnetwork File System或者Quorum Journal Node。前者是通过linux共享的文件系统，属于操作系统的配置；后者是hadoop自身的东西，属于软件的配置。 我们这里讲述使用Quorum Journal Node的配置方式，方式是手工切换。 集群启动时，可以同时启动2个NameNode。这些NameNode只有一个是active的，另一个属于standby状态。active状态意味着提供服务，standby状态意味着处于休眠状态，只进行数据同步，时刻准备着提供服务，如图2所示。图2 在一个典型的HA集群中，每个NameNode是一台独立的服务器。在任一时刻，只有一个NameNode处于active状态，另一个处于standby状态。其中，active状态的NameNode负责所有的客户端操作，standby状态的NameNode处于从属地位，维护着数据状态，随时准备切换。`两个NameNode为了数据同步，会通过一组称作JournalNodes的独立进程进行相互通信。当active状态的NameNode的命名空间有任何修改时，会告知大部分的JournalNodes进程。standby状态的NameNode有能力读取JNs中的变更信息，并且一直监控edit log的变化，把变化应用于自己的命名空间。standby可以确保在集群出错时，命名空间状态已经完全同步了，如图3所示。图3 为了确保快速切换，standby状态的NameNode有必要知道集群中所有数据块的位置。为了做到这点，所有的datanodes必须配置两个NameNode的地址，发送数据块位置信息和心跳给他们两个。 对于HA集群而言，确保同一时刻只有一个NameNode处于active状态是至关重要的。否则，两个NameNode的数据状态就会产生分歧，可能丢失数据，或者产生错误的结果。为了保证这点，JNs必须确保同一时刻只有一个NameNode可以向自己写数据。 硬件资源 为了部署HA集群，应该准备以下事情： NameNode服务器：运行NameNode的服务器应该有相同的硬件配置。 JournalNode服务器：运行的JournalNode进程非常轻量，可以部署在其他的服务器上。注意：必须允许至少3个节点。当然可以运行更多，但是必须是奇数个，如3、5、7、9个等等。当运行N个节点时，系统可以容忍至少(N-1)/2(N至少为3)个节点失败而不影响正常运行。 在HA集群中，standby状态的NameNode可以完成checkpoint操作，因此没必要配置Secondary NameNode、CheckpointNode、BackupNode。如果真的配置了，还会报错。]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>JournalNode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop 2.0 HA 3节点高可用集群搭建]]></title>
    <url>%2F2018%2F10%2F22%2Fblog181022-1%2F</url>
    <content type="text"><![CDATA[概览1.集群规划2.准备3.修改Hadoop配置文件4.复制内容5.启动集群6.查看jps7.测试 1.集群规划 HDFS HA背景 HDFS集群中NameNode 存在单点故障（SPOF）。对于只有一个NameNode的集群，如果NameNode机器出现意外情况，将导致整个集群无法使用，直到NameNode 重新启动。 影响HDFS集群不可用主要包括以下两种情况：一是NameNode机器宕机，将导致集群不可用，重启NameNode之后才可使用；二是计划内的NameNode节点软件或硬件升级，导致集群在短时间内不可用。 为了解决上述问题，Hadoop给出了HDFS的高可用HA方案：HDFS通常由两个NameNode组成，一个处于active状态，另一个处于standby状态。Active NameNode对外提供服务，比如处理来自客户端的RPC请求，而Standby NameNode则不对外提供服务，仅同步Active NameNode的状态，以便能够在它失败时快速进行切换。 规划之后的服务启动和配置文件都是安装此配置来,master上是namenode,slave2上是yarn,而slave1则是namenode和yarn的备用 需要说明以下几点： HDFS HA通常由两个NameNode组成，一个处于Active状态，另一个处于Standby状态。Active NameNode对外提供服务，而Standby NameNode则不对外提供服务，仅同步Active NameNode的状态，以便能够在它失败时快速进行切换。 Hadoop 2.0官方提供了两种HDFS HA的解决方案，一种是NFS，另一种是QJM。这里我们使用简单的QJM。在该方案中，主备NameNode之间通过一组JournalNode同步元数据信息，一条数据只要成功写入多数JournalNode即认为写入成功。通常配置奇数个JournalNode，这里还配置了一个Zookeeper集群，用于ZKFC故障转移，当Active NameNode挂掉了，会自动切换Standby NameNode为Active状态。 YARN的ResourceManager也存在单点故障问题，这个问题在hadoop-2.4.1得到了解决：有两个ResourceManager，一个是Active，一个是Standby，状态由zookeeper进行协调。 YARN框架下的MapReduce可以开启JobHistoryServer来记录历史任务信息，否则只能查看当前正在执行的任务信息。 Zookeeper的作用是负责HDFS中NameNode主备节点的选举，和YARN框架下ResourceManaer主备节点的选举。 2.准备软件:1.jdk1.8.1412.hadoop2.7.3(jdk1.8版本编译)3.Zookeeper3.4.124.Xshell5 + Xftp5 2.1.设置静态ip,参考Hadoop集群单机版的设置静态ip,然后使用Xshell工具连接(官网有免费版本) 2.2.配置jdk,hosts文件jdk安装参考Hadoop集群单机版的jdk安装123456789[root@master bin]# vi /etc/hosts#127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 #::1 localhost localhost.localdomain localhost6 localhost6.localdomain6# 上面的给注释掉或者删除192.168.134.154 master192.168.134.155 slave1192.168.134.156 slave2 2.3.配置ssh免密登录,参考Hadoop集群搭建的ssh免密登录 2.4.配置Zookeeper,参考Zookeeper的安装 3.修改Hadoop配置文件如果你之前搭建过hadoop集群,只需要将其中的配置文件做修改即可 1.在/usr下创建个hadoop文件夹,作为hadoop安装(压缩)包的存放路径和解压路径 123456#进入usr文件夹下cd /usr#创建hadoop文件夹mkdir hadoop#进入hadoop文件夹cd hadoop 利用Xftp工具将文件传输到虚拟机中解压后进入到 hadoop的解压路径/etc/hadoop文件夹下 1cd /usr/hadoop/hadoop-2.7.3/etc/hadoop/ 3.1.core-site.xml 1vim core-site.xml 在其中的configuration标签中添加以下内容 123456789101112131415161718192021222324252627282930313233343536&lt;!-- 指定hdfs的nameservice为ns --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://ns&lt;/value&gt; &lt;/property&gt; &lt;!--指定hadoop数据存放目录--&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/HA/hadoop/tmp&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;io.file.buffer.size&lt;/name&gt; &lt;value&gt;4096&lt;/value&gt; &lt;/property&gt; &lt;!--指定zookeeper地址--&gt; &lt;property&gt; &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt; &lt;value&gt;master:2181,slave1:2181,slave2:2181&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;ipc.client.connect.max.retries&lt;/name&gt; &lt;value&gt;100&lt;/value&gt; &lt;description&gt;Indicates the number of retries a client will make to establish a server connection. &lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;ipc.client.connect.retry.interval&lt;/name&gt; &lt;value&gt;10000&lt;/value&gt; &lt;description&gt;Indicates the number of milliseconds a client will wait for before retrying to establish a server connection. &lt;/description&gt; &lt;/property&gt; 3.2.hdfs-site.xml 1vim hdfs-site.xml 在其中的configuration标签中添加以下内容1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980&lt;!--指定hdfs的nameservice为ns，需要和core-site.xml中的保持一致 --&gt; &lt;property&gt; &lt;name&gt;dfs.nameservices&lt;/name&gt; &lt;value&gt;ns&lt;/value&gt; &lt;/property&gt; &lt;!-- ns下面有两个NameNode，分别是nn1，nn2 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.namenodes.ns&lt;/name&gt; &lt;value&gt;nn1,nn2&lt;/value&gt; &lt;/property&gt; &lt;!-- nn1的RPC通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.ns.nn1&lt;/name&gt; &lt;value&gt;master:9000&lt;/value&gt; &lt;/property&gt; &lt;!-- nn1的http通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.ns.nn1&lt;/name&gt; &lt;value&gt;master:50070&lt;/value&gt; &lt;/property&gt; &lt;!-- nn2的RPC通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.ns.nn2&lt;/name&gt; &lt;value&gt;slave1:9000&lt;/value&gt; &lt;/property&gt; &lt;!-- nn2的http通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.ns.nn2&lt;/name&gt; &lt;value&gt;slave1:50070&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定NameNode的元数据在JournalNode上的存放位置 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt; &lt;value&gt;qjournal://master:8485;slave1:8485;slave2:8485/ns&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定JournalNode在本地磁盘存放数据的位置 --&gt; &lt;property&gt; &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt; &lt;value&gt;/HA/hadoop/journal&lt;/value&gt; &lt;/property&gt; &lt;!-- 开启NameNode故障时自动切换 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置失败自动切换实现方式 --&gt; &lt;property&gt; &lt;name&gt;dfs.client.failover.proxy.provider.ns&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置隔离机制 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt; &lt;value&gt;sshfence&lt;/value&gt; &lt;/property&gt; &lt;!-- 使用隔离机制时需要ssh免登陆 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt; &lt;value&gt;/root/.ssh/id_rsa&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:///HA/hadoop/hdfs/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:///HA/hadoop/hdfs/data&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;/property&gt; &lt;!-- 在NN和DN上开启WebHDFS (REST API)功能,不是必须 --&gt; &lt;property&gt; &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; 3.3.mapred-site.xml这个文件刚开始是没有的,所以我们需要将其创建出来 12#利用模版文件copy出来一个cp mapred-site.xml.template mapred-site.xml 然后在其configuration标签中添加以下内容vim mapred-site.xml 12345&lt;!-- 指定mr运行在yarn上 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; 3.4.yarn-site.xml 1vim yarn-site.xml 在其configuration标签中添加以下内容普通版只有slave2有Resourcemanager12345678910&lt;!-- 指定nodemanager启动时加载server的方式为shuffle server --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定resourcemanager地址 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;slave2&lt;/value&gt; &lt;/property&gt; yarn HA高可用版 slave1和slave2都有Resourcemanager1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980&lt;!-- //////////////以下为YARN HA的配置////////////// --&gt; &lt;!-- 开启YARN HA --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 启用自动故障转移 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.automatic-failover.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定YARN HA的名称 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt; &lt;value&gt;yarncluster&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定两个resourcemanager的名称 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt; &lt;value&gt;rm1,rm2&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置rm1，rm2的主机 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt; &lt;value&gt;slave2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt; &lt;value&gt;slave1&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置YARN的http端口 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address.rm1&lt;/name&gt; &lt;value&gt;slave2:8088&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address.rm2&lt;/name&gt; &lt;value&gt;slave1:8088&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置zookeeper的地址 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt; &lt;value&gt;master:2181,slave1:2181,slave2:2181&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置zookeeper的存储位置 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.zk-state-store.parent-path&lt;/name&gt; &lt;value&gt;/rmstore&lt;/value&gt; &lt;/property&gt; &lt;!-- 开启yarn resourcemanager restart --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.recovery.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置resourcemanager的状态存储到zookeeper中 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.store.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore&lt;/value&gt; &lt;/property&gt; &lt;!-- 开启yarn nodemanager restart --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.recovery.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置nodemanager IPC的通信端口 --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.address&lt;/name&gt; &lt;value&gt;0.0.0.0:45454&lt;/value&gt; &lt;/property&gt; 3.5 .hadoop-env.sh 12export JAVA_HOME=$&#123;JAVA_HOME&#125; 一行，将其修改为 export JAVA_HOME=/usr/java/jdkxxx(jdk的安装路径) 3.6.修改slaves文件(dataNode)修改为 123456#localhost#你的集群主机名masterslave1slave2 4.复制内容到slave1,slave2如果你的slave1和slave2什么也没有,可以一并将配置jdk的profile文件和配置ip映射的hosts文件一起复制过去,Zookeeper则需要注意改下配置文件1234#复制给slave1,如果之前有hadoop也会覆盖[root@master hadoop]# scp -r /usr/hadoop root@slave1:/usr/#复制给slave2[root@master hadoop]# scp -r /usr/hadoop root@slave2:/usr/ 5.启动集群5.1分别启动Zookeeper所有虚拟机全部启动在Zookeeper安装目录的/bin目录下启动 12345[root@master hadoop]# cd /usr/zookeeper/zookeeper-3.4.12/bin[root@master bin]# ./zkServer.sh startZooKeeper JMX enabled by defaultUsing config: /usr/zookeeper/zookeeper-3.4.12/bin/../conf/zoo.cfgStarting zookeeper ... STARTED 启动后查看状态 1234[root@slave1 bin]# ./zkServer.sh statusZooKeeper JMX enabled by defaultUsing config: /usr/zookeeper/zookeeper-3.4.12/bin/../conf/zoo.cfgMode: leader #leader或者follower则代表启动Zookeeper成功 5.2在master,slave1,slave2上启动journalnode 123456789101112131415#进入到hadoop安装目录sbin文件夹下[root@master bin]# cd /usr/hadoop/hadoop-2.7.3/sbin/[root@master sbin]# lsdistribute-exclude.sh kms.sh start-balancer.sh stop-all.cmd stop-yarn.cmdhadoop-daemon.sh mr-jobhistory-daemon.sh start-dfs.cmd stop-all.sh stop-yarn.shhadoop-daemons.sh refresh-namenodes.sh start-dfs.sh stop-balancer.sh yarn-daemon.shhdfs-config.cmd slaves.sh start-secure-dns.sh stop-dfs.cmd yarn-daemons.shhdfs-config.sh start-all.cmd start-yarn.cmd stop-dfs.shhttpfs.sh start-all.sh start-yarn.sh stop-secure-dns.sh#这里有一个daemons和daemon,不带s是启动单个,带s是启动集群[root@master sbin]# ./hadoop-daemons.sh start journalnodeslave2: starting journalnode, logging to /usr/hadoop/hadoop-2.7.3/logs/hadoop-root-journalnode-slave2.outslave1: starting journalnode, logging to /usr/hadoop/hadoop-2.7.3/logs/hadoop-root-journalnode-slave1.outmaster: starting journalnode, logging to /usr/hadoop/hadoop-2.7.3/logs/hadoop-root-journalnode-master.out 分别在master,slave1,slave2上查看jps 12345#这样正常,否则查看你的Zookeeper是否启动成功[root@master sbin]# jps2232 JournalNode2281 Jps2157 QuorumPeerMain 5.3在master上格式化zkfc 1[root@master sbin]# hdfs zkfc -formatZK 5.4在master上格式化hdfs 1[root@master sbin]# hadoop namenode -format 5.5在master上启动namenode 1234567[root@master sbin]# ./hadoop-daemon.sh start namenodestarting namenode, logging to /usr/hadoop/hadoop-2.7.3/logs/hadoop-root-namenode-master.out[root@master sbin]# jps2232 JournalNode2490 Jps2157 QuorumPeerMain2431 NameNode 5.6在slave1上启动数据同步和standby的namenode 123[root@slave1 sbin]# hdfs namenode -bootstrapStandby[root@slave1 sbin]# ./hadoop-daemon.sh start namenode 5.7在master上启动datanode 1234[root@master sbin]# ./hadoop-daemons.sh start datanodemaster: starting datanode, logging to /usr/hadoop/hadoop-2.7.3/logs/hadoop-root-datanode-master.outslave2: starting datanode, logging to /usr/hadoop/hadoop-2.7.3/logs/hadoop-root-datanode-slave2.outslave1: starting datanode, logging to /usr/hadoop/hadoop-2.7.3/logs/hadoop-root-datanode-slave1.out 5.8在slave1和slave2上启动yarn 1./start-yarn.sh 5.9在master上启动zkfc 1./hadoop-daemons.sh start zkfc 6.查看jpsmaster 12345678[root@master sbin]# jps2593 DataNode2709 NodeManager2902 DFSZKFailoverController2232 JournalNode2969 Jps2157 QuorumPeerMain2431 NameNode slave1 12345678[root@slave1 sbin]# jps2337 QuorumPeerMain3074 Jps2259 JournalNode2709 ResourceManager2475 NameNode2587 DataNode3007 DFSZKFailoverController slave2 1234567[root@slave2 sbin]# jps2355 DataNode2164 JournalNode2244 QuorumPeerMain3126 NodeManager3017 ResourceManager3162 Jps 启动如上则正常如果有服务没有启动,重启该服务,例如Resourcemanager没启动 1234#停止./stop-yarn.sh#启动./start-yarn.sh 然后在50070和8088端口进行测试在测试之前为了防止namenode不能热切换,最好安装此插件在master和slave1上安装 1yum -y install psmisc 7.测试在(master的ip)192.168.134.154:50070和(slave1的ip)192.168.134.155:50070上查看namenode的状态 都能访问且一个是active一个是standby状态 然后访问(slave1)192.168.134.155:8088和(slave2)192.168.134.156:8088查看Resourcemanager状态 若是一个能访问,访问另一个时跳到前一个的时候并不是错误,那样是正常的能访问的那个是active状态,若是两个都能访问则一个是active一个是standby 首先在master主机上想hdfs上传一个文件,然后尝试能否在slave1和slave2上查看 1234567[root@master tmp]# cd /usr/tmp[root@master tmp]# touch test[root@master tmp]# hadoop fs -put test /#分别在三台虚拟机上查看[root@master tmp]# hadoop fs -ls /Found 1 items-rw-r--r-- 3 root supergroup 0 2018-10-22 20:42 /test 如果都能查看到,接下来再测试是否能够热切换 1234567891011#查看进程[root@master tmp]# jps2593 DataNode2902 DFSZKFailoverController2232 JournalNode3609 NodeManager2157 QuorumPeerMain2431 NameNode3807 Jps#杀死active的namenode[root@master tmp]# kill -9 2431 在网页查看standby的是否变为active从standby成功变更为active则表示成功同样,测试yarn HA高可用 12345678910#查看进程[root@slave2 sbin]# jps4050 Jps2355 DataNode2164 JournalNode2244 QuorumPeerMain3423 ResourceManager3919 NodeManager#杀死active的ResourceManager[root@slave2 sbin]# kill -9 3423 在网页查看 如果说你杀死了namenode进程,那么相应的50070端口自然无法访问了,同理8088端口一样 至此hadoop HA高可用版搭建完成. 接下来会说一下hive的安装和使用]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Zookeeper</tag>
        <tag>HA高可用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Combiners编程]]></title>
    <url>%2F2018%2F10%2F19%2Fblog181019-4%2F</url>
    <content type="text"><![CDATA[这个Combiners编程示范是基于 MapReduce对手机上网记录的简单分析和Partitioner分区]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Java</tag>
        <tag>Combiners</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zookeeper的安装]]></title>
    <url>%2F2018%2F10%2F19%2Fblog181019-3%2F</url>
    <content type="text"><![CDATA[概览1.Zookeeper简介2.Zookeeper的安装3.Zookeeper的配置4.启动集群5.数据同步测试 1.Zookeeper简介Zookeeper功能简介 ZooKeeper 是一个开源的分布式协调服务，由雅虎创建，是 Google Chubby 的开源实现分布式应用程序可以基于ZooKeeper 实现诸如数据发布/订阅、负载均衡、命名服务、分布式协调/通知、集群管理、Master选举、配置维护，名字服务、分布式同步、分布式锁和分布式队列等功能。 在Zookeeper中共有三个角色 1. Leader 2. Follower 3. Observe 一个 ZooKeeper 集群同一时刻只会有一个 Leader，其他都是 Follower 或 Observer。 2.Zookeeper的安装这次我们是在之前的Hadoop集群搭建的基础上安装的,所以没有搭建好的小伙伴最好先配置好Hadoop集群,不然还要安装jdk 在虚拟机的/usr下创建个zookeeper文件夹mkdir zookeeper然后将zookeeper的安装包(压缩包)上传到该文件夹下 解压 123456[root@master usr]# mkdir zookeeper[root@master usr]# lsbin etc games hadoop include java lib lib64 libexec local sbin share src tmp zookeeper[root@master usr]# cd zookeeper/#解压[root@master zookeeper]# tar -zxf zookeeper-3.4.12.tar.gz 3.Zookeeper的配置（先在一台节点上配置）添加一个zoo.cfg配置文件 1234#进入到Zookeeper的配置目录cd /usr/zookeeper/zookeeper-3.4.12/conf#copy出来一个配置文件cp zoo_sample.cfg zoo.cfg 这是zoo.cfg中各项的含义 zookeeper的默认配置文件为zookeeper/conf/zoo_sample.cfg，需要将其修改为zoo.cfg。其中各配置项的含义，解释如下： 1.tickTime：CS通信心跳时间 Zookeeper 服务器之间或客户端与服务器之间维持心跳的时间间隔，也就是每个 tickTime 时间就会发送一个心跳。tickTime以毫秒为单位。 tickTime=2000 2.initLimit：LF初始通信时限 集群中的follower服务器(F)与leader服务器(L)之间初始连接时能容忍的最多心跳数（tickTime的数量）。initLimit=5 3.syncLimit：LF同步通信时限 集群中的follower服务器与leader服务器之间请求和应答之间能容忍的最多心跳数（tickTime的数量）。 syncLimit=2 4.dataDir：数据文件目录 Zookeeper保存数据的目录，默认情况下，Zookeeper将写数据的日志文件也保存在这个目录里。 dataDir=/home/michael/opt/zookeeper/data 5.clientPort：客户端连接端口 客户端连接 Zookeeper 服务器的端口，Zookeeper 会监听这个端口，接受客户端的访问请求。 clientPort=2181 6.服务器名称与地址：集群信息（服务器编号，服务器地址，LF通信端口，选举端口） 这个配置项的书写格式比较特殊，规则如下： server.N=YYY:A:B 修改配置文件（zoo.cfg）将dataDir=/tmp/zookeeper 改为 dataDir=/usr/zookeeper/data然后在最后追加 123456#master,slave1,slave2这是我的三台虚拟机主机名#并且在/usr/hosts文件中做了ip映射#所以可以直接写主机名称,如果你没有做映射,将主机名替换为ip地址server.1=master:2888:3888server.2=slave1:2888:3888server.3=slave2:2888:3888 然后我们在/usr/zookeeper文件夹下创建一个data文件夹 1234[root@master conf]# cd /usr/zookeeper/[root@master zookeeper]# lszookeeper-3.4.12 zookeeper-3.4.12.tar.gz[root@master zookeeper]# mkdir data 然后在data下创建一个myid文件(ZooKeeper 配置很简单，每个节点的配置文件(zoo.cfg)都是一样的，只有 myid 文件不一样。myid 的值必须是 zoo.cfg中server.{数值} 的{数值}部分。)在（/usr/zookeeper/data 需要自己创建）创建一个myid文件，里面内容是server.N中的N（server.2里面内容为2） 1234567#建立文件[root@master data]# touch myid#传入值[root@master data]# echo 1 &gt; myid #查看[root@master data]# cat myid 1 然后我们把配置好的zookeeper传给另外两台主机slave1和slave2 12scp -r /usr/zookeeper root@slave1:/usrscp -r /usr/zookeeper root@slave2:/usr 注意：在其他节点上一定要修改myid的内容 在slave1应该讲myid的内容改为2（echo 2 &gt; myid） 在slave2应该讲myid的内容改为3 （echo 3 &gt; myid） 4.启动集群(注意关闭防火墙)在master , slave1 , slave2分别启动zookeeper 123456#进入zookeeper下的bin文件夹[root@master usr]# cd /usr/zookeeper/zookeeper-3.4.12/bin[root@master bin]# lsREADME.txt zkCleanup.sh zkCli.cmd zkCli.sh zkEnv.cmd zkEnv.sh zkServer.cmd zkServer.sh#启动服务[root@master bin]# ./zkServer.sh start 查看状态12345678[root@master bin]# ./zkServer.sh startZooKeeper JMX enabled by defaultUsing config: /usr/zookeeper/zookeeper-3.4.12/bin/../conf/zoo.cfgStarting zookeeper ... STARTED[root@master bin]# ./zkServer.sh statusZooKeeper JMX enabled by defaultUsing config: /usr/zookeeper/zookeeper-3.4.12/bin/../conf/zoo.cfgMode: follower #这里是follower便可以认为启动成功 5.数据同步测试进入到zkCilent 1./zkCli.sh 在主机master上创建一个文件,看看是否同步到其他机器上 1234567891011121314151617181920#查看[zk: localhost:2181(CONNECTED) 0] ls /[zookeeper]#创建文件 /路径 内容[zk: localhost:2181(CONNECTED) 1] create /test helloCreated /test#查看文件内容[zk: localhost:2181(CONNECTED) 2] get /testhellocZxid = 0x100000002ctime = Fri Oct 19 21:51:21 CST 2018mZxid = 0x100000002mtime = Fri Oct 19 21:51:21 CST 2018pZxid = 0x100000002cversion = 0dataVersion = 0aclVersion = 0ephemeralOwner = 0x0dataLength = 5numChildren = 0 在其他机器上也进行查看 123456789101112131415161718#进入到zkCli[root@slave1 bin]# ./zkCli.sh#查看[zk: localhost:2181(CONNECTED) 0] ls /[zookeeper, test][zk: localhost:2181(CONNECTED) 1] get /testhellocZxid = 0x100000002ctime = Fri Oct 19 21:51:21 CST 2018mZxid = 0x100000002mtime = Fri Oct 19 21:51:21 CST 2018pZxid = 0x100000002cversion = 0dataVersion = 0aclVersion = 0ephemeralOwner = 0x0dataLength = 5numChildren = 0 再到slave2上查看,数据是同步的 删除 123[zk: localhost:2181(CONNECTED) 3] delete /test[zk: localhost:2181(CONNECTED) 4] ls /[zookeeper] 再到其他机器进行查看,发现也删除了,表示你的zookeeper的集群安装已完成 退出zkCli 1[zk: localhost:2181(CONNECTED) 5] quit 结束zkServer12#结束服务[root@master bin]# ./zkServer.sh stop 接下来还会介绍hadoop+zookeeper3节点高可用集群搭建]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[将MapReduce分析手机上网记录的结果进行排序操作]]></title>
    <url>%2F2018%2F10%2F19%2Fblog181019-2%2F</url>
    <content type="text"><![CDATA[上次我们说过了MapReduce对手机上网记录的简单分析和Partitioner分区这次我们介绍一下如何将手机上网记录根据总流量的多少进行排序 1.编写Java代码,并将其打包成jar包在eclipse上创建个新的java项目,创建lib文件夹,将上次的jar同样导入进来 然后创建个TelBean类这里实现了WritableComparable接口,就是序列化的比较,详情查询api文档 public interface Comparator比较功能，对一些对象的集合施加了一个整体排序 。 可以将比较器传递给排序方法（如Collections.sort或Arrays.sort ），以便对排序顺序进行精确控制。 比较器还可以用来控制某些数据结构（如顺序sorted sets或sorted maps ），或对于不具有对象的集合提供的排序natural ordering 。通过比较c上的一组元素S的确定的顺序对被认为是与equals一致当且仅当c.compare(e1, e2)==0具有用于S每e1和e2相同布尔值e1.equals(e2)。 当使用能够强制排序不一致的比较器时，应注意使用排序集（或排序图）。 假设具有显式比较器c的排序集（或排序映射）与从集合S中绘制的元素（或键）一起使用 。 如果88446235254451上的c强制的排序与equals不一致，则排序集（或排序映射）将表现为“奇怪”。特别是排序集（或排序图）将违反用于设置（或映射）的一般合同，其按equals定义。 例如，假设一个将两个元件a和b ，使得(a.equals(b) &amp;&amp; c.compare(a, b) != 0)到空TreeSet与比较c。 因为a和b与树集的角度不相等，所以第二个add操作将返回true（并且树集的大小将增加），即使这与Set.add方法的规范相反。 注意：这通常是一个好主意比较，也能实现java.io.Serializable，因为它们可能被用来作为排序的序列化数据结构的方法（如TreeSet， TreeMap ）。 为了使数据结构成功序列化，比较器（如果提供）必须实现Serializable 。 对于数学上的倾斜，即限定了施加顺序 ，给定的比较器c上一组给定对象的S强加关系式为： {(x, y) such that c.compare(x, y) &lt;= 0}. 这个总订单的商是： {(x, y) suchthat c.compare(x, y) == 0}. 它从合同compare，该商数是S的等价关系紧随其后，而强加的排序是S， 总订单 。当我们说S上的c所规定的顺序与等于一致时，我们的意思是排序的商是由对象’ equals(Object)方法定义的等价关系： {(x,y) such that x.equals(y)}. 与Comparable不同，比较器可以可选地允许比较空参数，同时保持对等价关系的要求。 此接口是成员Java Collections Framework 。 从以下版本开始：1.2 另请参见： Comparable ， Serializable 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495package com.zy.hadoop.entity;import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;import org.apache.hadoop.io.Writable;import org.apache.hadoop.io.WritableComparable;public class TelBean implements WritableComparable&lt;TelBean&gt;&#123; private String tel; private long upPayLoad; private long downPayLoad; private long totalPayLoad; public String getTel() &#123; return tel; &#125; public void setTel(String tel) &#123; this.tel = tel; &#125; public long getUpPayLoad() &#123; return upPayLoad; &#125; public void setUpPayLoad(long upPayLoad) &#123; this.upPayLoad = upPayLoad; &#125; public long getDownPayLoad() &#123; return downPayLoad; &#125; public void setDownPayLoad(long downPayLoad) &#123; this.downPayLoad = downPayLoad; &#125; public long getTotalPayLoad() &#123; return totalPayLoad; &#125; public void setTotalPayLoad(long totalPayLoad) &#123; this.totalPayLoad = totalPayLoad; &#125; public TelBean(String tel, long upPayLoad, long downPayLoad, long totalPayLoad) &#123; super(); this.tel = tel; this.upPayLoad = upPayLoad; this.downPayLoad = downPayLoad; this.totalPayLoad = totalPayLoad; &#125; public TelBean() &#123; super(); // TODO Auto-generated constructor stub &#125; @Override public String toString() &#123; return tel + &quot;\t&quot; + upPayLoad + &quot;\t&quot; + downPayLoad + &quot;\t&quot; + totalPayLoad ; &#125; //反序列化的过程 @Override public void readFields(DataInput in) throws IOException &#123; this.tel = in.readUTF(); this.upPayLoad = in.readLong(); this.downPayLoad = in.readLong(); this.totalPayLoad = in.readLong(); &#125; //序列化的过程 @Override public void write(DataOutput out) throws IOException &#123; // TODO Auto-generated method stub out.writeUTF(this.tel); out.writeLong(this.upPayLoad); out.writeLong(this.downPayLoad); out.writeLong(this.totalPayLoad); &#125; //compare比较,详情查阅java的api文档 @Override public int compareTo(TelBean bean) &#123; // TODO Auto-generated method stub return (int)(this.totalPayLoad-bean.getTotalPayLoad()); &#125;&#125; 然后在mr包下依次建立SortMapper,SortReducer,SortCount 关于分析可以查看MapReduce对手机上网记录的简单分析和Partitioner分区SortMapper 12345678910111213141516171819202122232425262728293031323334353637package com.zy.hadoop.mr2;import java.io.IOException;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;import com.zy.hadoop.entity.TelBean;public class SortMapper extends Mapper&lt;LongWritable, Text, TelBean, NullWritable&gt;&#123; @Override protected void map(LongWritable key, Text value, Mapper&lt;LongWritable, Text, TelBean, NullWritable&gt;.Context context) throws IOException, InterruptedException &#123; //value ,第一mr出来的结果中的每一行 String line = value.toString(); //拆分字符串&quot;\t&quot; String[] strs = line.split(&quot;\t&quot;); //直接通过下标取值 //电话号码 String tel = strs[0]; //上行流量 long upPayLoad=Long.parseLong(strs[2]); //下行流量 long downPayLoad=Long.parseLong(strs[3]); //总流量 long totalPayLoad=Long.parseLong(strs[4]); //把去除的值封装到对象中 TelBean telBean = new TelBean(tel, upPayLoad, downPayLoad, totalPayLoad); //输出k2,v2 context.write(telBean, NullWritable.get()); &#125;&#125; SortReducer 1234567891011121314151617181920package com.zy.hadoop.mr2;import java.io.IOException;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.mapreduce.Reducer;import com.zy.hadoop.entity.TelBean;public class SortReducer extends Reducer&lt;TelBean, NullWritable, TelBean, NullWritable&gt;&#123; @Override protected void reduce(TelBean arg0, Iterable&lt;NullWritable&gt; arg1, Reducer&lt;TelBean, NullWritable, TelBean, NullWritable&gt;.Context arg2) throws IOException, InterruptedException &#123; arg2.write(arg0, NullWritable.get()); &#125; &#125; SortCount 123456789101112131415161718192021222324252627282930313233343536373839404142package com.zy.hadoop.mr2;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import com.zy.hadoop.entity.TelBean;public class SortCount &#123; public static void main(String[] args) throws Exception &#123; // 1.获取job Configuration conf = new Configuration(); Job job = Job.getInstance(conf); // 2.指定job使用的类 job.setJarByClass(SortCount.class); // 3.设置Mapper的属性 job.setMapperClass(SortMapper.class); job.setMapOutputKeyClass(TelBean.class); job.setMapOutputValueClass(NullWritable.class); // 4.设置输入文件 FileInputFormat.setInputPaths(job, new Path(args[0])); // 5.设置reducer的属性 job.setReducerClass(SortReducer.class); job.setOutputKeyClass(TelBean.class); job.setMapOutputValueClass(NullWritable.class); // 6.设置输出文件夹,查看结果保存到hdfs文件夹中的位置 FileOutputFormat.setOutputPath(job, new Path(args[1])); // 7.提交 true 提交的时候打印日志信息 job.waitForCompletion(true); &#125;&#125; 接下来将项目打包成jar包,上传到虚拟机/usr/tmp下 2.虚拟机上运行jar包,查看结果启动hadoop集群服务 1start-all.sh 查看是否成功 我们将之前处理过一次的文件/tel1/part-r-00000(/tel2下的进行过分区了,所以不进行处理)作为源文件进行分析排序 1hadoop jar tel_3.jar /tel/part-r-00000 /tel3 等待执行完毕查看结果 结果如下123456789101112131415161718192021222324252627282930[root@master tmp]# hadoop fs -ls /hadoopFound 5 items-rw-r--r-- 1 root supergroup 2315 2018-10-19 19:33 /tel.logdrwxr-xr-x - root supergroup 0 2018-10-19 19:59 /tel1drwxr-xr-x - root supergroup 0 2018-10-19 20:10 /tel2drwxr-xr-x - root supergroup 0 2018-10-19 20:47 /tel3drwx------ - root supergroup 0 2018-10-19 19:58 /tmp[root@master tmp]# hadoop fs -ls /tel3Found 2 items-rw-r--r-- 1 root supergroup 0 2018-10-19 20:47 /tel3/_SUCCESS-rw-r--r-- 1 root supergroup 477 2018-10-19 20:47 /tel3/part-r-00000[root@master tmp]# hadoop fs -cat /tel3/part-r-0000013926251106 240 0 24013826544101 264 0 26413480253104 180 180 36013926435656 132 1512 164415989002119 1938 180 211818211575961 1527 2106 363313560436666 2232 1908 414013602846565 1938 2910 484884138413 4116 1432 554815920133257 3156 2936 609213922314466 3008 3720 672815013685858 3659 3538 719713660577991 6960 690 765013560439658 2034 5892 792618320173382 9531 2412 1194313726238888 2481 24681 2716213925057413 11058 48243 5930113502468823 7335 110349 117684 这就是MapReduce进行简单的数据分析 不过hadoop集群的namenode如果只有一个,namenode机器宕机整个集群都会不可用 , 接下来会介绍zookeeper的高可用hadoop集群如何搭建]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Java</tag>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapReduce对手机上网记录的简单分析和Partitioner分区]]></title>
    <url>%2F2018%2F10%2F19%2Fblog181019-1%2F</url>
    <content type="text"><![CDATA[概览1.MapReduce处理手机上网记录2.Partitioner分区 上次说过了关于MapReduce的执行流程和原理,下面来说下分区和简单示例 1.MapReduce处理手机上网记录首先我们需要先模拟一个通话记录文件 在Windows的桌面建个tel.log的文件,里面模拟一些通话记录信息 12345678910111213141516171819202122231363157985066 13726230503 00-FD-07-A4-72-B8:CMCC 120.196.100.82 i02.c.aliimg.com 24 27 2481 24681 2001363157995052 13826544101 5C-0E-8B-C7-F1-E0:CMCC 120.197.40.4 4 0 264 0 2001363157991076 13926435656 20-10-7A-28-CC-0A:CMCC 120.196.100.99 2 4 132 1512 2001363154400022 13926251106 5C-0E-8B-8B-B1-50:CMCC 120.197.40.4 4 0 240 0 2001363157993044 18211575961 94-71-AC-CD-E6-18:CMCC-EASY 120.196.100.99 iface.qiyi.com 视频网站 15 12 1527 2106 2001363157995074 84138413 5C-0E-8B-8C-E8-20:7DaysInn 120.197.40.4 122.72.52.12 20 16 4116 1432 2001363157993055 13560439658 C4-17-FE-BA-DE-D9:CMCC 120.196.100.99 18 15 1116 954 2001363157995033 15920133257 5C-0E-8B-C7-BA-20:CMCC 120.197.40.4 sug.so.360.cn 信息安全 20 20 3156 2936 2001363157983019 13719199419 68-A1-B7-03-07-B1:CMCC-EASY 120.196.100.82 4 0 240 0 2001363157984041 13660577991 5C-0E-8B-92-5C-20:CMCC-EASY 120.197.40.4 s19.cnzz.com 站点统计 24 9 6960 690 2001363157973098 15013685858 5C-0E-8B-C7-F7-90:CMCC 120.197.40.4 rank.ie.sogou.com 搜索引擎 28 27 3659 3538 2001363157986029 15989002119 E8-99-C4-4E-93-E0:CMCC-EASY 120.196.100.99 www.umeng.com 站点统计 3 3 1938 180 2001363157992093 13560439658 C4-17-FE-BA-DE-D9:CMCC 120.196.100.99 15 9 918 4938 2001363157986041 13480253104 5C-0E-8B-C7-FC-80:CMCC-EASY 120.197.40.4 3 3 180 180 2001363157984040 13602846565 5C-0E-8B-8B-B6-00:CMCC 120.197.40.4 2052.flash2-http.qq.com 综合门户 15 12 1938 2910 2001363157995093 13922314466 00-FD-07-A2-EC-BA:CMCC 120.196.100.82 img.qfc.cn 12 12 3008 3720 2001363157982040 13502468823 5C-0A-5B-6A-0B-D4:CMCC-EASY 120.196.100.99 y0.ifengimg.com 综合门户 57 102 7335 110349 2001363157986072 18320173382 84-25-DB-4F-10-1A:CMCC-EASY 120.196.100.99 input.shouji.sogou.com 搜索引擎 21 18 9531 2412 2001363157990043 13925057413 00-1F-64-E1-E6-9A:CMCC 120.196.100.55 t3.baidu.com 搜索引擎 69 63 11058 48243 2001363157988072 13760778710 00-FD-07-A4-7B-08:CMCC 120.196.100.82 2 2 120 120 2001363157985066 13726238888 00-FD-07-A4-72-B8:CMCC 120.196.100.82 i02.c.aliimg.com 24 27 2481 24681 2001363157993055 13560436666 C4-17-FE-BA-DE-D9:CMCC 120.196.100.99 18 15 1116 954 2001363157993055 13560436666 C4-17-FE-BA-DE-D9:CMCC 120.196.100.99 18 15 1116 954 200 这些字段代表的是 首先我们需要将部分字段提取出来,以便之后进行分析 在主机master上启动hadoop集群,hadoop集群版的搭建可以参照简单的hadoop集群搭建 1start-all.sh 验证是否启动成功 然后将tel.log文件利用Xftp传输到虚拟机中的/usr/tmp下cd /usr/tmp/ 然后上传到hdfs 123456#上传[root@master tmp]# hadoop fs -put tel.log /#查看[root@master tmp]# hadoop fs -ls /Found 1 items-rw-r--r-- 1 root supergroup 2315 2018-10-19 19:33 /tel.log 然后在eclipse上新建java项目,并在项目下建个lib文件夹,然后将jar包放到lib中导入项目然后创建包,创建一个telBean实体类,这次我们分析的是手机号和其对应的上行流量,下行流量和总流量所以将其封装成实体类 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889package com.hd.entity;import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;import org.apache.hadoop.io.Writable;public class TelBean implements Writable&#123; private String tel; private long upPayLoad; private long downPayLoad; private long totalPayLoad; public String getTel() &#123; return tel; &#125; public void setTel(String tel) &#123; this.tel = tel; &#125; public long getUpPayLoad() &#123; return upPayLoad; &#125; public void setUpPayLoad(long upPayLoad) &#123; this.upPayLoad = upPayLoad; &#125; public long getDownPayLoad() &#123; return downPayLoad; &#125; public void setDownPayLoad(long downPayLoad) &#123; this.downPayLoad = downPayLoad; &#125; public long getTotalPayLoad() &#123; return totalPayLoad; &#125; public void setTotalPayLoad(long totalPayLoad) &#123; this.totalPayLoad = totalPayLoad; &#125; public TelBean(String tel, long upPayLoad, long downPayLoad, long totalPayLoad) &#123; super(); this.tel = tel; this.upPayLoad = upPayLoad; this.downPayLoad = downPayLoad; this.totalPayLoad = totalPayLoad; &#125; public TelBean() &#123; super(); // TODO Auto-generated constructor stub &#125; @Override public String toString() &#123; return tel + &quot;\t&quot; + upPayLoad + &quot;\t&quot; + downPayLoad + &quot;\t&quot; + totalPayLoad ; &#125; //反序列化的过程 @Override public void readFields(DataInput in) throws IOException &#123; this.tel = in.readUTF(); this.upPayLoad = in.readLong(); this.downPayLoad = in.readLong(); this.totalPayLoad = in.readLong(); &#125; //序列化的过程 @Override public void write(DataOutput out) throws IOException &#123; // TODO Auto-generated method stub out.writeUTF(this.tel); out.writeLong(this.upPayLoad); out.writeLong(this.downPayLoad); out.writeLong(this.totalPayLoad); &#125;&#125; 在mr包下创建个TelMapper类继承Mapper首先分析一下,我们要传入的第一个需要Map处理的&lt;k1,v1&gt;是long类型(电话号码)和String(Text)类型(与之对应的一行记录),而从Map处理过的&lt;k2,v2&gt;是String类型(电话号码)和TelBean对象(将我们需要的字段封装成对象) 1234567891011121314151617181920212223242526272829303132package com.hd.hadoop.mr;import java.io.IOException;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;import com.zy.hadoop.entity.TelBean;//k1,v1 long string k2,v2 string TelBeanpublic class TelMapper extends Mapper&lt;LongWritable, Text, Text, TelBean&gt; &#123; @Override protected void map(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, TelBean&gt;.Context context) throws IOException, InterruptedException &#123; //value 对应 tel.log中的每一行数据,行中的数据以\t隔开的 String line = value.toString(); //对正航读取的数据进行拆分 String[] res = line.split(&quot;\t&quot;);//0---res.length-1 //取数组中的电话号码 String tel = res[1]; //取上行流量 long upPayLoad = Long.parseLong(res[8]); //取下行流量 long downPayLoad = Long.parseLong(res[9]); //创建telBean对象 TelBean telBean = new TelBean(tel, upPayLoad, downPayLoad, 0); context.write(new Text(tel), telBean); &#125;&#125; 然后创建个TelReducer类继承Reducer分析一下,这里传入的&lt;k2,v2&gt;是String(Text)类型和TelBean类型,而我们处理过输出的&lt;k3,v3&gt;也是相同类型,这里要记得将TelBean的toString方法重写,不然输出的是对象地址 12345678910111213141516171819202122232425262728293031package com.hd.hadoop.mr;import java.io.IOException;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Reducer;import com.zy.hadoop.entity.TelBean;public class TelReducer extends Reducer&lt;Text, TelBean, Text, TelBean&gt; &#123; @Override protected void reduce(Text key, Iterable&lt;TelBean&gt; value, Reducer&lt;Text, TelBean, Text, TelBean&gt;.Context context) throws IOException, InterruptedException &#123; //声明一个上行流量的变量 long upPayLoad = 0; //声明一个下行流量 long downPayLoad = 0; for (TelBean telBean : value) &#123; //统计相同电话的上行流量的和,下行流量的和 upPayLoad += telBean.getUpPayLoad(); downPayLoad += telBean.getDownPayLoad(); &#125; //k3 v3 TelBean telBean= new TelBean(key.toString(), upPayLoad, downPayLoad, upPayLoad+downPayLoad); context.write(key, telBean); &#125;&#125; 最后我们创建个主方法TelCount类 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748package com.zy.hadoop.mr1;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import com.zy.hadoop.entity.TelBean;public class TelCount &#123; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; // 1.获取job Configuration conf = new Configuration(); Job job = Job.getInstance(conf); // 2.指定job使用的类 job.setJarByClass(TelCount.class); // 3.设置Mapper的属性 job.setMapperClass(TelMapper.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(TelBean.class); // 4.设置输入文件 args[0]手动输入输入文件的位置 FileInputFormat.setInputPaths(job, new Path(args[0])); // 5.设置reducer的属性 job.setReducerClass(TelReducer.class); job.setOutputKeyClass(Text.class); job.setMapOutputValueClass(TelBean.class); // 6.设置输出文件夹,查看结果保存到hdfs文件夹中的位置 //args[1]手动输入输出文件的位置 FileOutputFormat.setOutputPath(job, new Path(args[1])); // 7.提交 true 提交的时候打印日志信息 job.waitForCompletion(true); &#125;&#125; 最后将其打成Jar包,主方法选择TelCount,然后上传到虚拟机/usr/tmp下 然后执行jar包 1hadoop jar tel_1.jar /tel.log /tel1 等待执行成功后查看结果文件 1234567891011121314151617181920212223#结果如下[root@master tmp]# hadoop fs -cat /tel1/part-r-0000013480253104 13480253104 180 180 36013502468823 13502468823 7335 110349 11768413560436666 13560436666 2232 1908 414013560439658 13560439658 2034 5892 792613602846565 13602846565 1938 2910 484813660577991 13660577991 6960 690 765013719199419 13719199419 240 0 24013726230503 13726230503 2481 24681 2716213726238888 13726238888 2481 24681 2716213760778710 13760778710 120 120 24013826544101 13826544101 264 0 26413922314466 13922314466 3008 3720 672813925057413 13925057413 11058 48243 5930113926251106 13926251106 240 0 24013926435656 13926435656 132 1512 164415013685858 15013685858 3659 3538 719715920133257 15920133257 3156 2936 609215989002119 15989002119 1938 180 211818211575961 18211575961 1527 2106 363318320173382 18320173382 9531 2412 1194384138413 84138413 4116 1432 5548 这说明执行成功了 2.Partitioner分区什么是Partitioner? 在进行MapReduce计算时，有时候需要把最终的输出数据分到不同的文件中，比如按照省份划分的话，需要把同一省份的数据放到一个文件中；按照性别划分的话，需要把同一性别的数据放到一个文件中。我们知道最终的输出数据是来自于Reducer任务。那么，如果要得到多个文件，意味着有同样数量的Reducer任务在运行。Reducer任务的数据来自于Mapper任务，也就说Mapper任务要划分数据，对于不同的数据分配给不同的Reducer任务运行。Mapper任务划分数据的过程就称作Partition。负责实现划分数据的类称作Partitioner。 我们还是处理手机的上网记录在之前的mr包中见一个TCPartitioner类我们将135和136开头的号码视为移动用户,处理结果放到一起(part-r-00000),另外的号码处理结果放到一起(part-r-00001) 1234567891011121314151617181920212223242526package com.hd.hadoop.mr;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Partitioner;import com.zy.hadoop.entity.TelBean;public class TCPartitioner extends Partitioner&lt;Text, TelBean&gt; &#123; @Override public int getPartition(Text text, TelBean telBean, int arg2) &#123; // TODO Auto-generated method stub String tel = text.toString(); String sub_tel = tel.substring(0, 3);//取手机号前三位进行分区 //假设135 136的为移动的 放一个分区,其他的放一个分区 if(sub_tel.equals(&quot;135&quot;)||sub_tel.equals(&quot;136&quot;))&#123; //return的数对应着计算结果文件 part-r-00001 return 1; &#125; return 0; &#125;&#125; 然后在TelCount添加几行代码 再将项目打成jar包放入到虚拟机/usr/tmp下,然后执行 1hadoop jar tel_2.jar /tel.log /tel2 等待执行完毕后查看结果 12345678910111213#查看生成几个结果文件[root@master tmp]# hadoop fs -ls /tel2Found 3 items-rw-r--r-- 1 root supergroup 0 2018-10-19 20:10 /tel2/_SUCCESS-rw-r--r-- 1 root supergroup 603 2018-10-19 20:10 /tel2/part-r-00000-rw-r--r-- 1 root supergroup 198 2018-10-19 20:10 /tel2/part-r-00001#查看135和136开头的手机号的结果[root@master tmp]# hadoop fs -cat /tel2/part-r-0000113502468823 13502468823 7335 110349 11768413560436666 13560436666 2232 1908 414013560439658 13560439658 2034 5892 792613602846565 13602846565 1938 2910 484813660577991 13660577991 6960 690 7650 这就是简单的分区操作 接下来还有如何将分析的结果进行排序操作]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Java</tag>
        <tag>MapReduce</tag>
        <tag>Partitioner</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapReduce的执行流程和原理]]></title>
    <url>%2F2018%2F10%2F18%2Fblog181018-2%2F</url>
    <content type="text"><![CDATA[概览1.MapReduce简介2.MapReduce的执行流程3.MapReduce的原理4.测试MapReduce5.Java代码实现 1.MapReduce简介MapReduce是一种分布式计算模型，是Google提出的，主要用于搜索领域，解决海量数据的计算问题。 MR有两个阶段组成：Map和Reduce，用户只需实现map()和reduce()两个函数，即可实现分布式计算。 MapReduce是一种并行可扩展计算模型，并且有较好的容错性，主要解决海量离线数据的批处理。实现下面目标★ 易于编程★ 良好的扩展性★ 高容错性 MapReduce有哪些角色？各自的作用是什么？MapReduce由JobTracker和TaskTracker组成。JobTracker负责资源管理和作业控制，TaskTracker负责任务的运行。 2.MapReduce的执行流程MapReduce程序执行流程程序执行流程图如下： (1) 开发人员编写好MapReduce program，将程序打包运行。(2) JobClient向JobTracker申请可用Job，JobTracker返回JobClient一个可用Job ID。(3) JobClient得到Job ID后，将运行Job所需要的资源拷贝到共享文件系统HDFS中。(4) 资源准备完备后，JobClient向JobTracker提交Job。(5) JobTracker收到提交的Job后，初始化Job。(6) 初始化完成后，JobTracker从HDFS中获取输入splits(作业可以该启动多少Mapper任务)。(7) 与此同时，TaskTracker不断地向JobTracker汇报心跳信息，并且返回要执行的任务。(8) TaskTracker得到JobTracker分配(尽量满足数据本地化)的任务后，向HDFS获取Job资源(若数据是本地的，不需拷贝数据)。(9) 获取资源后，TaskTracker会开启JVM子进程运行任务。注：(3)中资源具体指什么？主要包含： ● 程序jar包、作业配置文件xml ● 输入划分信息，决定作业该启动多少个map任务 ● 本地文件，包含依赖的第三方jar包(-libjars)、依赖的归档文件(-archives)和普通文件(-files)，如果已经上传，则不需上传 3.MapReduce原理 MapReduce的执行步骤： 1、Map任务处理1.1 读取HDFS中的文件。每一行解析成一个&lt;k,v&gt;。每一个键值对调用一次map函数。 &lt;0,hello you&gt; &lt;10,hello me&gt;1.2 覆盖map()，接收1.1产生的&lt;k,v&gt;，进行处理，转换为新的&lt;k,v&gt;输出。 &lt;hello,1&gt; &lt;you,1&gt; &lt;hello,1&gt; &lt;me,1&gt; 1.3 对1.2输出的&lt;k,v&gt;进行分区。默认分为一个区。详见《Partitioner》1.4 对不同分区中的数据进行排序（按照k）、分组。分组指的是相同key的value放到一个集合中。 排序后： &lt;hello,1&gt; &lt;hello,1&gt; &lt;me,1&gt; &lt;you,1&gt;分组后：&lt;hello,{1,1}&gt;&lt;me,{1}&gt;&lt;you,{1}&gt; 1.5 （可选）对分组后的数据进行归约。详见《Combiner》2、Reduce任务处理 2.1 多个map任务的输出，按照不同的分区，通过网络copy到不同的reduce节点上。（shuffle）详见《shuffle过程分析》2.2 对多个map的输出进行合并、排序。覆盖reduce函数，接收的是分组后的数据，实现自己的业务逻辑， &lt;hello,2&gt; &lt;me,1&gt; &lt;you,1&gt; 处理后，产生新的&lt;k,v&gt;输出。 2.3 对reduce输出的&lt;k,v&gt;写到HDFS中。 4.测试MapReduce启动虚拟机利用Xshell工具连接 启动hadoop 1start-all.sh 上传到hdfs上一个文件test1文件test1的内容如下 123456hello worldhello lileihello haimeimeihello hadoophello girlhello girl 进入/usr/hadoop/hadoop-2.7.3/share/hadoop/mapreduce下 1cd /usr/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/ 有个 hadoop-mapreduce-examples-2.7.3.jar的jar包为test1执行这个jar包 1hadoop jar hadoop-mapreduce-examples-2.7.3.jar wordcount /test1 /result 等待执行,然后查看result文件内容 12345678910111213#查看result文件夹[root@master mapreduce]# hadoop fs -ls /resultFound 2 items-rw-r--r-- 1 root supergroup 0 2018-10-18 21:40 /result/_SUCCESS-rw-r--r-- 1 root supergroup 52 2018-10-18 21:40 /result/part-r-00000#查看part-r-00000内容[root@master mapreduce]# hadoop fs -cat /result/part-r-00000girl 2hadoop 1haimeimei 1hello 6lilei 1world 1 这个就是按照你所执行的文件,一次读取一行的内容,然后每行用空格分隔如图 5.Java代码实现首先eclipse建个Java项目,然后项目下建个lib文件夹放置jar包将复制到lib下,然后add to Build Path建立个mr包,在其中建立WCMapper,WCReducer,WordCount WCMapper 12345678910111213141516171819202122232425262728package com.hd.hadoop.mr;import java.io.IOException;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;//Mapper&lt;k1,v1,k2,v2&gt;//&lt;k1 long,v1 String&gt; &lt;k2 String,v2 long&gt;public class WCMapper extends Mapper&lt;LongWritable, Text, Text, LongWritable&gt; &#123; //重新一个map方法 @Override protected void map(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, LongWritable&gt;.Context context) throws IOException, InterruptedException &#123; // value代表文件中的每一行的数据 String line = value.toString(); //根据空格拆分字符串 String[] results = line.split(&quot; &quot;); //遍历数组得到每一个结果 for (String str : results) &#123; context.write(new Text(str), new LongWritable(1)); &#125; &#125;&#125; WCReducer 123456789101112131415161718192021222324252627package com.hd.hadoop.mr;import java.io.IOException;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Reducer;//k2 ,v2 k3 , v3public class WCReducer extends Reducer&lt;Text, LongWritable, Text, LongWritable&gt; &#123; //重写一个reduce方法 @Override protected void reduce(Text key2, Iterable&lt;LongWritable&gt; v2, Reducer&lt;Text, LongWritable, Text, LongWritable&gt;.Context context) throws IOException, InterruptedException &#123; //写自己的逻辑,统计单词个数 //定义一个遍历存放累加数据 long count=0; for (LongWritable lw : v2) &#123; count += lw.get(); &#125; //输出k3,v3 --&gt; String,Long context.write(key2, new LongWritable(count)); &#125; &#125; WordCount 1234567891011121314151617181920212223242526272829303132333435363738394041package com.hd.hadoop.mr;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class WordCount &#123; public static void main(String[] args) throws Exception &#123; // 1.获取job Configuration conf = new Configuration(); Job job = Job.getInstance(conf); // 2.指定job使用的类 job.setJarByClass(WordCount.class); // 3.设置Mapper的属性 job.setMapperClass(WCMapper.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(LongWritable.class); // 4.设置输入文件 FileInputFormat.setInputPaths(job, new Path(&quot;/test1&quot;)); // 5.设置reducer的属性 job.setReducerClass(WCReducer.class); job.setOutputKeyClass(Text.class); job.setMapOutputValueClass(LongWritable.class); // 6.设置输出文件夹,查看结果保存到hdfs文件夹中的位置 FileOutputFormat.setOutputPath(job, new Path(&quot;/result1&quot;)); // 7.提交 true 提交的时候打印日志信息 job.waitForCompletion(true); &#125;&#125; 然后打成jar包项目右键export 然后将打好的jar包利用Xftp工具放入到虚拟机master的/usr/tmp文件夹下cd /usr/tmp 然后执行1hadoop jar test1.jar 等待执行完毕 查看 123456789101112[root@master tmp]# hadoop fs -ls /result1Found 2 items-rw-r--r-- 1 root supergroup 0 2018-10-18 22:01 /result1/_SUCCESS-rw-r--r-- 1 root supergroup 55 2018-10-18 22:01 /result1/part-r-00000[root@master tmp]# hadoop fs -cat /result1/part-r-00000#与上面的测试执行结果相同,成功girl 2hadoop 1haimeimei 1hello 6lilei 1world 1 这就是MapReduce的原理和执行流程不清楚的话可以去多查看一些资料 接下来还会写一些别的示例还有Partitioner分区的用法]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Java</tag>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop之简单的hdfs上文件的上传删除及查询]]></title>
    <url>%2F2018%2F10%2F18%2Fblog181018-1%2F</url>
    <content type="text"><![CDATA[概览1.Linux上进行上传下载查询操作2.Java代码实现上传下载查询操作 上次将Hadoop集群版搭建完成了,那么怎么上传下载文件呢? 1.Linux上进行上传下载查询操作首先将Hadoop服务启动 将master,slave1,slave2三台虚拟机启动 在master上启动Hadoop服务 1start-all.sh 然后jps查看命令是否启动成功,和Hadoop集群搭建中验证方法一致 确认启动成功后,使用Xshell分别连接虚拟机 这时候需要你先将需要操作的文件传输到虚拟机中或者在虚拟机中创建文件 使用Xftp将文件上传到虚拟机中或者自己创建文件,我们就将文件放置在/usr/tmp中 1cd /usr/tmp 然后我们创建一个文本文件 1234#touch 创建文件 mkdir 创建文件夹touch test#编辑文件vim test 可以随便输入一些内容 123456hello worldhello lileihello haimeimeihello hadoophello girlhello girl 保存退出然后将文件上传到hdfs根目录中 12345678#将文件上传到Hadoop根目录中hadoop fs -put test /#查看是否上传成功hadoop fs -ls /#查看文件内容,发现与之前的内容相同则上传成功#如果你在安装虚拟机的时候没有选择中文,而在文件中有中文内容,有可能造成乱码hadoop fs -cat /test#hdfs上的文件是不支持修改的 文件上传成功了,那么接下来试一试文件夹 123456#创建文件夹mkdir testdir#上传hadoop fs -put testdir /#查看hadoop fs -ls / 删除文件/文件夹 123456#删除文件 hadoop fs -rm -f /test#删除文件夹hadoop fs -rm -r /testdir#查看hadoop fs -ls / 将hdfs上的文件下载到本地 1234567891011121314151617#将文件上传上去[root@master tmp]# hadoop fs -put test /#将test改名为test1[root@master tmp]# hadoop fs -mv /test /test1#查看文件内容[root@master tmp]# hadoop fs -cat /test1hello worldhello lileihello haimeimeihello hadoophello girlhello girl#文件下载[root@master tmp]# hadoop fs -get /test1#查看tmp文件夹下的内容[root@master tmp]# lstest test1 testdir 这就是几种基本的Linux上进行上传下载查询操作 2.Java代码实现上传下载查询操作启动eclipse或者其他工具新建个Java项目,test包,TestHadoop类 然后在项目下创建个lib文件夹存放jar包 将hadoop解压文件夹中的jar复制到lib内如图,大概共有69个jar包 然后在eclipse中选择lib文件夹下所有jar包–&gt;右键Build Path–&gt;add to Build Path 因为是测试类,所以我们将JUnit4导入到Path中 在项目上右键–&gt;Build Path–&gt;Confirgure Build Path 然后,这是TestHadoop的内容 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111package com.hd.test;import java.io.FileNotFoundException;import java.io.IOException;import java.net.URI;import java.net.URISyntaxException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileStatus;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.junit.Test;public class TestHadoop &#123; /** * 上传文件 * @throws IOException * @throws InterruptedException * @throws URISyntaxException */ @Test public void upLoad() throws IOException, InterruptedException, URISyntaxException&#123; // 创建config对象 Configuration conf = new Configuration(); // 创建FileSystem对象 192.168.134.154是你的主机master的ip地址,root是你的用户名 FileSystem fs = FileSystem.get(new URI(&quot;hdfs://192.168.134.154:9000&quot;), conf, &quot;root&quot;); // 把本地磁盘上的文件(这个文件可以自己选择)上传到hdfs上面的根目录上,这里可以改名 fs.copyFromLocalFile(false, new Path(&quot;d:/TABS.DBF&quot;), new Path(&quot;/abc.a&quot;)); // 关闭 fs.close(); &#125; /** * 下载 * @throws IOException * @throws InterruptedException * @throws URISyntaxException */ @Test public void downLoad() throws IOException, InterruptedException, URISyntaxException&#123; // 创建config对象 Configuration conf = new Configuration(); // 创建FileSystem对象 FileSystem fs = FileSystem.get(new URI(&quot;hdfs://192.168.134.154:9000&quot;), conf, &quot;root&quot;); // 把hdfs上面的根目录上的文件下载到本地磁盘上 fs.copyToLocalFile(false, new Path(&quot;/abc.a&quot;), new Path(&quot;E:/&quot;), true); // 关闭 fs.close(); &#125; /** * 删除 * @throws IOException * @throws InterruptedException * @throws URISyntaxException */ @Test public void del() throws IOException, InterruptedException, URISyntaxException&#123; // 创建config对象 Configuration conf = new Configuration(); // 创建FileSystem对象 FileSystem fs = FileSystem.get(new URI(&quot;hdfs://192.168.134.154:9000&quot;), conf, &quot;root&quot;); // 删除,false只能删除空文件夹 fs.delete(new Path(&quot;/abc.a&quot;), true); // 关闭 fs.close(); &#125; /** * 创建文件夹 * @throws IOException * @throws InterruptedException * @throws URISyntaxException */ @Test public void mkdir() throws IOException, InterruptedException, URISyntaxException&#123; // 创建config对象 Configuration conf = new Configuration(); // 创建FileSystem对象 FileSystem fs = FileSystem.get(new URI(&quot;hdfs://192.168.134.154:9000&quot;), conf, &quot;root&quot;); // 创建目录 fs.mkdirs(new Path(&quot;/a/b&quot;)); // 关闭 fs.close(); &#125; /** * 遍历查询输出 * @throws FileNotFoundException * @throws IllegalArgumentException * @throws IOException * @throws InterruptedException * @throws URISyntaxException */ @Test public void out() throws FileNotFoundException, IllegalArgumentException, IOException, InterruptedException, URISyntaxException&#123; // 创建config对象 Configuration conf = new Configuration(); // 创建FileSystem对象 FileSystem fs = FileSystem.get(new URI(&quot;hdfs://192.168.134.154:9000&quot;), conf, &quot;root&quot;); // 遍历 FileStatus[] status = fs.listStatus(new Path(&quot;/&quot;)); for (int i = 0; i &lt; status.length; i++) &#123; if (status[i].isFile()) &#123; System.err.println(&quot;文件:&quot; + status[i].getPath().toString()); &#125; else if (status[i].isDirectory()) &#123; System.err.println(&quot;目录:&quot; + status[i].getPath().toString()); &#125; &#125; // 关闭 fs.close(); &#125; &#125; 每次运行只要双击方法名然后右键Run –&gt;JUnit Test就能测试运行 然后分别在hdfs上 , 本地E盘 和 eclipse控制台 查看是否运行成功 以后会接着介绍更多的关于hadoop的操作]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop集群版搭建]]></title>
    <url>%2F2018%2F10%2F17%2Fblog181017-1%2F</url>
    <content type="text"><![CDATA[概览1.规划2.克隆虚拟机3.在hosts文件修改ip映射4.修改hadoop配置文件5.更改slaves文件6.集群版ssh免密钥登录7.重新格式化namenode8.启动hadoop并验证是否成功9.若slave的datanode没有启动 上次说了Hadoop集群单机版的搭建,这次来依照单机版的基础搭建一个简单的集群版 1.规划这次搭建的是一个主机和两个从机,也就是只有两个node节点,也可以让主机上有node节点,之后会说|主机名|cluster规划 ||–|–|| master | namenode,secondarynamenode,ResourceManager || slave1 | Datanode, NodeManager || slave2 | Datanode, NodeManager | 首先克隆一台单机版虚拟机 2.克隆虚拟机虚拟机右键–&gt;管理–&gt;克隆,选择创建完整克隆,这里克隆几个要看你创建的集群规模,我便克隆三台,一台主机,两台从机. 然后启动虚拟机,改动静态ip 这个需要三台虚拟机都改动 还是到/etc/sysconfig/network-scripts文件夹下改动ifcfg-ens33文件 1vim /etc/sysconfig/network-scripts/ifcfg-ens33 前面配置单机版的时候,如果觉得vi的命令不好用,可以安装vim命令 yum -y install vim之后就可以使用vim命令了.比vi编辑命令更加清晰 改动如图:将改为这个虚拟机的ip只要是你没有使用的即可然后保存,重启服务 1systemctl restart network 接下来使用Xshell工具分别连接三台虚拟机(没有的去官网下载,有免费版本),Xshell的优点在于你可以随意的复制粘贴命令语句 3.在hosts文件修改ip映射找到到hosts文件进行编辑 1vim /etc/hosts 写入三台主机的ip地址和主机名 123192.168.134.154 master192.168.134.155 slave1192.168.134.156 slave2 这里改动过之后最好重启虚拟机reboot,让其生效,这样最后配置ssh免密钥登录时不会出现异常 4.修改hadoop配置文件如果你的主机名还是单机版的.可以不用更改 然后进入到 /usr/hadoop/hadoop-2.7.3/etc/hadoop/下修改core-site.xml和yarn-site.xml , 三个虚拟机都要更改,也可以只修改一台之后发送将之前的主机名改为现在的主机 master如图改为 将yarn-site.xml内的改为 当然,如果你的主机名还是用的单机版的,那么上述两步可以不改 接下来更改slaves文件 5.更改slaves文件还是在该文件夹下更改slaves文件三个虚拟机都需要更改(其实这些都可以在克隆前更改,再克隆,不过也可以配置好后再发送)1vim slaves 改为这里需要说明一下,如果你要主机也当作一个节点的话 , 那么在里面也要添加主机名称,这个文件就是告诉hadoop该启动谁的datanode 如果需要发送给另外主机的话就是 123#会直接覆盖掉之前的[root@master .ssh]# scp -r /usr/hadoop root@slave1:/usr/[root@master .ssh]# scp -r /usr/hadoop root@slave2:/usr/ 之后最重要的是ssh免密登录的配置 6.集群版ssh免密钥登录三台虚拟机都需要操作 进入到~/.ssh #每台机器先使用ssh执行以下，以在主目录产生一个.ssh 文件夹 ssh master 创建 ,然后进入 cd ~/.ssh 12345678910111213141516171819202122232425#输入以下命令，一路回车，用以产生公钥和秘钥[root@master .ssh]# ssh-keygen -t rsa -P &apos;&apos;#出现以下信息说明生成成功Generating public/private rsa key pair.Enter file in which to save the key (/root/.ssh/id_rsa): Your identification has been saved in /root/.ssh/id_rsa.Your public key has been saved in /root/.ssh/id_rsa.pub.The key fingerprint is:SHA256:6YO1h1emM9gcWvv9OT6ftHxLnjP9u8p25x1o30oq3No root@masterThe key&apos;s randomart image is:+---[RSA 2048]----+| || || || . || S o o || + O * . || . B.X. o.+.|| +o=+=**%|| .oEo*^^|+----[SHA256]-----+#将每台机器上的id_rsa.pub公钥内容复制到authorized_keys文件中[root@master .ssh]# cp id_rsa.pub authorized_keys 如果more authorized_keys 查看如图@后是你的主机名则表示正常,否则重新进行上几步进行覆写 然后分别把从机slave1和slave2的authorized_keys文件进行合并（最简单的方法是将其余三台slave主机的文件内容追加到master主机上） 12345678910111213141516171819202122#将所有的authorized_keys文件进行合并（最简单的方法是将其余三台slave主机的文件内容追加到master主机上）[root@slave1 .ssh]# cat ~/.ssh/authorized_keys | ssh root@master &apos;cat &gt;&gt; ~/.ssh/authorized_keys&apos;[root@slave2 .ssh]# cat ~/.ssh/authorized_keys | ssh root@master &apos;cat &gt;&gt; ~/.ssh/authorized_keys&apos;#查看master上的authorized_keys文件内容，类似如下即可[root@master .ssh]# more authorized_keys ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC5iw8+LlLxo0d77uaTChOKKJqfMHzp2jgzqV2hFAneFXqqWmrZ4/FrMUPenmdss19bP4Up9G7PGbJu29yZDvkDwlmuqnVajYyDOsCl7PPXPWXMIlxMGUHgSXLnQQi6QnWp04vJKDs0EbiRTd0ZYCSQefzJcZ8jbQ7bLYt6jtil7FfUupTdHTeexKKd8Mq3K7YFZHumKvhzs6wWiM+n41jANS083ss3OYmAdO2cU0w1BhLVvJhdzd6fNG3RXVCXI2v0XxCUHiqI9Oewl2qPOfKzeyy09bJxo371Ezjmt8GMrkA/Ecepkvx12qwNzC9bSPLfbnPWVo2gIxe4mMaFqCFJ root@masterssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC3CkB3Jejavt+yFphsbciktWciJmbcUBOv3ZLPVLW18ZxXGZKvG50EPXo/4By7P6IRW0wCa5YuckQEW+q6kmSatxqJ8e/K9a1mAk13N4f7V7M71Nn8IkujlF3gHYjKrmnEWpGJCyURzywIQTRArlIac1xj2SeM6q+gTMV9WrAKJupIRHli+W0kHVaYHNdKl7KMUT4KVrSl+h4wFwAd7Tcyj7JIbUcCCL6o/v/LqGFwpcJfbfUsuKJJho+tImh41j7mSXR8kRbTSZkcq5KX+iANrANwOHZ58tV5KXmMQjuVq7aJ985C16hHssB6zq/zjAxpxAyQIeE8Incc8U8ix root@slave1ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC//uaMbzbkYqxdgZJSdq+gdQYldzMQ7D3SxsUaNO5oVnVOszwbNnmL8vp1EUUehabQHPCAvCmLKUPXzfcxlyJEF/pnY77u4ySwsRVEpHvsDZbrclgCOrS6hW00sSx303KHLOgX70LfrmnohfUhvTxajzLXT+C8f5ZfTZ8meKD73HKl16jRwZQ8YhW9GUyuCkgQTGtKtTKPsRUd9LpAc/7/u8xvvvNvTYPxgyTJcUMzGSOHh8J3upI54ykY0FgBkjs1fCUaDalxAgsHw9B1iyx706WbcT6ymiQVMKGnnnM6k2KPvUvfD0swVfUSG+4ZsYSRHRTgWuiBbHoIr7DVd root@slave2 然后分发主机上的密钥 authorized_keys 12345#将master上的authorized_keys文件分发到其他主机上[root@master .ssh]# scp ~/.ssh/authorized_keys root@slave1:~/.ssh/[root@master .ssh]# scp ~/.ssh/authorized_keys root@slave2:~/.ssh/ 7.重新格式化namenode三台虚拟机都需要1hadoop namenode -format 8.启动hadoop并验证是否成功在主机master上直接启动start-all.sh从机会跟着启动 然后分别在主机从机上查看jps应该与规划相同|主机名|cluster规划 ||–|–|| master | namenode,secondarynamenode,ResourceManager || slave1 | Datanode, NodeManager || slave2 | Datanode, NodeManager | 9.若slave的datanode没有启动如果发现从机的datanode没有启动首先在主机master停止 stop-all.sh 然后进入到从机的/usr/local/hadoop/tmp/dfs/data cd /usr/local/hadoop/tmp/dfs/data 也就是hdfs-site.xml文件中dfs.datanode.data.dir的路径,data的存放位置,将其中的current删除 rm -rf current/ 然后重新初始化namenode 再重新启动hadoop即可 hadoop namenode -format start-all.sh 然后在http://主机master的ip:50070和http://主机master的ip:8088分别查看 若是如此便启动成功了 至此,简单的hadoop集群版搭建完成了 接下来会进行java代码操作hadoop文件上传下载删除的操作,看之后的blog]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop集群单机版搭建]]></title>
    <url>%2F2018%2F10%2F16%2Fblog181016-1%2F</url>
    <content type="text"><![CDATA[概览1.CentOS的安装2.设置Linux静态ip3.JDK的安装4.修改主机名和ip映射5.安装hadoop并修改配置文件6.格式化namenode7.启动hadoop8.验证是否启动成功9.设置ssh免密登录 首先本文是基于CentOS 7 , jdk1.8.0_141 和Hadoop2.7.3环境搭建 1.CentOS的安装 首先准备好CentOS7 64位的镜像 然后在VMware上安装虚拟机 这里注意选择镜像自动检测CentOS 64位, 不然之后步骤比较麻烦其他步骤都与普通安装虚拟机一样,直接默认下一步,然后开启虚拟机 这里直接进行回车继续即可 语言选择可以选择简体中文这时选择安装位置,直接点击完成即可,这样才能继续下一步操作 然后开始安装,设置并且设置root密码然后重新开机登录root账号即可,成功登录则表示CentOS安装成功 2.设置Linux静态ip 开机完成后需要我们设置静态ip,这样之后开机都不需要dhclient动态分配ip地址 首先找到/etc/sysconfig/network-scripts/下的ifcfg-ens33配置文件(如果没有找到此文件,说明你没有选择安装CentOS64位系统,建议重新安装) 1vi /etc/sysconfig/network-scripts/ifcfg-ens33 2.首先把BOOTPROTO=”dhcp”改成BOOTPROTO=”static”表示静态获取,然后把UUID注释掉,把ONBOOT=no改为yes表示开机自动静态获取,然后在最后追加比如下面的配置： 1234IPADDR=192.168.134.151 #自己的ip地址NETMASK=255.255.255.0GATEWAY=192.168.134.2DNS1=8.8.8.8 IPADDR就是静态IP，NETMASK是子网掩码，GATEWAY就是网关或者路由地址；需要说明，原来还有个NETWORK配置的是局域网网络号，这个是ifcalc自动计算的，所以这里配置这些就足够了，最终配置如下图： 如果不知道自己的GATEWAY可以去虚拟机的编辑查看虚拟网络编辑器中的NAT设置 最后保存退出 重启服务 centos6的网卡重启方法：service network restart centos7的网卡重启方法：systemctl restart network 然后查看自己的ip地址是不是自己设置的 centos6的查看ip方法: ifconfig centos7的查看ip方法: ip addr 接下来可以用Xshell工具连接虚拟机了,这样比较好操作(如果没有此工具的需要去官网下载,Xshell和配套的文件传输工具Xftp,都有免费版本) 3.JDK的安装 首先在usr的目录下创建一个java文件夹用来存放jdk的安装包并作为安装路径 这时候新建文件传输,将jdk的压缩包放入到CentOS中 然后解压jdk 解压完成之后返回到根目录的etc文件夹下,改写profile配置文件 vi /etc/profile 在profile最后加上: 1234export JAVA_HOME=/usr/java/jdk1.8.0_141export JAVA_BIN=/usr/java/jdk1.8.0_141/binexport PATH=$PATH:$JAVA_HOME/binexport CLASSPATH=:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar 保存退出,source /etc/profile刷新配置文件用java -version看看jdk环境是否配置完毕 4.修改主机名和ip映射修改etc文件夹下的hosts文件vi /etc/hosts如图: 5.安装hadoop并修改配置文件 在usr文件夹下创建hadoop文件夹作为压缩包存放和解压路径,将hadoop的压缩包传输到此文件夹下 然后解压 tar -zxf hadoop-2.7.3.tar.gz 5.1配置proflie文件注意：hadoop2.x的配置文件$HADOOP_HOME/etc/hadoop在etc的profile最后添加,然后source刷新配置文件 12export HADOOP_HOME=/usr/hadoop/hadoop-2.7.3export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin 接下来需要改五个配置文件5.2 第一个：hadoop-env.sh进入到hadoop-2.7.3/etc/hadoop文件夹下,修改hadoop-env.sh vi hadoop-env.sh第25行将改为保存退出5.3 第二个 core-site.xml在configuration中加上 12345678910&lt;!-- 制定HDFS的老大（NameNode）的地址 --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://zhiyou:9000&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定hadoop运行时产生文件的存储目录[能自动生成目录] --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/zhiyou/hadoop/tmp&lt;/value&gt; &lt;/property&gt; 保存退出 5.4 第三个 hdfs-site.xml同上configuration中添加 12345678910111213&lt;!-- 指定HDFS副本的数量 --&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;/usr/local/hadoop/tmp/dfs/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;/usr/local/hadoop/tmp/dfs/data&lt;/value&gt; &lt;/property&gt; 5.5 第四个mapred-site.xml 这个是需要你复制一个模版文件出来的 cp mapred-site.xml.template mapred-site.xml然后 vi mapred-site.xml添加 12345&lt;!-- 指定mr运行在yarn上 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; 保存退出 5.6 第五个 yarn-site.xml 12345678910&lt;!-- 指定YARN的老大（ResourceManager）的地址 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;zhiyou&lt;/value&gt; &lt;/property&gt; &lt;!-- reducer获取数据的方式 --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; 6.格式化namenode是对namenode进行初始化 1hadoop namenode -format 如果没有报错说明配置文件成功,否则重新检查配置文件 7.启动hadoop 先启动HDFS 1start-dfs.sh 再启动YARN 1start-yarn.sh 这里需要yes三次并输入你的root密码三次 8.验证是否启动成功jps 3912 DataNode4378 Jps4331 NodeManager4093 SecondaryNameNode3822 NameNode4239 ResourceManager 关闭防火墙 #停止firewall systemctl stop firewalld systemctl disable firewalld.service #禁止firewall开机启动 浏览器查看 http://ip地址:50070 （HDFS管理界面） http://ip地址:8088 (yarn管理界面）如果页面正常则说明hadoop配置成功 有人觉得每次启动都需要输入密码很繁琐,那么就设置ssh免密登录 9.设置ssh免密登录生成ssh免登陆密钥 #进入到我的home目录 cd ~/.ssh ssh-keygen -t rsa （四个回车）执行完这个命令后，会生成两个文件id_rsa（私钥）、id_rsa.pub（公钥） 将公钥拷贝到要免登陆的机器上1ssh-copy-id 192.168.134.151 然后重新启动虚拟机reboot 进入到hadoop下的sbin文件夹 1cd /usr/hadoop/hadoop-2.7.3/sbin 有个start-all.sh和stop-all.sh,这是启动和停止所有服务,这样更加快捷 启动之后再次验证是否启动成功 至此,基本的搭建已经完成]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[海琴烟]]></title>
    <url>%2F2018%2F10%2F15%2Ftest%2F</url>
    <content type="text"><![CDATA[]]></content>
  </entry>
  <entry>
    <title><![CDATA[你好]]></title>
    <url>%2F2018%2F10%2F15%2Fhello%2F</url>
    <content type="text"><![CDATA[你好,欢迎来到我的blog.]]></content>
  </entry>
</search>
