<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[MapReduce的原理和执行流程]]></title>
    <url>%2F2018%2F10%2F18%2Fblog181018-2%2F</url>
    <content type="text"><![CDATA[概览1.MapReduce简介2.MapReduce的原理和执行流程3.测试MapReduce4.Java代码实现 MapReduce简介MapReduce是一种分布式计算模型，是Google提出的，主要用于搜索领域，解决海量数据的计算问题。 MR有两个阶段组成：Map和Reduce，用户只需实现map()和reduce()两个函数，即可实现分布式计算。 MapReduce是一种并行可扩展计算模型，并且有较好的容错性，主要解决海量离线数据的批处理。实现下面目标★ 易于编程★ 良好的扩展性★ 高容错性 MapReduce有哪些角色？各自的作用是什么？MapReduce由JobTracker和TaskTracker组成。JobTracker负责资源管理和作业控制，TaskTracker负责任务的运行。 MapReduce的原理和执行流程MapReduce程序执行流程程序执行流程图如下： (1) 开发人员编写好MapReduce program，将程序打包运行。(2) JobClient向JobTracker申请可用Job，JobTracker返回JobClient一个可用Job ID。(3) JobClient得到Job ID后，将运行Job所需要的资源拷贝到共享文件系统HDFS中。(4) 资源准备完备后，JobClient向JobTracker提交Job。(5) JobTracker收到提交的Job后，初始化Job。(6) 初始化完成后，JobTracker从HDFS中获取输入splits(作业可以该启动多少Mapper任务)。(7) 与此同时，TaskTracker不断地向JobTracker汇报心跳信息，并且返回要执行的任务。(8) TaskTracker得到JobTracker分配(尽量满足数据本地化)的任务后，向HDFS获取Job资源(若数据是本地的，不需拷贝数据)。(9) 获取资源后，TaskTracker会开启JVM子进程运行任务。注：(3)中资源具体指什么？主要包含： ● 程序jar包、作业配置文件xml ● 输入划分信息，决定作业该启动多少个map任务 ● 本地文件，包含依赖的第三方jar包(-libjars)、依赖的归档文件(-archives)和普通文件(-files)，如果已经上传，则不需上传 MapReduce原理 MapReduce的执行步骤： 1、Map任务处理1.1 读取HDFS中的文件。每一行解析成一个&lt;k,v&gt;。每一个键值对调用一次map函数。 &lt;0,hello you&gt; &lt;10,hello me&gt;1.2 覆盖map()，接收1.1产生的&lt;k,v&gt;，进行处理，转换为新的&lt;k,v&gt;输出。 &lt;hello,1&gt; &lt;you,1&gt; &lt;hello,1&gt; &lt;me,1&gt; 1.3 对1.2输出的&lt;k,v&gt;进行分区。默认分为一个区。详见《Partitioner》1.4 对不同分区中的数据进行排序（按照k）、分组。分组指的是相同key的value放到一个集合中。 排序后： &lt;hello,1&gt; &lt;hello,1&gt; &lt;me,1&gt; &lt;you,1&gt;分组后：&lt;hello,{1,1}&gt;&lt;me,{1}&gt;&lt;you,{1}&gt; 1.5 （可选）对分组后的数据进行归约。详见《Combiner》2、Reduce任务处理 2.1 多个map任务的输出，按照不同的分区，通过网络copy到不同的reduce节点上。（shuffle）详见《shuffle过程分析》2.2 对多个map的输出进行合并、排序。覆盖reduce函数，接收的是分组后的数据，实现自己的业务逻辑， &lt;hello,2&gt; &lt;me,1&gt; &lt;you,1&gt; 处理后，产生新的&lt;k,v&gt;输出。 2.3 对reduce输出的&lt;k,v&gt;写到HDFS中。 测试MapReduce启动虚拟机利用Xshell工具连接 启动hadoop 1start-all.sh 上传到hdfs上一个文件test1文件test1的内容如下 123456hello worldhello lileihello haimeimeihello hadoophello girlhello girl 进入/usr/hadoop/hadoop-2.7.3/share/hadoop/mapreduce下 1cd /usr/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/ 有个 hadoop-mapreduce-examples-2.7.3.jar的jar包为test1执行这个jar包 1hadoop jar hadoop-mapreduce-examples-2.7.3.jar wordcount /test1 /result 等待执行,然后查看result文件内容 12345678910111213#查看result文件夹[root@master mapreduce]# hadoop fs -ls /resultFound 2 items-rw-r--r-- 1 root supergroup 0 2018-10-18 21:40 /result/_SUCCESS-rw-r--r-- 1 root supergroup 52 2018-10-18 21:40 /result/part-r-00000#查看part-r-00000内容[root@master mapreduce]# hadoop fs -cat /result/part-r-00000girl 2hadoop 1haimeimei 1hello 6lilei 1world 1 这个就是按照你所执行的文件,一次读取一行的内容,然后每行用空格分隔如图 Java代码实现首先eclipse建个Java项目,然后项目下建个lib文件夹放置jar包将复制到lib下,然后add to Build Path建立个mr包,在其中建立WCMapper,WCReducer,WordCount WCMapper 12345678910111213141516171819202122232425262728package com.hd.hadoop.mr;import java.io.IOException;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;//Mapper&lt;k1,v1,k2,v2&gt;//&lt;k1 long,v1 String&gt; &lt;k2 String,v2 long&gt;public class WCMapper extends Mapper&lt;LongWritable, Text, Text, LongWritable&gt; &#123; //重新一个map方法 @Override protected void map(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, LongWritable&gt;.Context context) throws IOException, InterruptedException &#123; // value代表文件中的每一行的数据 String line = value.toString(); //根据空格拆分字符串 String[] results = line.split(&quot; &quot;); //遍历数组得到每一个结果 for (String str : results) &#123; context.write(new Text(str), new LongWritable(1)); &#125; &#125;&#125; WCReducer 123456789101112131415161718192021222324252627package com.hd.hadoop.mr;import java.io.IOException;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Reducer;//k2 ,v2 k3 , v3public class WCReducer extends Reducer&lt;Text, LongWritable, Text, LongWritable&gt; &#123; //重写一个reduce方法 @Override protected void reduce(Text key2, Iterable&lt;LongWritable&gt; v2, Reducer&lt;Text, LongWritable, Text, LongWritable&gt;.Context context) throws IOException, InterruptedException &#123; //写自己的逻辑,统计单词个数 //定义一个遍历存放累加数据 long count=0; for (LongWritable lw : v2) &#123; count += lw.get(); &#125; //输出k3,v3 --&gt; String,Long context.write(key2, new LongWritable(count)); &#125; &#125; WordCount 1234567891011121314151617181920212223242526272829303132333435363738394041package com.zy.hadoop.mr;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class WordCount &#123; public static void main(String[] args) throws Exception &#123; // 1.获取job Configuration conf = new Configuration(); Job job = Job.getInstance(conf); // 2.指定job使用的类 job.setJarByClass(WordCount.class); // 3.设置Mapper的属性 job.setMapperClass(WCMapper.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(LongWritable.class); // 4.设置输入文件 FileInputFormat.setInputPaths(job, new Path(&quot;/test1&quot;)); // 5.设置reducer的属性 job.setReducerClass(WCReducer.class); job.setOutputKeyClass(Text.class); job.setMapOutputValueClass(LongWritable.class); // 6.设置输出文件夹,查看结果保存到hdfs文件夹中的位置 FileOutputFormat.setOutputPath(job, new Path(&quot;/result1&quot;)); // 7.提交 true 提交的时候打印日志信息 job.waitForCompletion(true); &#125;&#125; 然后打成jar包项目右键export 然后将打好的jar包利用Xftp工具放入到虚拟机master的/usr/tmp文件夹下cd /usr/tmp 然后执行1hadoop jar test1.jar 等待执行完毕 查看 123456789101112[root@master tmp]# hadoop fs -ls /result1Found 2 items-rw-r--r-- 1 root supergroup 0 2018-10-18 22:01 /result1/_SUCCESS-rw-r--r-- 1 root supergroup 55 2018-10-18 22:01 /result1/part-r-00000[root@master tmp]# hadoop fs -cat /result1/part-r-00000#与上面的测试执行结果相同,成功girl 2hadoop 1haimeimei 1hello 6lilei 1world 1 这就是MapReduce的原理和执行流程不清楚的话可以去多查看一些资料 接下来还会写一些别的示例还有Partitioner分区的用法]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop之简单的hdfs上文件的上传删除及查询]]></title>
    <url>%2F2018%2F10%2F18%2Fblog181018-1%2F</url>
    <content type="text"><![CDATA[概览1.Linux上进行上传下载查询操作2.Java代码实现上传下载查询操作 上次将Hadoop集群版搭建完成了,那么怎么上传下载文件呢? Linux上进行上传下载查询操作首先将Hadoop服务启动 将master,slave1,slave2三台虚拟机启动 在master上启动Hadoop服务 1start-all.sh 然后jps查看命令是否启动成功,和Hadoop集群搭建中验证方法一致 确认启动成功后,使用Xshell分别连接虚拟机 这时候需要你先将需要操作的文件传输到虚拟机中或者在虚拟机中创建文件 使用Xftp将文件上传到虚拟机中或者自己创建文件,我们就将文件放置在/usr/tmp中 1cd /usr/tmp 然后我们创建一个文本文件 1234#touch 创建文件 mkdir 创建文件夹touch test#编辑文件vim test 可以随便输入一些内容 123456hello worldhello lileihello haimeimeihello hadoophello girlhello girl 保存退出然后将文件上传到hdfs根目录中 12345678#将文件上传到Hadoop根目录中hadoop fs -put test /#查看是否上传成功hadoop fs -ls /#查看文件内容,发现与之前的内容相同则上传成功#如果你在安装虚拟机的时候没有选择中文,而在文件中有中文内容,有可能造成乱码hadoop fs -cat /test#hdfs上的文件是不支持修改的 文件上传成功了,那么接下来试一试文件夹 123456#创建文件夹mkdir testdir#上传hadoop fs -put testdir /#查看hadoop fs -ls / 删除文件/文件夹 123456#删除文件 hadoop fs -rm -f /test#删除文件夹hadoop fs -rm -r /testdir#查看hadoop fs -ls / 将hdfs上的文件下载到本地 1234567891011121314151617#将文件上传上去[root@master tmp]# hadoop fs -put test /#将test改名为test1[root@master tmp]# hadoop fs -mv /test /test1#查看文件内容[root@master tmp]# hadoop fs -cat /test1hello worldhello lileihello haimeimeihello hadoophello girlhello girl#文件下载[root@master tmp]# hadoop fs -get /test1#查看tmp文件夹下的内容[root@master tmp]# lstest test1 testdir 这就是几种基本的Linux上进行上传下载查询操作 Java代码实现上传下载查询操作启动eclipse或者其他工具新建个Java项目,test包,TestHadoop类 然后在项目下创建个lib文件夹存放jar包 将hadoop解压文件夹中的jar复制到lib内如图,大概共有69个jar包 然后在eclipse中选择lib文件夹下所有jar包–&gt;右键Build Path–&gt;add to Build Path 因为是测试类,所以我们将JUnit4导入到Path中 在项目上右键–&gt;Build Path–&gt;Confirgure Build Path 然后,这是TestHadoop的内容 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111package com.hd.test;import java.io.FileNotFoundException;import java.io.IOException;import java.net.URI;import java.net.URISyntaxException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileStatus;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.junit.Test;public class TestHadoop &#123; /** * 上传文件 * @throws IOException * @throws InterruptedException * @throws URISyntaxException */ @Test public void upLoad() throws IOException, InterruptedException, URISyntaxException&#123; // 创建config对象 Configuration conf = new Configuration(); // 创建FileSystem对象 192.168.134.154是你的主机master的ip地址,root是你的用户名 FileSystem fs = FileSystem.get(new URI(&quot;hdfs://192.168.134.154:9000&quot;), conf, &quot;root&quot;); // 把本地磁盘上的文件(这个文件可以自己选择)上传到hdfs上面的根目录上,这里可以改名 fs.copyFromLocalFile(false, new Path(&quot;d:/TABS.DBF&quot;), new Path(&quot;/abc.a&quot;)); // 关闭 fs.close(); &#125; /** * 下载 * @throws IOException * @throws InterruptedException * @throws URISyntaxException */ @Test public void downLoad() throws IOException, InterruptedException, URISyntaxException&#123; // 创建config对象 Configuration conf = new Configuration(); // 创建FileSystem对象 FileSystem fs = FileSystem.get(new URI(&quot;hdfs://192.168.134.154:9000&quot;), conf, &quot;root&quot;); // 把hdfs上面的根目录上的文件下载到本地磁盘上 fs.copyToLocalFile(false, new Path(&quot;/abc.a&quot;), new Path(&quot;E:/&quot;), true); // 关闭 fs.close(); &#125; /** * 删除 * @throws IOException * @throws InterruptedException * @throws URISyntaxException */ @Test public void del() throws IOException, InterruptedException, URISyntaxException&#123; // 创建config对象 Configuration conf = new Configuration(); // 创建FileSystem对象 FileSystem fs = FileSystem.get(new URI(&quot;hdfs://192.168.134.154:9000&quot;), conf, &quot;root&quot;); // 删除,false只能删除空文件夹 fs.delete(new Path(&quot;/abc.a&quot;), true); // 关闭 fs.close(); &#125; /** * 创建文件夹 * @throws IOException * @throws InterruptedException * @throws URISyntaxException */ @Test public void mkdir() throws IOException, InterruptedException, URISyntaxException&#123; // 创建config对象 Configuration conf = new Configuration(); // 创建FileSystem对象 FileSystem fs = FileSystem.get(new URI(&quot;hdfs://192.168.134.154:9000&quot;), conf, &quot;root&quot;); // 创建目录 fs.mkdirs(new Path(&quot;/a/b&quot;)); // 关闭 fs.close(); &#125; /** * 遍历查询输出 * @throws FileNotFoundException * @throws IllegalArgumentException * @throws IOException * @throws InterruptedException * @throws URISyntaxException */ @Test public void out() throws FileNotFoundException, IllegalArgumentException, IOException, InterruptedException, URISyntaxException&#123; // 创建config对象 Configuration conf = new Configuration(); // 创建FileSystem对象 FileSystem fs = FileSystem.get(new URI(&quot;hdfs://192.168.134.154:9000&quot;), conf, &quot;root&quot;); // 遍历 FileStatus[] status = fs.listStatus(new Path(&quot;/&quot;)); for (int i = 0; i &lt; status.length; i++) &#123; if (status[i].isFile()) &#123; System.err.println(&quot;文件:&quot; + status[i].getPath().toString()); &#125; else if (status[i].isDirectory()) &#123; System.err.println(&quot;目录:&quot; + status[i].getPath().toString()); &#125; &#125; // 关闭 fs.close(); &#125; &#125; 每次运行只要双击方法名然后右键Run –&gt;JUnit Test就能测试运行 然后分别在hdfs上 , 本地E盘 和 eclipse控制台 查看是否运行成功 以后会接着介绍更多的关于hadoop的操作]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop集群版搭建]]></title>
    <url>%2F2018%2F10%2F17%2Fblog181017-1%2F</url>
    <content type="text"><![CDATA[概览1.规划2.克隆虚拟机3.在hosts文件修改ip映射4.修改hadoop配置文件5.更改slaves文件6.集群版ssh免密钥登录7.重新格式化namenode8.启动hadoop并验证是否成功9.若slave的datenode没有启动 上次说了Hadoop集群单机版的搭建,这次来依照单机版的基础搭建一个简单的集群版 规划这次搭建的是一个主机和两个从机,也就是只有两个node节点,也可以让主机上有node节点,之后会说|主机名|cluster规划 ||–|–|| master | namenode,secondarynamenode,ResourceManager || slave1 | Datanode, NodeManager || slave2 | Datanode, NodeManager | 首先克隆一台单机版虚拟机 克隆虚拟机虚拟机右键–&gt;管理–&gt;克隆,选择创建完整克隆,这里克隆几个要看你创建的集群规模,我便克隆三台,一台主机,两台从机. 然后启动虚拟机,改动静态ip 这个需要三台虚拟机都改动 还是到/etc/sysconfig/network-scripts文件夹下改动ifcfg-ens33文件 1vim /etc/sysconfig/network-scripts/ifcfg-ens33 前面配置单机版的时候,如果觉得vi的命令不好用,可以安装vim命令 yum -y install vim之后就可以使用vim命令了.比vi编辑命令更加清晰 改动如图:将改为这个虚拟机的ip只要是你没有使用的即可然后保存,重启服务 1systemctl restart network 接下来使用Xshell工具分别连接三台虚拟机(没有的去官网下载,有免费版本),Xshell的优点在于你可以随意的复制粘贴命令语句 在hosts文件修改ip映射找到到hosts文件进行编辑 1vim /etc/hosts 写入三台主机的ip地址和主机名 123192.168.134.154 master192.168.134.155 slave1192.168.134.156 slave2 这里改动过之后最好重启虚拟机reboot,让其生效,这样最后配置ssh免密钥登录时不会出现异常 修改hadoop配置文件如果你的主机名还是单机版的.可以不用更改 然后进入到 /usr/hadoop/hadoop-2.7.3/etc/hadoop/下修改core-site.xml和yarn-site.xml , 三个虚拟机都要更改将之前的主机名改为现在的主机 master如图改为 将yarn-site.xml内的改为 当然,如果你的主机名还是用的单机版的,那么上述两步可以不改 接下来更改slaves文件 更改slaves文件还是在该文件夹下更改slaves文件三个虚拟机都需要更改(其实这些都可以在克隆前更改,再克隆,不过都开始了就算了o(∩_∩)o)1vim slaves 改为这里需要说明一下,如果你要主机也当作一个节点的话 , 那么在里面也要添加主机名称,这个文件就是告诉hadoop该启动谁的datanode 之后最重要的是ssh免密登录的配置 集群版ssh免密钥登录三台虚拟机都需要操作 进入到~/.ssh #每台机器先使用ssh执行以下，以在主目录产生一个.ssh 文件夹 ssh master 创建 ,然后进入 cd ~/.ssh 12345678910111213141516171819202122232425#输入以下命令，一路回车，用以产生公钥和秘钥[root@master .ssh]# ssh-keygen -t rsa -P &apos;&apos;#出现以下信息说明生成成功Generating public/private rsa key pair.Enter file in which to save the key (/root/.ssh/id_rsa): Your identification has been saved in /root/.ssh/id_rsa.Your public key has been saved in /root/.ssh/id_rsa.pub.The key fingerprint is:SHA256:6YO1h1emM9gcWvv9OT6ftHxLnjP9u8p25x1o30oq3No root@masterThe key&apos;s randomart image is:+---[RSA 2048]----+| || || || . || S o o || + O * . || . B.X. o.+.|| +o=+=**%|| .oEo*^^|+----[SHA256]-----+#将每台机器上的id_rsa.pub公钥内容复制到authorized_keys文件中[root@master .ssh]# cp id_rsa.pub authorized_keys 如果more authorized_keys 查看如图@后是你的主机名则表示正常,否则重新进行上几步进行覆写 然后分别把从机slave1和slave2的authorized_keys文件进行合并（最简单的方法是将其余三台slave主机的文件内容追加到master主机上） 12345678910111213141516171819202122#将所有的authorized_keys文件进行合并（最简单的方法是将其余三台slave主机的文件内容追加到master主机上）[root@slave1 .ssh]# cat ~/.ssh/authorized_keys | ssh root@master &apos;cat &gt;&gt; ~/.ssh/authorized_keys&apos;[root@slave2 .ssh]# cat ~/.ssh/authorized_keys | ssh root@master &apos;cat &gt;&gt; ~/.ssh/authorized_keys&apos;#查看master上的authorized_keys文件内容，类似如下即可[root@master .ssh]# more authorized_keys ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC5iw8+LlLxo0d77uaTChOKKJqfMHzp2jgzqV2hFAneFXqqWmrZ4/FrMUPenmdss19bP4Up9G7PGbJu29yZDvkDwlmuqnVajYyDOsCl7PPXPWXMIlxMGUHgSXLnQQi6QnWp04vJKDs0EbiRTd0ZYCSQefzJcZ8jbQ7bLYt6jtil7FfUupTdHTeexKKd8Mq3K7YFZHumKvhzs6wWiM+n41jANS083ss3OYmAdO2cU0w1BhLVvJhdzd6fNG3RXVCXI2v0XxCUHiqI9Oewl2qPOfKzeyy09bJxo371Ezjmt8GMrkA/Ecepkvx12qwNzC9bSPLfbnPWVo2gIxe4mMaFqCFJ root@masterssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC3CkB3Jejavt+yFphsbciktWciJmbcUBOv3ZLPVLW18ZxXGZKvG50EPXo/4By7P6IRW0wCa5YuckQEW+q6kmSatxqJ8e/K9a1mAk13N4f7V7M71Nn8IkujlF3gHYjKrmnEWpGJCyURzywIQTRArlIac1xj2SeM6q+gTMV9WrAKJupIRHli+W0kHVaYHNdKl7KMUT4KVrSl+h4wFwAd7Tcyj7JIbUcCCL6o/v/LqGFwpcJfbfUsuKJJho+tImh41j7mSXR8kRbTSZkcq5KX+iANrANwOHZ58tV5KXmMQjuVq7aJ985C16hHssB6zq/zjAxpxAyQIeE8Incc8U8ix root@slave1ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC//uaMbzbkYqxdgZJSdq+gdQYldzMQ7D3SxsUaNO5oVnVOszwbNnmL8vp1EUUehabQHPCAvCmLKUPXzfcxlyJEF/pnY77u4ySwsRVEpHvsDZbrclgCOrS6hW00sSx303KHLOgX70LfrmnohfUhvTxajzLXT+C8f5ZfTZ8meKD73HKl16jRwZQ8YhW9GUyuCkgQTGtKtTKPsRUd9LpAc/7/u8xvvvNvTYPxgyTJcUMzGSOHh8J3upI54ykY0FgBkjs1fCUaDalxAgsHw9B1iyx706WbcT6ymiQVMKGnnnM6k2KPvUvfD0swVfUSG+4ZsYSRHRTgWuiBbHoIr7DVd root@slave2 然后分发主机上的密钥 authorized_keys 12345#将master上的authorized_keys文件分发到其他主机上[root@master .ssh]# scp ~/.ssh/authorized_keys root@slave1:~/.ssh/[root@master .ssh]# scp ~/.ssh/authorized_keys root@slave2:~/.ssh/ 重新格式化namenode三台虚拟机都需要1hadoop namenode -format 启动hadoop并验证是否成功在主机master上直接启动start-all.sh从机会跟着启动 然后分别在主机从机上查看jps应该与规划相同|主机名|cluster规划 ||–|–|| master | namenode,secondarynamenode,ResourceManager || slave1 | Datanode, NodeManager || slave2 | Datanode, NodeManager | 若slave的datenode没有启动如果发现从机的datenode没有启动首先在主机master停止 stop-all.sh 然后进入到从机的/usr/local/hadoop/tmp/dfs/data cd /usr/local/hadoop/tmp/dfs/data 也就是hdfs-site.xml文件中dfs.datanode.data.dir的路径,data的存放位置,将其中的current删除 rm -rf current/ 然后重新初始化namenode 再重新启动hadoop即可 hadoop namenode -format start-all.sh 然后在http://主机master的ip:50070和http://主机master的ip:8088分别查看 若是如此便启动成功了 至此,简单的hadoop集群版搭建完成了 接下来会进行java代码操作hadoop文件上传下载删除的操作,看之后的blog]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop集群单机版搭建]]></title>
    <url>%2F2018%2F10%2F16%2Fblog181016-1%2F</url>
    <content type="text"><![CDATA[概览1.CentOS的安装2.设置Linux静态ip3.JDK的安装4.修改主机名和ip映射5.安装hadoop并修改配置文件6.格式化namenode7.启动hadoop8.验证是否启动成功9.设置ssh免密登录 首先本文是基于CentOS 7 , jdk1.8.0_141 和Hadoop2.7.3环境搭建 CentOS的安装 首先准备好CentOS7 64位的镜像 然后在VMware上安装虚拟机 这里注意选择镜像自动检测CentOS 64位, 不然之后步骤比较麻烦其他步骤都与普通安装虚拟机一样,直接默认下一步,然后开启虚拟机 这里直接进行回车继续即可 语言选择可以选择简体中文这时选择安装位置,直接点击完成即可,这样才能继续下一步操作 然后开始安装,设置并且设置root密码然后重新开机登录root账号即可,成功登录则表示CentOS安装成功 设置Linux静态ip 开机完成后需要我们设置静态ip,这样之后开机都不需要dhclient动态分配ip地址 首先找到/etc/sysconfig/network-scripts/下的ifcfg-ens33配置文件(如果没有找到此文件,说明你没有选择安装CentOS64位系统,建议重新安装) 1vi /etc/sysconfig/network-scripts/ifcfg-ens33 2.首先把BOOTPROTO=”dhcp”改成BOOTPROTO=”static”表示静态获取,然后把UUID注释掉,把ONBOOT=no改为yes表示开机自动静态获取,然后在最后追加比如下面的配置： 1234IPADDR=192.168.134.151 #自己的ip地址NETMASK=255.255.255.0GATEWAY=192.168.134.2DNS1=8.8.8.8 IPADDR就是静态IP，NETMASK是子网掩码，GATEWAY就是网关或者路由地址；需要说明，原来还有个NETWORK配置的是局域网网络号，这个是ifcalc自动计算的，所以这里配置这些就足够了，最终配置如下图： 如果不知道自己的GATEWAY可以去虚拟机的编辑查看虚拟网络编辑器中的NAT设置 最后保存退出 重启服务 centos6的网卡重启方法：service network restart centos7的网卡重启方法：systemctl restart network 然后查看自己的ip地址是不是自己设置的 centos6的查看ip方法: ifconfig centos7的查看ip方法: ip addr 接下来可以用Xshell工具连接虚拟机了,这样比较好操作(如果没有此工具的需要去官网下载,Xshell和配套的文件传输工具Xftp,都有免费版本) JDK的安装 首先在usr的目录下创建一个java文件夹用来存放jdk的安装包并作为安装路径 这时候新建文件传输,将jdk的压缩包放入到CentOS中 然后解压jdk 解压完成之后返回到根目录的etc文件夹下,改写profile配置文件 vi /etc/profile 在profile最后加上: 1234export JAVA_HOME=/usr/java/jdk1.8.0_141export JAVA_BIN=/usr/java/jdk1.8.0_141/binexport PATH=$PATH:$JAVA_HOME/binexport CLASSPATH=:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar 保存退出,source /etc/profile刷新配置文件用java -version看看jdk环境是否配置完毕 修改主机名和ip映射修改etc文件夹下的hosts文件vi /etc/hosts如图: 安装hadoop并修改配置文件 在usr文件夹下创建hadoop文件夹作为压缩包存放和解压路径,将hadoop的压缩包传输到此文件夹下 然后解压 tar -zxf hadoop-2.7.3.tar.gz 5.1配置proflie文件注意：hadoop2.x的配置文件$HADOOP_HOME/etc/hadoop在etc的profile最后添加,然后source刷新配置文件 12export HADOOP_HOME=/usr/hadoop/hadoop-2.7.3export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin 接下来需要改五个配置文件5.2 第一个：hadoop-env.sh进入到hadoop-2.7.3/etc/hadoop文件夹下,修改hadoop-env.sh vi hadoop-env.sh第25行将改为保存退出5.3 第二个 core-site.xml在configuration中加上 12345678910&lt;!-- 制定HDFS的老大（NameNode）的地址 --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://zhiyou:9000&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定hadoop运行时产生文件的存储目录[能自动生成目录] --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/zhiyou/hadoop/tmp&lt;/value&gt; &lt;/property&gt; 保存退出 5.4 第三个 hdfs-site.xml同上configuration中添加 12345678910111213&lt;!-- 指定HDFS副本的数量 --&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;/usr/local/hadoop/tmp/dfs/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;/usr/local/hadoop/tmp/dfs/data&lt;/value&gt; &lt;/property&gt; 5.5 第四个mapred-site.xml 这个是需要你复制一个模版文件出来的 cp mapred-site.xml.template mapred-site.xml然后 vi mapred-site.xml添加 12345&lt;!-- 指定mr运行在yarn上 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; 保存退出 5.6 第五个 yarn-site.xml 12345678910&lt;!-- 指定YARN的老大（ResourceManager）的地址 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;zhiyou&lt;/value&gt; &lt;/property&gt; &lt;!-- reducer获取数据的方式 --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; 格式化namenode是对namenode进行初始化 1hadoop namenode -format 如果没有报错说明配置文件成功,否则重新检查配置文件 启动hadoop 先启动HDFS 1start-dfs.sh 再启动YARN 1start-yarn.sh 这里需要yes三次并输入你的root密码三次 验证是否启动成功jps 3912 DataNode4378 Jps4331 NodeManager4093 SecondaryNameNode3822 NameNode4239 ResourceManager 关闭防火墙 #停止firewall systemctl stop firewalld systemctl disable firewalld.service #禁止firewall开机启动 浏览器查看 http://ip地址:50070 （HDFS管理界面） http://ip地址:8088 (yarn管理界面）如果页面正常则说明hadoop配置成功 有人觉得每次启动都需要输入密码很繁琐,那么就设置ssh免密登录 设置ssh免密登录生成ssh免登陆密钥 #进入到我的home目录 cd ~/.ssh ssh-keygen -t rsa （四个回车）执行完这个命令后，会生成两个文件id_rsa（私钥）、id_rsa.pub（公钥） 将公钥拷贝到要免登陆的机器上1ssh-copy-id 192.168.134.151 然后重新启动虚拟机reboot 进入到hadoop下的sbin文件夹 1cd /usr/hadoop/hadoop-2.7.3/sbin 有个start-all.sh和stop-all.sh,这是启动和停止所有服务,这样更加快捷 启动之后再次验证是否启动成功 至此,基本的搭建已经完成]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[海琴烟]]></title>
    <url>%2F2018%2F10%2F15%2Ftest%2F</url>
    <content type="text"><![CDATA[]]></content>
  </entry>
  <entry>
    <title><![CDATA[你好]]></title>
    <url>%2F2018%2F10%2F15%2Fhello%2F</url>
    <content type="text"><![CDATA[你好,欢迎来到我的blog.]]></content>
  </entry>
</search>
