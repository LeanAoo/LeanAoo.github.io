<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[sqoop的安装]]></title>
    <url>%2F2018%2F10%2F23%2Fblog181023-2%2F</url>
    <content type="text"><![CDATA[概览1.sqoop简介2.sqoop的安装和配置3.测试4.异常处理 1.sqoop简介Sqoop(发音：skup)是一款开源的工具，主要用于在Hadoop(Hive)与传统的数据库(mysql、postgresql…)间进行数据的传递，可以将一个关系型数据库（例如 ： MySQL ,Oracle ,Postgres等）中的数据导进到Hadoop的HDFS中，也可以将HDFS的数据导进到关系型数据库中。 sqoop 是 apache 旗下一款“Hadoop 和关系数据库服务器之间传送数据”的工具。 核心的功能有两个：导入和导出! 导入数据：MySQL，Oracle 导入数据到 Hadoop 的 HDFS、HIVE、HBASE 等数据存储系统 导出数据：从 Hadoop 的文件系统中导出数据到关系数据库 mysql 等 Sqoop 的本质还是一个命令行工具，和 HDFS，Hive 相比，并没有什么高深的理论。 sqoop： 工具：本质就是迁移数据， 迁移的方式：就是把sqoop的迁移命令转换成MR程序 hive 工具，本质就是执行计算，依赖于HDFS存储数据，把SQL转换成MR程序 工作原理是将导入或导出命令翻译成 MapReduce 程序来实现 在翻译出的 MapReduce 中主要是对 InputFormat 和 OutputFormat 进行定制 2.sqoop的安装和配置软件:sqoop-1.4.7.bin__hadoop-2.6.0.tar2.1.安装在/usr下创建个sqoop文件夹,作为压缩包的存放路径和解压路径 123456789#进入/usr下[root@master ~]# cd /usr/#创建sqoop文件夹[root@master usr]# mkdir sqoop[root@master usr]# lsbin etc games hadoop include java lib lib64 libexec local sbin share sqoop src tmp zookeeper#进入sqoop下[root@master usr]# cd sqoop[root@master sqoop]# 利用Xshell连接虚拟机,并利用Xftp将sqoop压缩包上传到sqoop文件夹下解压sqoop 1[root@master sqoop]# tar -zxf sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz 2.2.配置profile环境变量进入到根目录的etc文件夹下,更改profile文件 1[root@master sqoop]# vim /etc/profile 在profile最后添加 12export SQOOP_HOME=/usr/sqoop/sqoop-1.4.7.bin__hadoop-2.6.0export PATH=$PATH:$SQOOP_HOME/bin 保存退出,刷新profile 1[root@master sqoop]# source /etc/profile 2.3拷贝驱动 将数据库连接驱动拷贝到$SQOOP_HOME(sqoop安装目录)/lib里进入到你的sqoop安装目录的lib下,利用Xftp将驱动传输进去我的mysql数据库连接驱动的百度云链接：https://pan.baidu.com/s/1wCBuZQaCP_nKT5t504SeoA提取码：fakd 2.4.使用前准备,mysql允许远程连接如果你在安装mysql的时候已经允许过了,就可以跳过这个步骤 123456#进入mysql[root@master sqoop]# mysql#允许远程连接mysql&gt;GRANT ALL PRIVILEGES ON *.* TO &apos;root&apos;@&apos;%&apos; IDENTIFIED BY &apos;root&apos; WITH GRANT OPTION;#刷新权限mysql&gt;FLUSH PRIVILEGES; 2.5.集群配置如果是集群可以将sqoop发送到其他主机上,伪集群版可以跳过发送sqoop123[root@master sqoop]# scp -r /usr/sqoop root@slave1:/usr/[root@master sqoop]# scp -r /usr/sqoop root@slave2:/usr/ 发送配置好的profile文件 123[root@master sqoop]# scp -r /etc/profile root@slave1:/etc/[root@master sqoop]# scp -r /etc/profile root@slave2:/etc/ 分别刷新profile 123[root@slave1 ~]# source /etc/profile[root@slave2 ~]# source /etc/profile 3.测试关闭防火墙 1systemctl stop firewalld 分别在集群的Zookeeper安装目录下的bin下启动Zookeeper 123456[root@master sqoop]# cd /usr/zookeeper/zookeeper-3.4.12/bin/#启动Zookeeper[root@master bin]# ./zkServer.sh startZooKeeper JMX enabled by defaultUsing config: /usr/zookeeper/zookeeper-3.4.12/bin/../conf/zoo.cfgStarting zookeeper ... STARTED 确保的你的集群正常启动,请参考Hadoop HA高可用集群搭建 1[root@master bin]# start-all.sh 然后随便在哪个目录下(已经设置完成环境变量)输入 12#mysql后是我安装mysql主机的ip地址[root@master bin]# sqoop list-databases --connect jdbc:mysql://192.168.134.154:3306/ --username root --password root 在分别在别的虚拟机使用sqoop看结果是否相同如上图则说明sqoop安装成功了 4.异常处理如果你出了类似的异常 12345618/10/23 21:07:17 ERROR manager.CatalogQueryManager: Failed to list databasesjava.sql.SQLException: Access denied for user &apos;root&apos;@&apos;master&apos; (using password: YES) at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1094) at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4208) at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4140) at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:925) 利用SQLyog或者Navicat连接虚拟机数据库 将%这行的password复制给上面所以没有password的保存然后重启数据库服务 1[root@master bin]# systemctl restart mysqld 再次尝试,应该就能成功了 1[root@master bin]# sqoop list-databases --connect jdbc:mysql://192.168.134.154:3306/ --username root --password root 分别在别的虚拟机上查看是否成功 123[root@slave1 sbin]# sqoop list-databases --connect jdbc:mysql://192.168.134.154:3306/ --username root --password root[root@slave2 sbin]# sqoop list-databases --connect jdbc:mysql://192.168.134.154:3306/ --username root --password root]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>sqoop</tag>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql的安装(Linux)]]></title>
    <url>%2F2018%2F10%2F23%2Fblog181023-1%2F</url>
    <content type="text"><![CDATA[概览1.安装mysql客户端2.数据库字符集设置3.启动mysql服务4.测试mysql命令5.设置root密码6.设置mysql运行远程访问7.设置开机自启8.测试连接如果是集群版的安装在你规划的需要安装mysql的虚拟机(服务器)上安装 1.安装mysql客户端1.1 安装wget命令 1yum -y install wget 1.2 下载mysql的repo源 1wget http://repo.mysql.com/mysql-community-release-el7-5.noarch.rpm 1.3 安装 安装1.3.mysql-community-release-el7-5.noarch.rpm包 1rpm -ivh mysql-community-release-el7-5.noarch.rpm 1.4 安装mysql客户端 123yum install mysql-serveryun install mysql-devel 等待安装完毕 2.数据库字符集设置配置mysql文件: 1vim /etc/my.cnf 在最后添加配置参数 1character-set-server=utf8 3.启动mysql服务123service mysqld start或者systemctl start mysqld 4.测试mysql命令输入mysql命令 12345678910111213[root@master ~]# systemctl start mysqld[root@master ~]# mysqlWelcome to the MySQL monitor. Commands end with ; or \g.Your MySQL connection id is 2Server version: 5.6.42 MySQL Community Server (GPL)Copyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type &apos;help;&apos; or &apos;\h&apos; for help. Type &apos;\c&apos; to clear the current input statement. 5.设置root密码12345mysql&gt; use mysqlReading table information for completion of table and column namesYou can turn off this feature to get a quicker startup with -ADatabase changed 设置密码 123mysql&gt; update user set authentication_string = password(&apos;root&apos;), password_expired = &apos;N&apos; where user = &apos;root&apos;;Query OK, 4 rows affected (0.00 sec)Rows matched: 4 Changed: 4 Warnings: 0 6.设置mysql运行远程访问12345mysql&gt; GRANT ALL PRIVILEGES ON *.* TO &apos;root&apos;@&apos;%&apos; IDENTIFIED BY &apos;root&apos; WITH GRANT OPTION;Query OK, 0 rows affected (0.00 sec)mysql&gt; flush privileges; #刷新权限Query OK, 0 rows affected (0.00 sec) 7.设置开机自启123456789101112131415161718192021mysql&gt; exit;Bye[root@master ~]# vim /etc/rc.local #!/bin/bash# THIS FILE IS ADDED FOR COMPATIBILITY PURPOSES## It is highly advisable to create own systemd services or udev rules# to run scripts during boot instead of using this file.## In contrast to previous versions due to parallel execution during boot# this script will NOT be run after all other services.## Please note that you must run &apos;chmod +x /etc/rc.d/rc.local&apos; to ensure# that this script will be executed during boot.touch /var/lock/subsys/local#追加内容service mysqld start~ 8.测试连接使用SQLyog或者Navicat工具测试连接,注意将防火墙关闭systemctl stop firewalld 测试连接成功则代表安装成功]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>mysql</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop中JournalNode的作用]]></title>
    <url>%2F2018%2F10%2F22%2Fblog181022-2%2F</url>
    <content type="text"><![CDATA[NameNode之间共享数据（NFS 、Quorum Journal Node（用得多）） 两个NameNode为了数据同步，会通过一组称作JournalNodes的独立进程进行相互通信。当active状态的NameNode的命名空间有任何修改时，会告知大部分的JournalNodes进程。standby状态的NameNode有能力读取JNs中的变更信息，并且一直监控edit log的变化，把变化应用于自己的命名空间。standby可以确保在集群出错时，命名空间状态已经完全同步了。 Hadoop中的NameNode好比是人的心脏，非常重要，绝对不可以停止工作。在hadoop1时代，只有一个NameNode。如果该NameNode数据丢失或者不能工作，那么整个集群就不能恢复了。这是hadoop1中的单点问题，也是hadoop1不可靠的表现，如图1所示。hadoop2就解决了这个问题。图1 hadoop2.2.0（HA）中HDFS的高可靠指的是可以同时启动2个NameNode。其中一个处于工作状态，另一个处于随时待命状态。这样，当一个NameNode所在的服务器宕机时，可以在数据不丢失的情况下，手工或者自动切换到另一个NameNode提供服务。 这些NameNode之间通过共享数据，保证数据的状态一致。多个NameNode之间共享数据，可以通过Nnetwork File System或者Quorum Journal Node。前者是通过linux共享的文件系统，属于操作系统的配置；后者是hadoop自身的东西，属于软件的配置。 我们这里讲述使用Quorum Journal Node的配置方式，方式是手工切换。 集群启动时，可以同时启动2个NameNode。这些NameNode只有一个是active的，另一个属于standby状态。active状态意味着提供服务，standby状态意味着处于休眠状态，只进行数据同步，时刻准备着提供服务，如图2所示。图2 在一个典型的HA集群中，每个NameNode是一台独立的服务器。在任一时刻，只有一个NameNode处于active状态，另一个处于standby状态。其中，active状态的NameNode负责所有的客户端操作，standby状态的NameNode处于从属地位，维护着数据状态，随时准备切换。`两个NameNode为了数据同步，会通过一组称作JournalNodes的独立进程进行相互通信。当active状态的NameNode的命名空间有任何修改时，会告知大部分的JournalNodes进程。standby状态的NameNode有能力读取JNs中的变更信息，并且一直监控edit log的变化，把变化应用于自己的命名空间。standby可以确保在集群出错时，命名空间状态已经完全同步了，如图3所示。图3 为了确保快速切换，standby状态的NameNode有必要知道集群中所有数据块的位置。为了做到这点，所有的datanodes必须配置两个NameNode的地址，发送数据块位置信息和心跳给他们两个。 对于HA集群而言，确保同一时刻只有一个NameNode处于active状态是至关重要的。否则，两个NameNode的数据状态就会产生分歧，可能丢失数据，或者产生错误的结果。为了保证这点，JNs必须确保同一时刻只有一个NameNode可以向自己写数据。 硬件资源 为了部署HA集群，应该准备以下事情： NameNode服务器：运行NameNode的服务器应该有相同的硬件配置。 JournalNode服务器：运行的JournalNode进程非常轻量，可以部署在其他的服务器上。注意：必须允许至少3个节点。当然可以运行更多，但是必须是奇数个，如3、5、7、9个等等。当运行N个节点时，系统可以容忍至少(N-1)/2(N至少为3)个节点失败而不影响正常运行。 在HA集群中，standby状态的NameNode可以完成checkpoint操作，因此没必要配置Secondary NameNode、CheckpointNode、BackupNode。如果真的配置了，还会报错。]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>JournalNode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop 2.0 HA 3节点高可用集群搭建]]></title>
    <url>%2F2018%2F10%2F22%2Fblog181022-1%2F</url>
    <content type="text"><![CDATA[概览1.集群规划2.准备3.修改Hadoop配置文件4.复制内容5.启动集群6.查看jps7.测试 1.集群规划 HDFS HA背景 HDFS集群中NameNode 存在单点故障（SPOF）。对于只有一个NameNode的集群，如果NameNode机器出现意外情况，将导致整个集群无法使用，直到NameNode 重新启动。 影响HDFS集群不可用主要包括以下两种情况：一是NameNode机器宕机，将导致集群不可用，重启NameNode之后才可使用；二是计划内的NameNode节点软件或硬件升级，导致集群在短时间内不可用。 为了解决上述问题，Hadoop给出了HDFS的高可用HA方案：HDFS通常由两个NameNode组成，一个处于active状态，另一个处于standby状态。Active NameNode对外提供服务，比如处理来自客户端的RPC请求，而Standby NameNode则不对外提供服务，仅同步Active NameNode的状态，以便能够在它失败时快速进行切换。 规划之后的服务启动和配置文件都是安装此配置来,master上是namenode,slave2上是yarn,而slave1则是namenode和yarn的备用 需要说明以下几点： HDFS HA通常由两个NameNode组成，一个处于Active状态，另一个处于Standby状态。Active NameNode对外提供服务，而Standby NameNode则不对外提供服务，仅同步Active NameNode的状态，以便能够在它失败时快速进行切换。 Hadoop 2.0官方提供了两种HDFS HA的解决方案，一种是NFS，另一种是QJM。这里我们使用简单的QJM。在该方案中，主备NameNode之间通过一组JournalNode同步元数据信息，一条数据只要成功写入多数JournalNode即认为写入成功。通常配置奇数个JournalNode，这里还配置了一个Zookeeper集群，用于ZKFC故障转移，当Active NameNode挂掉了，会自动切换Standby NameNode为Active状态。 YARN的ResourceManager也存在单点故障问题，这个问题在hadoop-2.4.1得到了解决：有两个ResourceManager，一个是Active，一个是Standby，状态由zookeeper进行协调。 YARN框架下的MapReduce可以开启JobHistoryServer来记录历史任务信息，否则只能查看当前正在执行的任务信息。 Zookeeper的作用是负责HDFS中NameNode主备节点的选举，和YARN框架下ResourceManaer主备节点的选举。 2.准备软件:1.jdk1.8.1412.hadoop2.7.3(jdk1.8版本编译)3.Zookeeper3.4.124.Xshell5 + Xftp5 2.1.设置静态ip,参考Hadoop集群单机版的设置静态ip,然后使用Xshell工具连接(官网有免费版本) 2.2.配置jdk,hosts文件jdk安装参考Hadoop集群单机版的jdk安装123456789[root@master bin]# vi /etc/hosts#127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 #::1 localhost localhost.localdomain localhost6 localhost6.localdomain6# 上面的给注释掉或者删除192.168.134.154 master192.168.134.155 slave1192.168.134.156 slave2 2.3.配置ssh免密登录,参考Hadoop集群搭建的ssh免密登录 2.4.配置Zookeeper,参考Zookeeper的安装 3.修改Hadoop配置文件如果你之前搭建过hadoop集群,只需要将其中的配置文件做修改即可 1.在/usr下创建个hadoop文件夹,作为hadoop安装(压缩)包的存放路径和解压路径 123456#进入usr文件夹下cd /usr#创建hadoop文件夹mkdir hadoop#进入hadoop文件夹cd hadoop 利用Xftp工具将文件传输到虚拟机中解压后进入到 hadoop的解压路径/etc/hadoop文件夹下 1cd /usr/hadoop/hadoop-2.7.3/etc/hadoop/ 3.1.core-site.xml 1vim core-site.xml 在其中的configuration标签中添加以下内容 123456789101112131415161718192021222324252627282930313233343536&lt;!-- 指定hdfs的nameservice为ns --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://ns&lt;/value&gt; &lt;/property&gt; &lt;!--指定hadoop数据存放目录--&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/HA/hadoop/tmp&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;io.file.buffer.size&lt;/name&gt; &lt;value&gt;4096&lt;/value&gt; &lt;/property&gt; &lt;!--指定zookeeper地址--&gt; &lt;property&gt; &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt; &lt;value&gt;master:2181,slave1:2181,slave2:2181&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;ipc.client.connect.max.retries&lt;/name&gt; &lt;value&gt;100&lt;/value&gt; &lt;description&gt;Indicates the number of retries a client will make to establish a server connection. &lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;ipc.client.connect.retry.interval&lt;/name&gt; &lt;value&gt;10000&lt;/value&gt; &lt;description&gt;Indicates the number of milliseconds a client will wait for before retrying to establish a server connection. &lt;/description&gt; &lt;/property&gt; 3.2.hdfs-site.xml 1vim hdfs-site.xml 在其中的configuration标签中添加以下内容1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980&lt;!--指定hdfs的nameservice为ns，需要和core-site.xml中的保持一致 --&gt; &lt;property&gt; &lt;name&gt;dfs.nameservices&lt;/name&gt; &lt;value&gt;ns&lt;/value&gt; &lt;/property&gt; &lt;!-- ns下面有两个NameNode，分别是nn1，nn2 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.namenodes.ns&lt;/name&gt; &lt;value&gt;nn1,nn2&lt;/value&gt; &lt;/property&gt; &lt;!-- nn1的RPC通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.ns.nn1&lt;/name&gt; &lt;value&gt;master:9000&lt;/value&gt; &lt;/property&gt; &lt;!-- nn1的http通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.ns.nn1&lt;/name&gt; &lt;value&gt;master:50070&lt;/value&gt; &lt;/property&gt; &lt;!-- nn2的RPC通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.ns.nn2&lt;/name&gt; &lt;value&gt;slave1:9000&lt;/value&gt; &lt;/property&gt; &lt;!-- nn2的http通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.ns.nn2&lt;/name&gt; &lt;value&gt;slave1:50070&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定NameNode的元数据在JournalNode上的存放位置 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt; &lt;value&gt;qjournal://master:8485;slave1:8485;slave2:8485/ns&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定JournalNode在本地磁盘存放数据的位置 --&gt; &lt;property&gt; &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt; &lt;value&gt;/HA/hadoop/journal&lt;/value&gt; &lt;/property&gt; &lt;!-- 开启NameNode故障时自动切换 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置失败自动切换实现方式 --&gt; &lt;property&gt; &lt;name&gt;dfs.client.failover.proxy.provider.ns&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置隔离机制 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt; &lt;value&gt;sshfence&lt;/value&gt; &lt;/property&gt; &lt;!-- 使用隔离机制时需要ssh免登陆 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt; &lt;value&gt;/root/.ssh/id_rsa&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:///HA/hadoop/hdfs/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:///HA/hadoop/hdfs/data&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;/property&gt; &lt;!-- 在NN和DN上开启WebHDFS (REST API)功能,不是必须 --&gt; &lt;property&gt; &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; 3.3.mapred-site.xml这个文件刚开始是没有的,所以我们需要将其创建出来 12#利用模版文件copy出来一个cp mapred-site.xml.template mapred-site.xml 然后在其configuration标签中添加以下内容vim mapred-site.xml 12345&lt;!-- 指定mr运行在yarn上 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; 3.4.yarn-site.xml 1vim yarn-site.xml 在其configuration标签中添加以下内容普通版只有slave2有Resourcemanager12345678910&lt;!-- 指定nodemanager启动时加载server的方式为shuffle server --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定resourcemanager地址 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;slave2&lt;/value&gt; &lt;/property&gt; yarn HA高可用版 slave1和slave2都有Resourcemanager1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980&lt;!-- //////////////以下为YARN HA的配置////////////// --&gt; &lt;!-- 开启YARN HA --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 启用自动故障转移 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.automatic-failover.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定YARN HA的名称 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt; &lt;value&gt;yarncluster&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定两个resourcemanager的名称 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt; &lt;value&gt;rm1,rm2&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置rm1，rm2的主机 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt; &lt;value&gt;slave2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt; &lt;value&gt;slave1&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置YARN的http端口 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address.rm1&lt;/name&gt; &lt;value&gt;slave2:8088&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address.rm2&lt;/name&gt; &lt;value&gt;slave1:8088&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置zookeeper的地址 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt; &lt;value&gt;master:2181,slave1:2181,slave2:2181&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置zookeeper的存储位置 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.zk-state-store.parent-path&lt;/name&gt; &lt;value&gt;/rmstore&lt;/value&gt; &lt;/property&gt; &lt;!-- 开启yarn resourcemanager restart --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.recovery.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置resourcemanager的状态存储到zookeeper中 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.store.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore&lt;/value&gt; &lt;/property&gt; &lt;!-- 开启yarn nodemanager restart --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.recovery.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置nodemanager IPC的通信端口 --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.address&lt;/name&gt; &lt;value&gt;0.0.0.0:45454&lt;/value&gt; &lt;/property&gt; 3.5 .hadoop-env.sh 12export JAVA_HOME=$&#123;JAVA_HOME&#125; 一行，将其修改为 export JAVA_HOME=/usr/java/jdkxxx(jdk的安装路径) 3.6.修改slaves文件(dataNode)修改为 123456#localhost#你的集群主机名masterslave1slave2 4.复制内容到slave1,slave2如果你的slave1和slave2什么也没有,可以一并将配置jdk的profile文件和配置ip映射的hosts文件一起复制过去,Zookeeper则需要注意改下配置文件1234#复制给slave1,如果之前有hadoop也会覆盖[root@master hadoop]# scp -r /usr/hadoop root@slave1:/usr/#复制给slave2[root@master hadoop]# scp -r /usr/hadoop root@slave2:/usr/ 5.启动集群5.1分别启动Zookeeper所有虚拟机全部启动在Zookeeper安装目录的/bin目录下启动 12345[root@master hadoop]# cd /usr/zookeeper/zookeeper-3.4.12/bin[root@master bin]# ./zkServer.sh startZooKeeper JMX enabled by defaultUsing config: /usr/zookeeper/zookeeper-3.4.12/bin/../conf/zoo.cfgStarting zookeeper ... STARTED 启动后查看状态 1234[root@slave1 bin]# ./zkServer.sh statusZooKeeper JMX enabled by defaultUsing config: /usr/zookeeper/zookeeper-3.4.12/bin/../conf/zoo.cfgMode: leader #leader或者follower则代表启动Zookeeper成功 5.2在master,slave1,slave2上启动journalnode 123456789101112131415#进入到hadoop安装目录sbin文件夹下[root@master bin]# cd /usr/hadoop/hadoop-2.7.3/sbin/[root@master sbin]# lsdistribute-exclude.sh kms.sh start-balancer.sh stop-all.cmd stop-yarn.cmdhadoop-daemon.sh mr-jobhistory-daemon.sh start-dfs.cmd stop-all.sh stop-yarn.shhadoop-daemons.sh refresh-namenodes.sh start-dfs.sh stop-balancer.sh yarn-daemon.shhdfs-config.cmd slaves.sh start-secure-dns.sh stop-dfs.cmd yarn-daemons.shhdfs-config.sh start-all.cmd start-yarn.cmd stop-dfs.shhttpfs.sh start-all.sh start-yarn.sh stop-secure-dns.sh#这里有一个daemons和daemon,不带s是启动单个,带s是启动集群[root@master sbin]# ./hadoop-daemons.sh start journalnodeslave2: starting journalnode, logging to /usr/hadoop/hadoop-2.7.3/logs/hadoop-root-journalnode-slave2.outslave1: starting journalnode, logging to /usr/hadoop/hadoop-2.7.3/logs/hadoop-root-journalnode-slave1.outmaster: starting journalnode, logging to /usr/hadoop/hadoop-2.7.3/logs/hadoop-root-journalnode-master.out 分别在master,slave1,slave2上查看jps 12345#这样正常,否则查看你的Zookeeper是否启动成功[root@master sbin]# jps2232 JournalNode2281 Jps2157 QuorumPeerMain 5.3在master上格式化zkfc 1[root@master sbin]# hdfs zkfc -formatZK 5.4在master上格式化hdfs 1[root@master sbin]# hadoop namenode -format 5.5在master上启动namenode 1234567[root@master sbin]# ./hadoop-daemon.sh start namenodestarting namenode, logging to /usr/hadoop/hadoop-2.7.3/logs/hadoop-root-namenode-master.out[root@master sbin]# jps2232 JournalNode2490 Jps2157 QuorumPeerMain2431 NameNode 5.6在slave1上启动数据同步和standby的namenode 123[root@slave1 sbin]# hdfs namenode -bootstrapStandby[root@slave1 sbin]# ./hadoop-daemon.sh start namenode 5.7在master上启动datanode 1234[root@master sbin]# ./hadoop-daemons.sh start datanodemaster: starting datanode, logging to /usr/hadoop/hadoop-2.7.3/logs/hadoop-root-datanode-master.outslave2: starting datanode, logging to /usr/hadoop/hadoop-2.7.3/logs/hadoop-root-datanode-slave2.outslave1: starting datanode, logging to /usr/hadoop/hadoop-2.7.3/logs/hadoop-root-datanode-slave1.out 5.8在slave1和slave2上启动yarn 1./start-yarn.sh 5.9在master上启动zkfc 1./hadoop-daemons.sh start zkfc 6.查看jpsmaster 12345678[root@master sbin]# jps2593 DataNode2709 NodeManager2902 DFSZKFailoverController2232 JournalNode2969 Jps2157 QuorumPeerMain2431 NameNode slave1 12345678[root@slave1 sbin]# jps2337 QuorumPeerMain3074 Jps2259 JournalNode2709 ResourceManager2475 NameNode2587 DataNode3007 DFSZKFailoverController slave2 1234567[root@slave2 sbin]# jps2355 DataNode2164 JournalNode2244 QuorumPeerMain3126 NodeManager3017 ResourceManager3162 Jps 启动如上则正常如果有服务没有启动,重启该服务,例如Resourcemanager没启动 1234#停止./stop-yarn.sh#启动./start-yarn.sh 然后在50070和8088端口进行测试在测试之前为了防止namenode不能热切换,最好安装此插件在master和slave1上安装 1yum -y install psmisc 7.测试在(master的ip)192.168.134.154:50070和(slave1的ip)192.168.134.155:50070上查看namenode的状态 都能访问且一个是active一个是standby状态 然后访问(slave1)192.168.134.155:8088和(slave2)192.168.134.156:8088查看Resourcemanager状态 若是一个能访问,访问另一个时跳到前一个的时候并不是错误,那样是正常的能访问的那个是active状态,若是两个都能访问则一个是active一个是standby 首先在master主机上想hdfs上传一个文件,然后尝试能否在slave1和slave2上查看 1234567[root@master tmp]# cd /usr/tmp[root@master tmp]# touch test[root@master tmp]# hadoop fs -put test /#分别在三台虚拟机上查看[root@master tmp]# hadoop fs -ls /Found 1 items-rw-r--r-- 3 root supergroup 0 2018-10-22 20:42 /test 如果都能查看到,接下来再测试是否能够热切换 1234567891011#查看进程[root@master tmp]# jps2593 DataNode2902 DFSZKFailoverController2232 JournalNode3609 NodeManager2157 QuorumPeerMain2431 NameNode3807 Jps#杀死active的namenode[root@master tmp]# kill -9 2431 在网页查看standby的是否变为active从standby成功变更为active则表示成功同样,测试yarn HA高可用 12345678910#查看进程[root@slave2 sbin]# jps4050 Jps2355 DataNode2164 JournalNode2244 QuorumPeerMain3423 ResourceManager3919 NodeManager#杀死active的ResourceManager[root@slave2 sbin]# kill -9 3423 在网页查看 如果说你杀死了namenode进程,那么相应的50070端口自然无法访问了,同理8088端口一样 至此hadoop HA高可用版搭建完成. 接下来会说一下hive的安装和使用]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Zookeeper</tag>
        <tag>HA高可用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Combiners编程]]></title>
    <url>%2F2018%2F10%2F19%2Fblog181019-4%2F</url>
    <content type="text"><![CDATA[这个Combiners编程示范是基于 MapReduce对手机上网记录的简单分析和Partitioner分区]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Java</tag>
        <tag>Combiners</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zookeeper的安装]]></title>
    <url>%2F2018%2F10%2F19%2Fblog181019-3%2F</url>
    <content type="text"><![CDATA[概览1.Zookeeper简介2.Zookeeper的安装3.Zookeeper的配置4.启动集群5.数据同步测试 1.Zookeeper简介Zookeeper功能简介 ZooKeeper 是一个开源的分布式协调服务，由雅虎创建，是 Google Chubby 的开源实现分布式应用程序可以基于ZooKeeper 实现诸如数据发布/订阅、负载均衡、命名服务、分布式协调/通知、集群管理、Master选举、配置维护，名字服务、分布式同步、分布式锁和分布式队列等功能。 在Zookeeper中共有三个角色 1. Leader 2. Follower 3. Observe 一个 ZooKeeper 集群同一时刻只会有一个 Leader，其他都是 Follower 或 Observer。 2.Zookeeper的安装这次我们是在之前的Hadoop集群搭建的基础上安装的,所以没有搭建好的小伙伴最好先配置好Hadoop集群,不然还要安装jdk 在虚拟机的/usr下创建个zookeeper文件夹mkdir zookeeper然后将zookeeper的安装包(压缩包)上传到该文件夹下 解压 123456[root@master usr]# mkdir zookeeper[root@master usr]# lsbin etc games hadoop include java lib lib64 libexec local sbin share src tmp zookeeper[root@master usr]# cd zookeeper/#解压[root@master zookeeper]# tar -zxf zookeeper-3.4.12.tar.gz 3.Zookeeper的配置（先在一台节点上配置）添加一个zoo.cfg配置文件 1234#进入到Zookeeper的配置目录cd /usr/zookeeper/zookeeper-3.4.12/conf#copy出来一个配置文件cp zoo_sample.cfg zoo.cfg 这是zoo.cfg中各项的含义 zookeeper的默认配置文件为zookeeper/conf/zoo_sample.cfg，需要将其修改为zoo.cfg。其中各配置项的含义，解释如下： 1.tickTime：CS通信心跳时间 Zookeeper 服务器之间或客户端与服务器之间维持心跳的时间间隔，也就是每个 tickTime 时间就会发送一个心跳。tickTime以毫秒为单位。 tickTime=2000 2.initLimit：LF初始通信时限 集群中的follower服务器(F)与leader服务器(L)之间初始连接时能容忍的最多心跳数（tickTime的数量）。initLimit=5 3.syncLimit：LF同步通信时限 集群中的follower服务器与leader服务器之间请求和应答之间能容忍的最多心跳数（tickTime的数量）。 syncLimit=2 4.dataDir：数据文件目录 Zookeeper保存数据的目录，默认情况下，Zookeeper将写数据的日志文件也保存在这个目录里。 dataDir=/home/michael/opt/zookeeper/data 5.clientPort：客户端连接端口 客户端连接 Zookeeper 服务器的端口，Zookeeper 会监听这个端口，接受客户端的访问请求。 clientPort=2181 6.服务器名称与地址：集群信息（服务器编号，服务器地址，LF通信端口，选举端口） 这个配置项的书写格式比较特殊，规则如下： server.N=YYY:A:B 修改配置文件（zoo.cfg）将dataDir=/tmp/zookeeper 改为 dataDir=/usr/zookeeper/data然后在最后追加 123456#master,slave1,slave2这是我的三台虚拟机主机名#并且在/usr/hosts文件中做了ip映射#所以可以直接写主机名称,如果你没有做映射,将主机名替换为ip地址server.1=master:2888:3888server.2=slave1:2888:3888server.3=slave2:2888:3888 然后我们在/usr/zookeeper文件夹下创建一个data文件夹 1234[root@master conf]# cd /usr/zookeeper/[root@master zookeeper]# lszookeeper-3.4.12 zookeeper-3.4.12.tar.gz[root@master zookeeper]# mkdir data 然后在data下创建一个myid文件(ZooKeeper 配置很简单，每个节点的配置文件(zoo.cfg)都是一样的，只有 myid 文件不一样。myid 的值必须是 zoo.cfg中server.{数值} 的{数值}部分。)在（/usr/zookeeper/data 需要自己创建）创建一个myid文件，里面内容是server.N中的N（server.2里面内容为2） 1234567#建立文件[root@master data]# touch myid#传入值[root@master data]# echo 1 &gt; myid #查看[root@master data]# cat myid 1 然后我们把配置好的zookeeper传给另外两台主机slave1和slave2 12scp -r /usr/zookeeper root@slave1:/usrscp -r /usr/zookeeper root@slave2:/usr 注意：在其他节点上一定要修改myid的内容 在slave1应该讲myid的内容改为2（echo 2 &gt; myid） 在slave2应该讲myid的内容改为3 （echo 3 &gt; myid） 4.启动集群(注意关闭防火墙)在master , slave1 , slave2分别启动zookeeper 123456#进入zookeeper下的bin文件夹[root@master usr]# cd /usr/zookeeper/zookeeper-3.4.12/bin[root@master bin]# lsREADME.txt zkCleanup.sh zkCli.cmd zkCli.sh zkEnv.cmd zkEnv.sh zkServer.cmd zkServer.sh#启动服务[root@master bin]# ./zkServer.sh start 查看状态12345678[root@master bin]# ./zkServer.sh startZooKeeper JMX enabled by defaultUsing config: /usr/zookeeper/zookeeper-3.4.12/bin/../conf/zoo.cfgStarting zookeeper ... STARTED[root@master bin]# ./zkServer.sh statusZooKeeper JMX enabled by defaultUsing config: /usr/zookeeper/zookeeper-3.4.12/bin/../conf/zoo.cfgMode: follower #这里是follower便可以认为启动成功 5.数据同步测试进入到zkCilent 1./zkCli.sh 在主机master上创建一个文件,看看是否同步到其他机器上 1234567891011121314151617181920#查看[zk: localhost:2181(CONNECTED) 0] ls /[zookeeper]#创建文件 /路径 内容[zk: localhost:2181(CONNECTED) 1] create /test helloCreated /test#查看文件内容[zk: localhost:2181(CONNECTED) 2] get /testhellocZxid = 0x100000002ctime = Fri Oct 19 21:51:21 CST 2018mZxid = 0x100000002mtime = Fri Oct 19 21:51:21 CST 2018pZxid = 0x100000002cversion = 0dataVersion = 0aclVersion = 0ephemeralOwner = 0x0dataLength = 5numChildren = 0 在其他机器上也进行查看 123456789101112131415161718#进入到zkCli[root@slave1 bin]# ./zkCli.sh#查看[zk: localhost:2181(CONNECTED) 0] ls /[zookeeper, test][zk: localhost:2181(CONNECTED) 1] get /testhellocZxid = 0x100000002ctime = Fri Oct 19 21:51:21 CST 2018mZxid = 0x100000002mtime = Fri Oct 19 21:51:21 CST 2018pZxid = 0x100000002cversion = 0dataVersion = 0aclVersion = 0ephemeralOwner = 0x0dataLength = 5numChildren = 0 再到slave2上查看,数据是同步的 删除 123[zk: localhost:2181(CONNECTED) 3] delete /test[zk: localhost:2181(CONNECTED) 4] ls /[zookeeper] 再到其他机器进行查看,发现也删除了,表示你的zookeeper的集群安装已完成 退出zkCli 1[zk: localhost:2181(CONNECTED) 5] quit 结束zkServer12#结束服务[root@master bin]# ./zkServer.sh stop 接下来还会介绍hadoop+zookeeper3节点高可用集群搭建]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[将MapReduce分析手机上网记录的结果进行排序操作]]></title>
    <url>%2F2018%2F10%2F19%2Fblog181019-2%2F</url>
    <content type="text"><![CDATA[上次我们说过了MapReduce对手机上网记录的简单分析和Partitioner分区这次我们介绍一下如何将手机上网记录根据总流量的多少进行排序 1.编写Java代码,并将其打包成jar包在eclipse上创建个新的java项目,创建lib文件夹,将上次的jar同样导入进来 然后创建个TelBean类这里实现了WritableComparable接口,就是序列化的比较,详情查询api文档 public interface Comparator比较功能，对一些对象的集合施加了一个整体排序 。 可以将比较器传递给排序方法（如Collections.sort或Arrays.sort ），以便对排序顺序进行精确控制。 比较器还可以用来控制某些数据结构（如顺序sorted sets或sorted maps ），或对于不具有对象的集合提供的排序natural ordering 。通过比较c上的一组元素S的确定的顺序对被认为是与equals一致当且仅当c.compare(e1, e2)==0具有用于S每e1和e2相同布尔值e1.equals(e2)。 当使用能够强制排序不一致的比较器时，应注意使用排序集（或排序图）。 假设具有显式比较器c的排序集（或排序映射）与从集合S中绘制的元素（或键）一起使用 。 如果88446235254451上的c强制的排序与equals不一致，则排序集（或排序映射）将表现为“奇怪”。特别是排序集（或排序图）将违反用于设置（或映射）的一般合同，其按equals定义。 例如，假设一个将两个元件a和b ，使得(a.equals(b) &amp;&amp; c.compare(a, b) != 0)到空TreeSet与比较c。 因为a和b与树集的角度不相等，所以第二个add操作将返回true（并且树集的大小将增加），即使这与Set.add方法的规范相反。 注意：这通常是一个好主意比较，也能实现java.io.Serializable，因为它们可能被用来作为排序的序列化数据结构的方法（如TreeSet， TreeMap ）。 为了使数据结构成功序列化，比较器（如果提供）必须实现Serializable 。 对于数学上的倾斜，即限定了施加顺序 ，给定的比较器c上一组给定对象的S强加关系式为： {(x, y) such that c.compare(x, y) &lt;= 0}. 这个总订单的商是： {(x, y) suchthat c.compare(x, y) == 0}. 它从合同compare，该商数是S的等价关系紧随其后，而强加的排序是S， 总订单 。当我们说S上的c所规定的顺序与等于一致时，我们的意思是排序的商是由对象’ equals(Object)方法定义的等价关系： {(x,y) such that x.equals(y)}. 与Comparable不同，比较器可以可选地允许比较空参数，同时保持对等价关系的要求。 此接口是成员Java Collections Framework 。 从以下版本开始：1.2 另请参见： Comparable ， Serializable 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495package com.zy.hadoop.entity;import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;import org.apache.hadoop.io.Writable;import org.apache.hadoop.io.WritableComparable;public class TelBean implements WritableComparable&lt;TelBean&gt;&#123; private String tel; private long upPayLoad; private long downPayLoad; private long totalPayLoad; public String getTel() &#123; return tel; &#125; public void setTel(String tel) &#123; this.tel = tel; &#125; public long getUpPayLoad() &#123; return upPayLoad; &#125; public void setUpPayLoad(long upPayLoad) &#123; this.upPayLoad = upPayLoad; &#125; public long getDownPayLoad() &#123; return downPayLoad; &#125; public void setDownPayLoad(long downPayLoad) &#123; this.downPayLoad = downPayLoad; &#125; public long getTotalPayLoad() &#123; return totalPayLoad; &#125; public void setTotalPayLoad(long totalPayLoad) &#123; this.totalPayLoad = totalPayLoad; &#125; public TelBean(String tel, long upPayLoad, long downPayLoad, long totalPayLoad) &#123; super(); this.tel = tel; this.upPayLoad = upPayLoad; this.downPayLoad = downPayLoad; this.totalPayLoad = totalPayLoad; &#125; public TelBean() &#123; super(); // TODO Auto-generated constructor stub &#125; @Override public String toString() &#123; return tel + &quot;\t&quot; + upPayLoad + &quot;\t&quot; + downPayLoad + &quot;\t&quot; + totalPayLoad ; &#125; //反序列化的过程 @Override public void readFields(DataInput in) throws IOException &#123; this.tel = in.readUTF(); this.upPayLoad = in.readLong(); this.downPayLoad = in.readLong(); this.totalPayLoad = in.readLong(); &#125; //序列化的过程 @Override public void write(DataOutput out) throws IOException &#123; // TODO Auto-generated method stub out.writeUTF(this.tel); out.writeLong(this.upPayLoad); out.writeLong(this.downPayLoad); out.writeLong(this.totalPayLoad); &#125; //compare比较,详情查阅java的api文档 @Override public int compareTo(TelBean bean) &#123; // TODO Auto-generated method stub return (int)(this.totalPayLoad-bean.getTotalPayLoad()); &#125;&#125; 然后在mr包下依次建立SortMapper,SortReducer,SortCount 关于分析可以查看MapReduce对手机上网记录的简单分析和Partitioner分区SortMapper 12345678910111213141516171819202122232425262728293031323334353637package com.zy.hadoop.mr2;import java.io.IOException;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;import com.zy.hadoop.entity.TelBean;public class SortMapper extends Mapper&lt;LongWritable, Text, TelBean, NullWritable&gt;&#123; @Override protected void map(LongWritable key, Text value, Mapper&lt;LongWritable, Text, TelBean, NullWritable&gt;.Context context) throws IOException, InterruptedException &#123; //value ,第一mr出来的结果中的每一行 String line = value.toString(); //拆分字符串&quot;\t&quot; String[] strs = line.split(&quot;\t&quot;); //直接通过下标取值 //电话号码 String tel = strs[0]; //上行流量 long upPayLoad=Long.parseLong(strs[2]); //下行流量 long downPayLoad=Long.parseLong(strs[3]); //总流量 long totalPayLoad=Long.parseLong(strs[4]); //把去除的值封装到对象中 TelBean telBean = new TelBean(tel, upPayLoad, downPayLoad, totalPayLoad); //输出k2,v2 context.write(telBean, NullWritable.get()); &#125;&#125; SortReducer 1234567891011121314151617181920package com.zy.hadoop.mr2;import java.io.IOException;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.mapreduce.Reducer;import com.zy.hadoop.entity.TelBean;public class SortReducer extends Reducer&lt;TelBean, NullWritable, TelBean, NullWritable&gt;&#123; @Override protected void reduce(TelBean arg0, Iterable&lt;NullWritable&gt; arg1, Reducer&lt;TelBean, NullWritable, TelBean, NullWritable&gt;.Context arg2) throws IOException, InterruptedException &#123; arg2.write(arg0, NullWritable.get()); &#125; &#125; SortCount 123456789101112131415161718192021222324252627282930313233343536373839404142package com.zy.hadoop.mr2;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import com.zy.hadoop.entity.TelBean;public class SortCount &#123; public static void main(String[] args) throws Exception &#123; // 1.获取job Configuration conf = new Configuration(); Job job = Job.getInstance(conf); // 2.指定job使用的类 job.setJarByClass(SortCount.class); // 3.设置Mapper的属性 job.setMapperClass(SortMapper.class); job.setMapOutputKeyClass(TelBean.class); job.setMapOutputValueClass(NullWritable.class); // 4.设置输入文件 FileInputFormat.setInputPaths(job, new Path(args[0])); // 5.设置reducer的属性 job.setReducerClass(SortReducer.class); job.setOutputKeyClass(TelBean.class); job.setMapOutputValueClass(NullWritable.class); // 6.设置输出文件夹,查看结果保存到hdfs文件夹中的位置 FileOutputFormat.setOutputPath(job, new Path(args[1])); // 7.提交 true 提交的时候打印日志信息 job.waitForCompletion(true); &#125;&#125; 接下来将项目打包成jar包,上传到虚拟机/usr/tmp下 2.虚拟机上运行jar包,查看结果启动hadoop集群服务 1start-all.sh 查看是否成功 我们将之前处理过一次的文件/tel1/part-r-00000(/tel2下的进行过分区了,所以不进行处理)作为源文件进行分析排序 1hadoop jar tel_3.jar /tel/part-r-00000 /tel3 等待执行完毕查看结果 结果如下123456789101112131415161718192021222324252627282930[root@master tmp]# hadoop fs -ls /hadoopFound 5 items-rw-r--r-- 1 root supergroup 2315 2018-10-19 19:33 /tel.logdrwxr-xr-x - root supergroup 0 2018-10-19 19:59 /tel1drwxr-xr-x - root supergroup 0 2018-10-19 20:10 /tel2drwxr-xr-x - root supergroup 0 2018-10-19 20:47 /tel3drwx------ - root supergroup 0 2018-10-19 19:58 /tmp[root@master tmp]# hadoop fs -ls /tel3Found 2 items-rw-r--r-- 1 root supergroup 0 2018-10-19 20:47 /tel3/_SUCCESS-rw-r--r-- 1 root supergroup 477 2018-10-19 20:47 /tel3/part-r-00000[root@master tmp]# hadoop fs -cat /tel3/part-r-0000013926251106 240 0 24013826544101 264 0 26413480253104 180 180 36013926435656 132 1512 164415989002119 1938 180 211818211575961 1527 2106 363313560436666 2232 1908 414013602846565 1938 2910 484884138413 4116 1432 554815920133257 3156 2936 609213922314466 3008 3720 672815013685858 3659 3538 719713660577991 6960 690 765013560439658 2034 5892 792618320173382 9531 2412 1194313726238888 2481 24681 2716213925057413 11058 48243 5930113502468823 7335 110349 117684 这就是MapReduce进行简单的数据分析 不过hadoop集群的namenode如果只有一个,namenode机器宕机整个集群都会不可用 , 接下来会介绍zookeeper的高可用hadoop集群如何搭建]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Java</tag>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapReduce对手机上网记录的简单分析和Partitioner分区]]></title>
    <url>%2F2018%2F10%2F19%2Fblog181019-1%2F</url>
    <content type="text"><![CDATA[概览1.MapReduce处理手机上网记录2.Partitioner分区 上次说过了关于MapReduce的执行流程和原理,下面来说下分区和简单示例 1.MapReduce处理手机上网记录首先我们需要先模拟一个通话记录文件 在Windows的桌面建个tel.log的文件,里面模拟一些通话记录信息 12345678910111213141516171819202122231363157985066 13726230503 00-FD-07-A4-72-B8:CMCC 120.196.100.82 i02.c.aliimg.com 24 27 2481 24681 2001363157995052 13826544101 5C-0E-8B-C7-F1-E0:CMCC 120.197.40.4 4 0 264 0 2001363157991076 13926435656 20-10-7A-28-CC-0A:CMCC 120.196.100.99 2 4 132 1512 2001363154400022 13926251106 5C-0E-8B-8B-B1-50:CMCC 120.197.40.4 4 0 240 0 2001363157993044 18211575961 94-71-AC-CD-E6-18:CMCC-EASY 120.196.100.99 iface.qiyi.com 视频网站 15 12 1527 2106 2001363157995074 84138413 5C-0E-8B-8C-E8-20:7DaysInn 120.197.40.4 122.72.52.12 20 16 4116 1432 2001363157993055 13560439658 C4-17-FE-BA-DE-D9:CMCC 120.196.100.99 18 15 1116 954 2001363157995033 15920133257 5C-0E-8B-C7-BA-20:CMCC 120.197.40.4 sug.so.360.cn 信息安全 20 20 3156 2936 2001363157983019 13719199419 68-A1-B7-03-07-B1:CMCC-EASY 120.196.100.82 4 0 240 0 2001363157984041 13660577991 5C-0E-8B-92-5C-20:CMCC-EASY 120.197.40.4 s19.cnzz.com 站点统计 24 9 6960 690 2001363157973098 15013685858 5C-0E-8B-C7-F7-90:CMCC 120.197.40.4 rank.ie.sogou.com 搜索引擎 28 27 3659 3538 2001363157986029 15989002119 E8-99-C4-4E-93-E0:CMCC-EASY 120.196.100.99 www.umeng.com 站点统计 3 3 1938 180 2001363157992093 13560439658 C4-17-FE-BA-DE-D9:CMCC 120.196.100.99 15 9 918 4938 2001363157986041 13480253104 5C-0E-8B-C7-FC-80:CMCC-EASY 120.197.40.4 3 3 180 180 2001363157984040 13602846565 5C-0E-8B-8B-B6-00:CMCC 120.197.40.4 2052.flash2-http.qq.com 综合门户 15 12 1938 2910 2001363157995093 13922314466 00-FD-07-A2-EC-BA:CMCC 120.196.100.82 img.qfc.cn 12 12 3008 3720 2001363157982040 13502468823 5C-0A-5B-6A-0B-D4:CMCC-EASY 120.196.100.99 y0.ifengimg.com 综合门户 57 102 7335 110349 2001363157986072 18320173382 84-25-DB-4F-10-1A:CMCC-EASY 120.196.100.99 input.shouji.sogou.com 搜索引擎 21 18 9531 2412 2001363157990043 13925057413 00-1F-64-E1-E6-9A:CMCC 120.196.100.55 t3.baidu.com 搜索引擎 69 63 11058 48243 2001363157988072 13760778710 00-FD-07-A4-7B-08:CMCC 120.196.100.82 2 2 120 120 2001363157985066 13726238888 00-FD-07-A4-72-B8:CMCC 120.196.100.82 i02.c.aliimg.com 24 27 2481 24681 2001363157993055 13560436666 C4-17-FE-BA-DE-D9:CMCC 120.196.100.99 18 15 1116 954 2001363157993055 13560436666 C4-17-FE-BA-DE-D9:CMCC 120.196.100.99 18 15 1116 954 200 这些字段代表的是 首先我们需要将部分字段提取出来,以便之后进行分析 在主机master上启动hadoop集群,hadoop集群版的搭建可以参照简单的hadoop集群搭建 1start-all.sh 验证是否启动成功 然后将tel.log文件利用Xftp传输到虚拟机中的/usr/tmp下cd /usr/tmp/ 然后上传到hdfs 123456#上传[root@master tmp]# hadoop fs -put tel.log /#查看[root@master tmp]# hadoop fs -ls /Found 1 items-rw-r--r-- 1 root supergroup 2315 2018-10-19 19:33 /tel.log 然后在eclipse上新建java项目,并在项目下建个lib文件夹,然后将jar包放到lib中导入项目然后创建包,创建一个telBean实体类,这次我们分析的是手机号和其对应的上行流量,下行流量和总流量所以将其封装成实体类 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889package com.hd.entity;import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;import org.apache.hadoop.io.Writable;public class TelBean implements Writable&#123; private String tel; private long upPayLoad; private long downPayLoad; private long totalPayLoad; public String getTel() &#123; return tel; &#125; public void setTel(String tel) &#123; this.tel = tel; &#125; public long getUpPayLoad() &#123; return upPayLoad; &#125; public void setUpPayLoad(long upPayLoad) &#123; this.upPayLoad = upPayLoad; &#125; public long getDownPayLoad() &#123; return downPayLoad; &#125; public void setDownPayLoad(long downPayLoad) &#123; this.downPayLoad = downPayLoad; &#125; public long getTotalPayLoad() &#123; return totalPayLoad; &#125; public void setTotalPayLoad(long totalPayLoad) &#123; this.totalPayLoad = totalPayLoad; &#125; public TelBean(String tel, long upPayLoad, long downPayLoad, long totalPayLoad) &#123; super(); this.tel = tel; this.upPayLoad = upPayLoad; this.downPayLoad = downPayLoad; this.totalPayLoad = totalPayLoad; &#125; public TelBean() &#123; super(); // TODO Auto-generated constructor stub &#125; @Override public String toString() &#123; return tel + &quot;\t&quot; + upPayLoad + &quot;\t&quot; + downPayLoad + &quot;\t&quot; + totalPayLoad ; &#125; //反序列化的过程 @Override public void readFields(DataInput in) throws IOException &#123; this.tel = in.readUTF(); this.upPayLoad = in.readLong(); this.downPayLoad = in.readLong(); this.totalPayLoad = in.readLong(); &#125; //序列化的过程 @Override public void write(DataOutput out) throws IOException &#123; // TODO Auto-generated method stub out.writeUTF(this.tel); out.writeLong(this.upPayLoad); out.writeLong(this.downPayLoad); out.writeLong(this.totalPayLoad); &#125;&#125; 在mr包下创建个TelMapper类继承Mapper首先分析一下,我们要传入的第一个需要Map处理的&lt;k1,v1&gt;是long类型(电话号码)和String(Text)类型(与之对应的一行记录),而从Map处理过的&lt;k2,v2&gt;是String类型(电话号码)和TelBean对象(将我们需要的字段封装成对象) 1234567891011121314151617181920212223242526272829303132package com.hd.hadoop.mr;import java.io.IOException;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;import com.zy.hadoop.entity.TelBean;//k1,v1 long string k2,v2 string TelBeanpublic class TelMapper extends Mapper&lt;LongWritable, Text, Text, TelBean&gt; &#123; @Override protected void map(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, TelBean&gt;.Context context) throws IOException, InterruptedException &#123; //value 对应 tel.log中的每一行数据,行中的数据以\t隔开的 String line = value.toString(); //对正航读取的数据进行拆分 String[] res = line.split(&quot;\t&quot;);//0---res.length-1 //取数组中的电话号码 String tel = res[1]; //取上行流量 long upPayLoad = Long.parseLong(res[8]); //取下行流量 long downPayLoad = Long.parseLong(res[9]); //创建telBean对象 TelBean telBean = new TelBean(tel, upPayLoad, downPayLoad, 0); context.write(new Text(tel), telBean); &#125;&#125; 然后创建个TelReducer类继承Reducer分析一下,这里传入的&lt;k2,v2&gt;是String(Text)类型和TelBean类型,而我们处理过输出的&lt;k3,v3&gt;也是相同类型,这里要记得将TelBean的toString方法重写,不然输出的是对象地址 12345678910111213141516171819202122232425262728293031package com.hd.hadoop.mr;import java.io.IOException;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Reducer;import com.zy.hadoop.entity.TelBean;public class TelReducer extends Reducer&lt;Text, TelBean, Text, TelBean&gt; &#123; @Override protected void reduce(Text key, Iterable&lt;TelBean&gt; value, Reducer&lt;Text, TelBean, Text, TelBean&gt;.Context context) throws IOException, InterruptedException &#123; //声明一个上行流量的变量 long upPayLoad = 0; //声明一个下行流量 long downPayLoad = 0; for (TelBean telBean : value) &#123; //统计相同电话的上行流量的和,下行流量的和 upPayLoad += telBean.getUpPayLoad(); downPayLoad += telBean.getDownPayLoad(); &#125; //k3 v3 TelBean telBean= new TelBean(key.toString(), upPayLoad, downPayLoad, upPayLoad+downPayLoad); context.write(key, telBean); &#125;&#125; 最后我们创建个主方法TelCount类 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748package com.zy.hadoop.mr1;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import com.zy.hadoop.entity.TelBean;public class TelCount &#123; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; // 1.获取job Configuration conf = new Configuration(); Job job = Job.getInstance(conf); // 2.指定job使用的类 job.setJarByClass(TelCount.class); // 3.设置Mapper的属性 job.setMapperClass(TelMapper.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(TelBean.class); // 4.设置输入文件 args[0]手动输入输入文件的位置 FileInputFormat.setInputPaths(job, new Path(args[0])); // 5.设置reducer的属性 job.setReducerClass(TelReducer.class); job.setOutputKeyClass(Text.class); job.setMapOutputValueClass(TelBean.class); // 6.设置输出文件夹,查看结果保存到hdfs文件夹中的位置 //args[1]手动输入输出文件的位置 FileOutputFormat.setOutputPath(job, new Path(args[1])); // 7.提交 true 提交的时候打印日志信息 job.waitForCompletion(true); &#125;&#125; 最后将其打成Jar包,主方法选择TelCount,然后上传到虚拟机/usr/tmp下 然后执行jar包 1hadoop jar tel_1.jar /tel.log /tel1 等待执行成功后查看结果文件 1234567891011121314151617181920212223#结果如下[root@master tmp]# hadoop fs -cat /tel1/part-r-0000013480253104 13480253104 180 180 36013502468823 13502468823 7335 110349 11768413560436666 13560436666 2232 1908 414013560439658 13560439658 2034 5892 792613602846565 13602846565 1938 2910 484813660577991 13660577991 6960 690 765013719199419 13719199419 240 0 24013726230503 13726230503 2481 24681 2716213726238888 13726238888 2481 24681 2716213760778710 13760778710 120 120 24013826544101 13826544101 264 0 26413922314466 13922314466 3008 3720 672813925057413 13925057413 11058 48243 5930113926251106 13926251106 240 0 24013926435656 13926435656 132 1512 164415013685858 15013685858 3659 3538 719715920133257 15920133257 3156 2936 609215989002119 15989002119 1938 180 211818211575961 18211575961 1527 2106 363318320173382 18320173382 9531 2412 1194384138413 84138413 4116 1432 5548 这说明执行成功了 2.Partitioner分区什么是Partitioner? 在进行MapReduce计算时，有时候需要把最终的输出数据分到不同的文件中，比如按照省份划分的话，需要把同一省份的数据放到一个文件中；按照性别划分的话，需要把同一性别的数据放到一个文件中。我们知道最终的输出数据是来自于Reducer任务。那么，如果要得到多个文件，意味着有同样数量的Reducer任务在运行。Reducer任务的数据来自于Mapper任务，也就说Mapper任务要划分数据，对于不同的数据分配给不同的Reducer任务运行。Mapper任务划分数据的过程就称作Partition。负责实现划分数据的类称作Partitioner。 我们还是处理手机的上网记录在之前的mr包中见一个TCPartitioner类我们将135和136开头的号码视为移动用户,处理结果放到一起(part-r-00000),另外的号码处理结果放到一起(part-r-00001) 1234567891011121314151617181920212223242526package com.hd.hadoop.mr;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Partitioner;import com.zy.hadoop.entity.TelBean;public class TCPartitioner extends Partitioner&lt;Text, TelBean&gt; &#123; @Override public int getPartition(Text text, TelBean telBean, int arg2) &#123; // TODO Auto-generated method stub String tel = text.toString(); String sub_tel = tel.substring(0, 3);//取手机号前三位进行分区 //假设135 136的为移动的 放一个分区,其他的放一个分区 if(sub_tel.equals(&quot;135&quot;)||sub_tel.equals(&quot;136&quot;))&#123; //return的数对应着计算结果文件 part-r-00001 return 1; &#125; return 0; &#125;&#125; 然后在TelCount添加几行代码 再将项目打成jar包放入到虚拟机/usr/tmp下,然后执行 1hadoop jar tel_2.jar /tel.log /tel2 等待执行完毕后查看结果 12345678910111213#查看生成几个结果文件[root@master tmp]# hadoop fs -ls /tel2Found 3 items-rw-r--r-- 1 root supergroup 0 2018-10-19 20:10 /tel2/_SUCCESS-rw-r--r-- 1 root supergroup 603 2018-10-19 20:10 /tel2/part-r-00000-rw-r--r-- 1 root supergroup 198 2018-10-19 20:10 /tel2/part-r-00001#查看135和136开头的手机号的结果[root@master tmp]# hadoop fs -cat /tel2/part-r-0000113502468823 13502468823 7335 110349 11768413560436666 13560436666 2232 1908 414013560439658 13560439658 2034 5892 792613602846565 13602846565 1938 2910 484813660577991 13660577991 6960 690 7650 这就是简单的分区操作 接下来还有如何将分析的结果进行排序操作]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Java</tag>
        <tag>MapReduce</tag>
        <tag>Partitioner</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapReduce的执行流程和原理]]></title>
    <url>%2F2018%2F10%2F18%2Fblog181018-2%2F</url>
    <content type="text"><![CDATA[概览1.MapReduce简介2.MapReduce的执行流程3.MapReduce的原理4.测试MapReduce5.Java代码实现 1.MapReduce简介MapReduce是一种分布式计算模型，是Google提出的，主要用于搜索领域，解决海量数据的计算问题。 MR有两个阶段组成：Map和Reduce，用户只需实现map()和reduce()两个函数，即可实现分布式计算。 MapReduce是一种并行可扩展计算模型，并且有较好的容错性，主要解决海量离线数据的批处理。实现下面目标★ 易于编程★ 良好的扩展性★ 高容错性 MapReduce有哪些角色？各自的作用是什么？MapReduce由JobTracker和TaskTracker组成。JobTracker负责资源管理和作业控制，TaskTracker负责任务的运行。 2.MapReduce的执行流程MapReduce程序执行流程程序执行流程图如下： (1) 开发人员编写好MapReduce program，将程序打包运行。(2) JobClient向JobTracker申请可用Job，JobTracker返回JobClient一个可用Job ID。(3) JobClient得到Job ID后，将运行Job所需要的资源拷贝到共享文件系统HDFS中。(4) 资源准备完备后，JobClient向JobTracker提交Job。(5) JobTracker收到提交的Job后，初始化Job。(6) 初始化完成后，JobTracker从HDFS中获取输入splits(作业可以该启动多少Mapper任务)。(7) 与此同时，TaskTracker不断地向JobTracker汇报心跳信息，并且返回要执行的任务。(8) TaskTracker得到JobTracker分配(尽量满足数据本地化)的任务后，向HDFS获取Job资源(若数据是本地的，不需拷贝数据)。(9) 获取资源后，TaskTracker会开启JVM子进程运行任务。注：(3)中资源具体指什么？主要包含： ● 程序jar包、作业配置文件xml ● 输入划分信息，决定作业该启动多少个map任务 ● 本地文件，包含依赖的第三方jar包(-libjars)、依赖的归档文件(-archives)和普通文件(-files)，如果已经上传，则不需上传 3.MapReduce原理 MapReduce的执行步骤： 1、Map任务处理1.1 读取HDFS中的文件。每一行解析成一个&lt;k,v&gt;。每一个键值对调用一次map函数。 &lt;0,hello you&gt; &lt;10,hello me&gt;1.2 覆盖map()，接收1.1产生的&lt;k,v&gt;，进行处理，转换为新的&lt;k,v&gt;输出。 &lt;hello,1&gt; &lt;you,1&gt; &lt;hello,1&gt; &lt;me,1&gt; 1.3 对1.2输出的&lt;k,v&gt;进行分区。默认分为一个区。详见《Partitioner》1.4 对不同分区中的数据进行排序（按照k）、分组。分组指的是相同key的value放到一个集合中。 排序后： &lt;hello,1&gt; &lt;hello,1&gt; &lt;me,1&gt; &lt;you,1&gt;分组后：&lt;hello,{1,1}&gt;&lt;me,{1}&gt;&lt;you,{1}&gt; 1.5 （可选）对分组后的数据进行归约。详见《Combiner》2、Reduce任务处理 2.1 多个map任务的输出，按照不同的分区，通过网络copy到不同的reduce节点上。（shuffle）详见《shuffle过程分析》2.2 对多个map的输出进行合并、排序。覆盖reduce函数，接收的是分组后的数据，实现自己的业务逻辑， &lt;hello,2&gt; &lt;me,1&gt; &lt;you,1&gt; 处理后，产生新的&lt;k,v&gt;输出。 2.3 对reduce输出的&lt;k,v&gt;写到HDFS中。 4.测试MapReduce启动虚拟机利用Xshell工具连接 启动hadoop 1start-all.sh 上传到hdfs上一个文件test1文件test1的内容如下 123456hello worldhello lileihello haimeimeihello hadoophello girlhello girl 进入/usr/hadoop/hadoop-2.7.3/share/hadoop/mapreduce下 1cd /usr/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/ 有个 hadoop-mapreduce-examples-2.7.3.jar的jar包为test1执行这个jar包 1hadoop jar hadoop-mapreduce-examples-2.7.3.jar wordcount /test1 /result 等待执行,然后查看result文件内容 12345678910111213#查看result文件夹[root@master mapreduce]# hadoop fs -ls /resultFound 2 items-rw-r--r-- 1 root supergroup 0 2018-10-18 21:40 /result/_SUCCESS-rw-r--r-- 1 root supergroup 52 2018-10-18 21:40 /result/part-r-00000#查看part-r-00000内容[root@master mapreduce]# hadoop fs -cat /result/part-r-00000girl 2hadoop 1haimeimei 1hello 6lilei 1world 1 这个就是按照你所执行的文件,一次读取一行的内容,然后每行用空格分隔如图 5.Java代码实现首先eclipse建个Java项目,然后项目下建个lib文件夹放置jar包将复制到lib下,然后add to Build Path建立个mr包,在其中建立WCMapper,WCReducer,WordCount WCMapper 12345678910111213141516171819202122232425262728package com.hd.hadoop.mr;import java.io.IOException;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;//Mapper&lt;k1,v1,k2,v2&gt;//&lt;k1 long,v1 String&gt; &lt;k2 String,v2 long&gt;public class WCMapper extends Mapper&lt;LongWritable, Text, Text, LongWritable&gt; &#123; //重新一个map方法 @Override protected void map(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, LongWritable&gt;.Context context) throws IOException, InterruptedException &#123; // value代表文件中的每一行的数据 String line = value.toString(); //根据空格拆分字符串 String[] results = line.split(&quot; &quot;); //遍历数组得到每一个结果 for (String str : results) &#123; context.write(new Text(str), new LongWritable(1)); &#125; &#125;&#125; WCReducer 123456789101112131415161718192021222324252627package com.hd.hadoop.mr;import java.io.IOException;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Reducer;//k2 ,v2 k3 , v3public class WCReducer extends Reducer&lt;Text, LongWritable, Text, LongWritable&gt; &#123; //重写一个reduce方法 @Override protected void reduce(Text key2, Iterable&lt;LongWritable&gt; v2, Reducer&lt;Text, LongWritable, Text, LongWritable&gt;.Context context) throws IOException, InterruptedException &#123; //写自己的逻辑,统计单词个数 //定义一个遍历存放累加数据 long count=0; for (LongWritable lw : v2) &#123; count += lw.get(); &#125; //输出k3,v3 --&gt; String,Long context.write(key2, new LongWritable(count)); &#125; &#125; WordCount 1234567891011121314151617181920212223242526272829303132333435363738394041package com.hd.hadoop.mr;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class WordCount &#123; public static void main(String[] args) throws Exception &#123; // 1.获取job Configuration conf = new Configuration(); Job job = Job.getInstance(conf); // 2.指定job使用的类 job.setJarByClass(WordCount.class); // 3.设置Mapper的属性 job.setMapperClass(WCMapper.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(LongWritable.class); // 4.设置输入文件 FileInputFormat.setInputPaths(job, new Path(&quot;/test1&quot;)); // 5.设置reducer的属性 job.setReducerClass(WCReducer.class); job.setOutputKeyClass(Text.class); job.setMapOutputValueClass(LongWritable.class); // 6.设置输出文件夹,查看结果保存到hdfs文件夹中的位置 FileOutputFormat.setOutputPath(job, new Path(&quot;/result1&quot;)); // 7.提交 true 提交的时候打印日志信息 job.waitForCompletion(true); &#125;&#125; 然后打成jar包项目右键export 然后将打好的jar包利用Xftp工具放入到虚拟机master的/usr/tmp文件夹下cd /usr/tmp 然后执行1hadoop jar test1.jar 等待执行完毕 查看 123456789101112[root@master tmp]# hadoop fs -ls /result1Found 2 items-rw-r--r-- 1 root supergroup 0 2018-10-18 22:01 /result1/_SUCCESS-rw-r--r-- 1 root supergroup 55 2018-10-18 22:01 /result1/part-r-00000[root@master tmp]# hadoop fs -cat /result1/part-r-00000#与上面的测试执行结果相同,成功girl 2hadoop 1haimeimei 1hello 6lilei 1world 1 这就是MapReduce的原理和执行流程不清楚的话可以去多查看一些资料 接下来还会写一些别的示例还有Partitioner分区的用法]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Java</tag>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop之简单的hdfs上文件的上传删除及查询]]></title>
    <url>%2F2018%2F10%2F18%2Fblog181018-1%2F</url>
    <content type="text"><![CDATA[概览1.Linux上进行上传下载查询操作2.Java代码实现上传下载查询操作 上次将Hadoop集群版搭建完成了,那么怎么上传下载文件呢? 1.Linux上进行上传下载查询操作首先将Hadoop服务启动 将master,slave1,slave2三台虚拟机启动 在master上启动Hadoop服务 1start-all.sh 然后jps查看命令是否启动成功,和Hadoop集群搭建中验证方法一致 确认启动成功后,使用Xshell分别连接虚拟机 这时候需要你先将需要操作的文件传输到虚拟机中或者在虚拟机中创建文件 使用Xftp将文件上传到虚拟机中或者自己创建文件,我们就将文件放置在/usr/tmp中 1cd /usr/tmp 然后我们创建一个文本文件 1234#touch 创建文件 mkdir 创建文件夹touch test#编辑文件vim test 可以随便输入一些内容 123456hello worldhello lileihello haimeimeihello hadoophello girlhello girl 保存退出然后将文件上传到hdfs根目录中 12345678#将文件上传到Hadoop根目录中hadoop fs -put test /#查看是否上传成功hadoop fs -ls /#查看文件内容,发现与之前的内容相同则上传成功#如果你在安装虚拟机的时候没有选择中文,而在文件中有中文内容,有可能造成乱码hadoop fs -cat /test#hdfs上的文件是不支持修改的 文件上传成功了,那么接下来试一试文件夹 123456#创建文件夹mkdir testdir#上传hadoop fs -put testdir /#查看hadoop fs -ls / 删除文件/文件夹 123456#删除文件 hadoop fs -rm -f /test#删除文件夹hadoop fs -rm -r /testdir#查看hadoop fs -ls / 将hdfs上的文件下载到本地 1234567891011121314151617#将文件上传上去[root@master tmp]# hadoop fs -put test /#将test改名为test1[root@master tmp]# hadoop fs -mv /test /test1#查看文件内容[root@master tmp]# hadoop fs -cat /test1hello worldhello lileihello haimeimeihello hadoophello girlhello girl#文件下载[root@master tmp]# hadoop fs -get /test1#查看tmp文件夹下的内容[root@master tmp]# lstest test1 testdir 这就是几种基本的Linux上进行上传下载查询操作 2.Java代码实现上传下载查询操作启动eclipse或者其他工具新建个Java项目,test包,TestHadoop类 然后在项目下创建个lib文件夹存放jar包 将hadoop解压文件夹中的jar复制到lib内如图,大概共有69个jar包 然后在eclipse中选择lib文件夹下所有jar包–&gt;右键Build Path–&gt;add to Build Path 因为是测试类,所以我们将JUnit4导入到Path中 在项目上右键–&gt;Build Path–&gt;Confirgure Build Path 然后,这是TestHadoop的内容 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111package com.hd.test;import java.io.FileNotFoundException;import java.io.IOException;import java.net.URI;import java.net.URISyntaxException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileStatus;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.junit.Test;public class TestHadoop &#123; /** * 上传文件 * @throws IOException * @throws InterruptedException * @throws URISyntaxException */ @Test public void upLoad() throws IOException, InterruptedException, URISyntaxException&#123; // 创建config对象 Configuration conf = new Configuration(); // 创建FileSystem对象 192.168.134.154是你的主机master的ip地址,root是你的用户名 FileSystem fs = FileSystem.get(new URI(&quot;hdfs://192.168.134.154:9000&quot;), conf, &quot;root&quot;); // 把本地磁盘上的文件(这个文件可以自己选择)上传到hdfs上面的根目录上,这里可以改名 fs.copyFromLocalFile(false, new Path(&quot;d:/TABS.DBF&quot;), new Path(&quot;/abc.a&quot;)); // 关闭 fs.close(); &#125; /** * 下载 * @throws IOException * @throws InterruptedException * @throws URISyntaxException */ @Test public void downLoad() throws IOException, InterruptedException, URISyntaxException&#123; // 创建config对象 Configuration conf = new Configuration(); // 创建FileSystem对象 FileSystem fs = FileSystem.get(new URI(&quot;hdfs://192.168.134.154:9000&quot;), conf, &quot;root&quot;); // 把hdfs上面的根目录上的文件下载到本地磁盘上 fs.copyToLocalFile(false, new Path(&quot;/abc.a&quot;), new Path(&quot;E:/&quot;), true); // 关闭 fs.close(); &#125; /** * 删除 * @throws IOException * @throws InterruptedException * @throws URISyntaxException */ @Test public void del() throws IOException, InterruptedException, URISyntaxException&#123; // 创建config对象 Configuration conf = new Configuration(); // 创建FileSystem对象 FileSystem fs = FileSystem.get(new URI(&quot;hdfs://192.168.134.154:9000&quot;), conf, &quot;root&quot;); // 删除,false只能删除空文件夹 fs.delete(new Path(&quot;/abc.a&quot;), true); // 关闭 fs.close(); &#125; /** * 创建文件夹 * @throws IOException * @throws InterruptedException * @throws URISyntaxException */ @Test public void mkdir() throws IOException, InterruptedException, URISyntaxException&#123; // 创建config对象 Configuration conf = new Configuration(); // 创建FileSystem对象 FileSystem fs = FileSystem.get(new URI(&quot;hdfs://192.168.134.154:9000&quot;), conf, &quot;root&quot;); // 创建目录 fs.mkdirs(new Path(&quot;/a/b&quot;)); // 关闭 fs.close(); &#125; /** * 遍历查询输出 * @throws FileNotFoundException * @throws IllegalArgumentException * @throws IOException * @throws InterruptedException * @throws URISyntaxException */ @Test public void out() throws FileNotFoundException, IllegalArgumentException, IOException, InterruptedException, URISyntaxException&#123; // 创建config对象 Configuration conf = new Configuration(); // 创建FileSystem对象 FileSystem fs = FileSystem.get(new URI(&quot;hdfs://192.168.134.154:9000&quot;), conf, &quot;root&quot;); // 遍历 FileStatus[] status = fs.listStatus(new Path(&quot;/&quot;)); for (int i = 0; i &lt; status.length; i++) &#123; if (status[i].isFile()) &#123; System.err.println(&quot;文件:&quot; + status[i].getPath().toString()); &#125; else if (status[i].isDirectory()) &#123; System.err.println(&quot;目录:&quot; + status[i].getPath().toString()); &#125; &#125; // 关闭 fs.close(); &#125; &#125; 每次运行只要双击方法名然后右键Run –&gt;JUnit Test就能测试运行 然后分别在hdfs上 , 本地E盘 和 eclipse控制台 查看是否运行成功 以后会接着介绍更多的关于hadoop的操作]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop集群版搭建]]></title>
    <url>%2F2018%2F10%2F17%2Fblog181017-1%2F</url>
    <content type="text"><![CDATA[概览1.规划2.克隆虚拟机3.在hosts文件修改ip映射4.修改hadoop配置文件5.更改slaves文件6.集群版ssh免密钥登录7.重新格式化namenode8.启动hadoop并验证是否成功9.若slave的datanode没有启动 上次说了Hadoop集群单机版的搭建,这次来依照单机版的基础搭建一个简单的集群版 1.规划这次搭建的是一个主机和两个从机,也就是只有两个node节点,也可以让主机上有node节点,之后会说|主机名|cluster规划 ||–|–|| master | namenode,secondarynamenode,ResourceManager || slave1 | Datanode, NodeManager || slave2 | Datanode, NodeManager | 首先克隆一台单机版虚拟机 2.克隆虚拟机虚拟机右键–&gt;管理–&gt;克隆,选择创建完整克隆,这里克隆几个要看你创建的集群规模,我便克隆三台,一台主机,两台从机. 然后启动虚拟机,改动静态ip 这个需要三台虚拟机都改动 还是到/etc/sysconfig/network-scripts文件夹下改动ifcfg-ens33文件 1vim /etc/sysconfig/network-scripts/ifcfg-ens33 前面配置单机版的时候,如果觉得vi的命令不好用,可以安装vim命令 yum -y install vim之后就可以使用vim命令了.比vi编辑命令更加清晰 改动如图:将改为这个虚拟机的ip只要是你没有使用的即可然后保存,重启服务 1systemctl restart network 接下来使用Xshell工具分别连接三台虚拟机(没有的去官网下载,有免费版本),Xshell的优点在于你可以随意的复制粘贴命令语句 3.在hosts文件修改ip映射找到到hosts文件进行编辑 1vim /etc/hosts 写入三台主机的ip地址和主机名 123192.168.134.154 master192.168.134.155 slave1192.168.134.156 slave2 这里改动过之后最好重启虚拟机reboot,让其生效,这样最后配置ssh免密钥登录时不会出现异常 4.修改hadoop配置文件如果你的主机名还是单机版的.可以不用更改 然后进入到 /usr/hadoop/hadoop-2.7.3/etc/hadoop/下修改core-site.xml和yarn-site.xml , 三个虚拟机都要更改将之前的主机名改为现在的主机 master如图改为 将yarn-site.xml内的改为 当然,如果你的主机名还是用的单机版的,那么上述两步可以不改 接下来更改slaves文件 5.更改slaves文件还是在该文件夹下更改slaves文件三个虚拟机都需要更改(其实这些都可以在克隆前更改,再克隆,不过都开始了就算了o(∩_∩)o)1vim slaves 改为这里需要说明一下,如果你要主机也当作一个节点的话 , 那么在里面也要添加主机名称,这个文件就是告诉hadoop该启动谁的datanode 之后最重要的是ssh免密登录的配置 6.集群版ssh免密钥登录三台虚拟机都需要操作 进入到~/.ssh #每台机器先使用ssh执行以下，以在主目录产生一个.ssh 文件夹 ssh master 创建 ,然后进入 cd ~/.ssh 12345678910111213141516171819202122232425#输入以下命令，一路回车，用以产生公钥和秘钥[root@master .ssh]# ssh-keygen -t rsa -P &apos;&apos;#出现以下信息说明生成成功Generating public/private rsa key pair.Enter file in which to save the key (/root/.ssh/id_rsa): Your identification has been saved in /root/.ssh/id_rsa.Your public key has been saved in /root/.ssh/id_rsa.pub.The key fingerprint is:SHA256:6YO1h1emM9gcWvv9OT6ftHxLnjP9u8p25x1o30oq3No root@masterThe key&apos;s randomart image is:+---[RSA 2048]----+| || || || . || S o o || + O * . || . B.X. o.+.|| +o=+=**%|| .oEo*^^|+----[SHA256]-----+#将每台机器上的id_rsa.pub公钥内容复制到authorized_keys文件中[root@master .ssh]# cp id_rsa.pub authorized_keys 如果more authorized_keys 查看如图@后是你的主机名则表示正常,否则重新进行上几步进行覆写 然后分别把从机slave1和slave2的authorized_keys文件进行合并（最简单的方法是将其余三台slave主机的文件内容追加到master主机上） 12345678910111213141516171819202122#将所有的authorized_keys文件进行合并（最简单的方法是将其余三台slave主机的文件内容追加到master主机上）[root@slave1 .ssh]# cat ~/.ssh/authorized_keys | ssh root@master &apos;cat &gt;&gt; ~/.ssh/authorized_keys&apos;[root@slave2 .ssh]# cat ~/.ssh/authorized_keys | ssh root@master &apos;cat &gt;&gt; ~/.ssh/authorized_keys&apos;#查看master上的authorized_keys文件内容，类似如下即可[root@master .ssh]# more authorized_keys ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC5iw8+LlLxo0d77uaTChOKKJqfMHzp2jgzqV2hFAneFXqqWmrZ4/FrMUPenmdss19bP4Up9G7PGbJu29yZDvkDwlmuqnVajYyDOsCl7PPXPWXMIlxMGUHgSXLnQQi6QnWp04vJKDs0EbiRTd0ZYCSQefzJcZ8jbQ7bLYt6jtil7FfUupTdHTeexKKd8Mq3K7YFZHumKvhzs6wWiM+n41jANS083ss3OYmAdO2cU0w1BhLVvJhdzd6fNG3RXVCXI2v0XxCUHiqI9Oewl2qPOfKzeyy09bJxo371Ezjmt8GMrkA/Ecepkvx12qwNzC9bSPLfbnPWVo2gIxe4mMaFqCFJ root@masterssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC3CkB3Jejavt+yFphsbciktWciJmbcUBOv3ZLPVLW18ZxXGZKvG50EPXo/4By7P6IRW0wCa5YuckQEW+q6kmSatxqJ8e/K9a1mAk13N4f7V7M71Nn8IkujlF3gHYjKrmnEWpGJCyURzywIQTRArlIac1xj2SeM6q+gTMV9WrAKJupIRHli+W0kHVaYHNdKl7KMUT4KVrSl+h4wFwAd7Tcyj7JIbUcCCL6o/v/LqGFwpcJfbfUsuKJJho+tImh41j7mSXR8kRbTSZkcq5KX+iANrANwOHZ58tV5KXmMQjuVq7aJ985C16hHssB6zq/zjAxpxAyQIeE8Incc8U8ix root@slave1ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC//uaMbzbkYqxdgZJSdq+gdQYldzMQ7D3SxsUaNO5oVnVOszwbNnmL8vp1EUUehabQHPCAvCmLKUPXzfcxlyJEF/pnY77u4ySwsRVEpHvsDZbrclgCOrS6hW00sSx303KHLOgX70LfrmnohfUhvTxajzLXT+C8f5ZfTZ8meKD73HKl16jRwZQ8YhW9GUyuCkgQTGtKtTKPsRUd9LpAc/7/u8xvvvNvTYPxgyTJcUMzGSOHh8J3upI54ykY0FgBkjs1fCUaDalxAgsHw9B1iyx706WbcT6ymiQVMKGnnnM6k2KPvUvfD0swVfUSG+4ZsYSRHRTgWuiBbHoIr7DVd root@slave2 然后分发主机上的密钥 authorized_keys 12345#将master上的authorized_keys文件分发到其他主机上[root@master .ssh]# scp ~/.ssh/authorized_keys root@slave1:~/.ssh/[root@master .ssh]# scp ~/.ssh/authorized_keys root@slave2:~/.ssh/ 7.重新格式化namenode三台虚拟机都需要1hadoop namenode -format 8.启动hadoop并验证是否成功在主机master上直接启动start-all.sh从机会跟着启动 然后分别在主机从机上查看jps应该与规划相同|主机名|cluster规划 ||–|–|| master | namenode,secondarynamenode,ResourceManager || slave1 | Datanode, NodeManager || slave2 | Datanode, NodeManager | 9.若slave的datanode没有启动如果发现从机的datanode没有启动首先在主机master停止 stop-all.sh 然后进入到从机的/usr/local/hadoop/tmp/dfs/data cd /usr/local/hadoop/tmp/dfs/data 也就是hdfs-site.xml文件中dfs.datanode.data.dir的路径,data的存放位置,将其中的current删除 rm -rf current/ 然后重新初始化namenode 再重新启动hadoop即可 hadoop namenode -format start-all.sh 然后在http://主机master的ip:50070和http://主机master的ip:8088分别查看 若是如此便启动成功了 至此,简单的hadoop集群版搭建完成了 接下来会进行java代码操作hadoop文件上传下载删除的操作,看之后的blog]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop集群单机版搭建]]></title>
    <url>%2F2018%2F10%2F16%2Fblog181016-1%2F</url>
    <content type="text"><![CDATA[概览1.CentOS的安装2.设置Linux静态ip3.JDK的安装4.修改主机名和ip映射5.安装hadoop并修改配置文件6.格式化namenode7.启动hadoop8.验证是否启动成功9.设置ssh免密登录 首先本文是基于CentOS 7 , jdk1.8.0_141 和Hadoop2.7.3环境搭建 1.CentOS的安装 首先准备好CentOS7 64位的镜像 然后在VMware上安装虚拟机 这里注意选择镜像自动检测CentOS 64位, 不然之后步骤比较麻烦其他步骤都与普通安装虚拟机一样,直接默认下一步,然后开启虚拟机 这里直接进行回车继续即可 语言选择可以选择简体中文这时选择安装位置,直接点击完成即可,这样才能继续下一步操作 然后开始安装,设置并且设置root密码然后重新开机登录root账号即可,成功登录则表示CentOS安装成功 2.设置Linux静态ip 开机完成后需要我们设置静态ip,这样之后开机都不需要dhclient动态分配ip地址 首先找到/etc/sysconfig/network-scripts/下的ifcfg-ens33配置文件(如果没有找到此文件,说明你没有选择安装CentOS64位系统,建议重新安装) 1vi /etc/sysconfig/network-scripts/ifcfg-ens33 2.首先把BOOTPROTO=”dhcp”改成BOOTPROTO=”static”表示静态获取,然后把UUID注释掉,把ONBOOT=no改为yes表示开机自动静态获取,然后在最后追加比如下面的配置： 1234IPADDR=192.168.134.151 #自己的ip地址NETMASK=255.255.255.0GATEWAY=192.168.134.2DNS1=8.8.8.8 IPADDR就是静态IP，NETMASK是子网掩码，GATEWAY就是网关或者路由地址；需要说明，原来还有个NETWORK配置的是局域网网络号，这个是ifcalc自动计算的，所以这里配置这些就足够了，最终配置如下图： 如果不知道自己的GATEWAY可以去虚拟机的编辑查看虚拟网络编辑器中的NAT设置 最后保存退出 重启服务 centos6的网卡重启方法：service network restart centos7的网卡重启方法：systemctl restart network 然后查看自己的ip地址是不是自己设置的 centos6的查看ip方法: ifconfig centos7的查看ip方法: ip addr 接下来可以用Xshell工具连接虚拟机了,这样比较好操作(如果没有此工具的需要去官网下载,Xshell和配套的文件传输工具Xftp,都有免费版本) 3.JDK的安装 首先在usr的目录下创建一个java文件夹用来存放jdk的安装包并作为安装路径 这时候新建文件传输,将jdk的压缩包放入到CentOS中 然后解压jdk 解压完成之后返回到根目录的etc文件夹下,改写profile配置文件 vi /etc/profile 在profile最后加上: 1234export JAVA_HOME=/usr/java/jdk1.8.0_141export JAVA_BIN=/usr/java/jdk1.8.0_141/binexport PATH=$PATH:$JAVA_HOME/binexport CLASSPATH=:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar 保存退出,source /etc/profile刷新配置文件用java -version看看jdk环境是否配置完毕 4.修改主机名和ip映射修改etc文件夹下的hosts文件vi /etc/hosts如图: 5.安装hadoop并修改配置文件 在usr文件夹下创建hadoop文件夹作为压缩包存放和解压路径,将hadoop的压缩包传输到此文件夹下 然后解压 tar -zxf hadoop-2.7.3.tar.gz 5.1配置proflie文件注意：hadoop2.x的配置文件$HADOOP_HOME/etc/hadoop在etc的profile最后添加,然后source刷新配置文件 12export HADOOP_HOME=/usr/hadoop/hadoop-2.7.3export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin 接下来需要改五个配置文件5.2 第一个：hadoop-env.sh进入到hadoop-2.7.3/etc/hadoop文件夹下,修改hadoop-env.sh vi hadoop-env.sh第25行将改为保存退出5.3 第二个 core-site.xml在configuration中加上 12345678910&lt;!-- 制定HDFS的老大（NameNode）的地址 --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://zhiyou:9000&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定hadoop运行时产生文件的存储目录[能自动生成目录] --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/zhiyou/hadoop/tmp&lt;/value&gt; &lt;/property&gt; 保存退出 5.4 第三个 hdfs-site.xml同上configuration中添加 12345678910111213&lt;!-- 指定HDFS副本的数量 --&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;/usr/local/hadoop/tmp/dfs/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;/usr/local/hadoop/tmp/dfs/data&lt;/value&gt; &lt;/property&gt; 5.5 第四个mapred-site.xml 这个是需要你复制一个模版文件出来的 cp mapred-site.xml.template mapred-site.xml然后 vi mapred-site.xml添加 12345&lt;!-- 指定mr运行在yarn上 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; 保存退出 5.6 第五个 yarn-site.xml 12345678910&lt;!-- 指定YARN的老大（ResourceManager）的地址 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;zhiyou&lt;/value&gt; &lt;/property&gt; &lt;!-- reducer获取数据的方式 --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; 6.格式化namenode是对namenode进行初始化 1hadoop namenode -format 如果没有报错说明配置文件成功,否则重新检查配置文件 7.启动hadoop 先启动HDFS 1start-dfs.sh 再启动YARN 1start-yarn.sh 这里需要yes三次并输入你的root密码三次 8.验证是否启动成功jps 3912 DataNode4378 Jps4331 NodeManager4093 SecondaryNameNode3822 NameNode4239 ResourceManager 关闭防火墙 #停止firewall systemctl stop firewalld systemctl disable firewalld.service #禁止firewall开机启动 浏览器查看 http://ip地址:50070 （HDFS管理界面） http://ip地址:8088 (yarn管理界面）如果页面正常则说明hadoop配置成功 有人觉得每次启动都需要输入密码很繁琐,那么就设置ssh免密登录 9.设置ssh免密登录生成ssh免登陆密钥 #进入到我的home目录 cd ~/.ssh ssh-keygen -t rsa （四个回车）执行完这个命令后，会生成两个文件id_rsa（私钥）、id_rsa.pub（公钥） 将公钥拷贝到要免登陆的机器上1ssh-copy-id 192.168.134.151 然后重新启动虚拟机reboot 进入到hadoop下的sbin文件夹 1cd /usr/hadoop/hadoop-2.7.3/sbin 有个start-all.sh和stop-all.sh,这是启动和停止所有服务,这样更加快捷 启动之后再次验证是否启动成功 至此,基本的搭建已经完成]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[海琴烟]]></title>
    <url>%2F2018%2F10%2F15%2Ftest%2F</url>
    <content type="text"><![CDATA[]]></content>
  </entry>
  <entry>
    <title><![CDATA[你好]]></title>
    <url>%2F2018%2F10%2F15%2Fhello%2F</url>
    <content type="text"><![CDATA[你好,欢迎来到我的blog.]]></content>
  </entry>
</search>
